{"meta":{"title":"蜗牛君","subtitle":"一个技术沉淀的地方","description":null,"author":"蜗牛君","url":"","root":"/"},"pages":[{"title":"404 Not Found","date":"2019-10-20T07:30:53.075Z","updated":"2019-10-20T07:30:53.075Z","comments":true,"path":"404/index.html","permalink":"/404/index.html","excerpt":"","text":"**很抱歉，您访问的页面不存在** 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2019-11-03T13:54:58.336Z","updated":"2019-11-03T13:54:58.336Z","comments":true,"path":"about/index.html","permalink":"/about/index.html","excerpt":"","text":"个人信息基本资料姓名: 陈杨 微信: 1349259823 邮箱: dmlys@live.com 工作状态: 找工作 工作方向 服务端研发 微服务 数据挖掘 项目经历学术经历 Springer(EI检索): A Distributed Big Data Discretization Algorithm under Spar 计算机应用(核心期刊)：一种Spark下的分布式粗糙集属性约简算法"},{"title":"所有分类","date":"2019-10-20T07:28:14.801Z","updated":"2019-10-20T07:28:14.801Z","comments":true,"path":"categories/index.html","permalink":"/categories/index.html","excerpt":"","text":""},{"title":"contact","date":"2019-10-20T14:29:31.000Z","updated":"2019-10-20T06:29:31.584Z","comments":true,"path":"contact/index.html","permalink":"/contact/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2019-10-20T07:39:33.593Z","updated":"2019-10-20T07:39:33.593Z","comments":true,"path":"friends/index.html","permalink":"/friends/index.html","excerpt":"","text":"#这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"所有标签","date":"2019-10-20T07:27:47.070Z","updated":"2019-10-20T07:27:47.070Z","comments":true,"path":"tags/index.html","permalink":"/tags/index.html","excerpt":"","text":""},{"title":"mylist","date":"2019-10-20T07:27:19.181Z","updated":"2019-10-20T07:27:19.181Z","comments":true,"path":"mylist/index.html","permalink":"/mylist/index.html","excerpt":"","text":""},{"title":"projects","date":"2019-10-20T15:04:07.000Z","updated":"2019-10-20T07:04:07.249Z","comments":true,"path":"projects/index.html","permalink":"/projects/index.html","excerpt":"","text":""}],"posts":[{"title":"增肌塑形训练","slug":"增肌塑形训练","date":"2019-12-24T00:00:00.000Z","updated":"2019-12-24T15:22:44.210Z","comments":true,"path":"2019/12/24/增肌塑形训练/","link":"","permalink":"/2019/12/24/%E5%A2%9E%E8%82%8C%E5%A1%91%E5%BD%A2%E8%AE%AD%E7%BB%83/","excerpt":"","text":"增肌塑形训练全身肌群分解锻炼参考 健身计划推: 主要锻炼 胸部, 肩部, 三头肌 拉: 主要锻炼 二头, 背部 腿: 主要锻炼下肢整体 推 拉 腿为一个周期, 每个循环周期之间休息1-2天, 然后循环锻炼 动作计划-拉 俯身划船3*5 引体向上4*8 坐姿哑铃划船4*8 高位下拉3*10 俯身哑铃飞鸟3*10 哑铃交替弯举3*10 哑铃锤式弯举(侧弯举)3*10 动作计划-推 平板杠铃卧推3*5 站姿杠铃推举3*5 上斜哑铃卧推4*8 绳索飞鸟4*10 哑铃侧平举3*10 窄握卧推3*10 绳索下压3*10 动作计划-腿 深蹲3*5 腿举4*(8-10) 腿弯举4*(10-12) 腿屈伸4*(12-15) 站姿提臀4*(12-15) 训练动作 所有得训练动作基本都是放松时吸气, 发力时呼气, 每次得重量尽量突破上一次 杠铃系列窄距卧推窄距的杠铃卧推，是一个主要针对我们肱三头肌（手臂后侧）的训练动作，这个动作也能练到胸肌和三角肌，但是主要练的还是肱三头肌； 主要训练部位: 肱三头肌, 胸肌内侧。 组数: 3*10 动作要领 1.采用窄握距（相对于肩宽）的方式握持杠铃杆，握距大约15厘米左右，正握的方式握持； 2.将杠铃缓慢的下放，直到接触胸的中部； 3.向上推起杠铃，直到肘部绷紧。 注意事项 双手握距到差不多肩膀的宽度就可以了，再窄一点也没问题（国外有很多论坛的建议范围是15-30cm，绝对不要低于15cm），另外你的双手应该是在胸骨上方的位置, 距离过窄，一是不好发力，二是手腕和肘关节很容易受伤 站姿杠铃推举站姿杠铃推举是上肢训练的王牌的动作！不仅能训练出不亚于卧推的强壮发达的肩部，而且能强壮很多你平常想不到的肌肉。 主要训练: 背部, 肩部 组数: 3*8 标准动作 身体立正，挺胸收腹，两手握住杠铃，握距比肩稍宽。 2. 提起杠铃至肩上，掌心向出；把杠铃贴脸前向上推起，直至两臂伸直在头顶上方；然後，慢慢循原路放下至肩上。 重复。 注意事项 1. 动作开始後，只能手臂在动，身体的其他部位均要保持固定姿势。上推时，上体不要後仰、憋气。 2. 在动作过程中，手腕用力并保持固定，手腕的前後摆动会增加受伤的危险性。 3. 为了减少其他肌群参与程度，杠铃推肩时，杠铃下落到下巴处，这样对三角肌前束刺激的比较深，如果再往下直至贴近锁骨处，锁骨到下巴这个过程三头肌参与发力的比重会大一些。 4. 此外，要注意肘关节的位置，不要向後，小臂应垂直地面，当上举到最高点时注意不要锁定关节，过头顶後就可以下落，这样肩部会一直处於紧张状态。 常见的错误： 核心肌群没有收紧，使用重量过大导致过分挺腰，脊柱过度后伸，腰部承担了大部分压力，容易导致下背受伤。 平板杠铃卧推卧推是一个复合动作，既涉及到多个关节的动作，参与运动的有肩关节和肘关节。卧推的下半程会带动肩关节的运动。而肘关节的运动是由肱三头肌来带动，发生在上半程。所以在做卧推时不应将注意力放在手臂上，而应集中注意力体会胸肌发力推起杠铃的感觉，感受胸肌收缩的过程。此外，新手一开始必须学会集中体会肌肉的收缩和伸展，而不是盲目的往杠子加重量导致动作变形。 主要锻炼部位：胸大肌 辅助锻炼：三角肌，肱三头肌 动作要领: 采用宽握距，使胸大肌获得充分伸展和彻底收缩；要求躯干和胸部向上挺起成桥形，两肩下沉，横杠放在胸上置乳头上1厘米处；当杠铃推起至两臂伸直时，必须使胸大肌处于“顶峰收缩”状态，稍停。 上推时用鼻子呼气，还原时用口吸气。 注意事项： 1.不要把臀部和腰抬离凳子。 2.双脚的位置：两腿分开成45度角，平放在地上，可以有力支撑。如果把脚踩到板凳上，这样稳定性会比较差，需要分担一部分力量控制核心肌群的稳定，这样就不能发挥最大的力量锻炼胸肌。当然在史密斯机器上可以把脚放在凳子上就没有这一顾虑，而且能有效阻止腰腹协助发力，更孤立锻炼胸大肌。 动作详解 延展动作讲解 杠铃卧推 主要训练部位: 胸大肌 辅助部位: 三角肌前束, 肱三头肌 组数: 约(3-4)*(12-15), 组间休息 握距：不同握距，不同效果。握距不同，锻炼的重点也会有所不同。较宽的握距着重锻炼胸部，较窄的握距对肱三头肌和三角肌刺激更多一些。每个人的身体构造不同（臂长，肩宽），需要按照自己的情况控制握距！ 动作要领:首先仰卧, 两脚开立与肩同宽, 挺胸沉肩，肩胛骨往后夹紧 , 宽握杠铃,手腕与杠铃垂直不能弯曲(容易受伤), 举起杠铃于胸大肌中束上方, 臀部 上背和腰保持收紧, 深吸气得同时将杠铃放于胸大肌中束上方,呼气杠铃回到原位. ==臀部不要离开平凳子,腰部不要过于拱起==. 如果做完三角肌有些酸痛,可适当做一些拉伸 动作技巧:一定要热身，使用小重量卧推热身，从空杆做起，慢慢往上加重量。 做上4组小重量卧推，能够激活三角肌前束、肱三头肌和胸肌，让你的训练变得更加顺畅和完美。 详细降解 俯身杠铃划船 主要训练部位 : 中背部 辅助部位: 肱二头肌, 肩部, 背阔肌 组数: 约(3-4)*8, 组间休息 注意: 这个动作如果不标准, 是很容易伤到颈椎的，而且也容易造成你的腰部反弓！ 动作要领 双手正握杠铃，膝盖略微弯曲，上身弯腰前倾至几乎与地面平行，同时保持背部挺直。双臂自然下垂，目视前方。这是动作的起始位置。 收缩背部肌肉，以划船的轨迹将杠铃提起，同时呼气。注意肘部紧贴身体两侧。 在顶端稍适停留，然后慢慢回到起始位置，同时吸气。 以上是一次完整动作，重复动作至推荐次数。注意：背部有问题请不要做这个动作，可以用低位滑轮划船来替代。不确定自己该用多重的情况下，先选用较轻的重量。还有背部要保持挺直。变化：也可以用反握来做这个动作。 详细讲解 哑铃系列哑铃侧平举 主要训练部分: 三角肌中束 组数: 3*10 动作要领 呼气并张开双臂，但不变动肘部的角度，然后吸气并收回双臂回到起始位置，如此向上抬起，在上方的动作肩、肘与手腕形成一条单一的直线，且小拇指的位置要高于大拇指的位置，尽量少扭动躯干，惯性将大幅减少训练的效率，请不要将手完全放下，也不能过于晃动身体，否则你将感受不到三角肌被锻炼到，吸气时平稳放下，向上时呼气。 动作技巧 请降低哑铃侧平举的重量。哑铃的重量大，是导致斜方肌上部借力的最直接的一个原因。 减小动作幅度, 哑铃侧平举这个动作，你的手肘和肩膀一样高，已经足够了，再往上不但没有什么效果，还会导致借力，并且容易造成关节损伤！ 用坐姿来做哑铃侧平举。站姿做哑铃侧平举，更容易让斜方肌借力。 做哑铃侧平举的时候，沉肩收缩肩胛骨，然后抬起头来，看着斜上方。 上斜哑铃卧推 主要肌肉群 : 胸肌 其他肌肉 :肱三头肌、 肩部 斜板角度: 35-45度, 如果太大就会使三角肌承受过多的压力, 影响锻炼效果 组数: 4*8 动作要领 坐在上斜凳上，双手各持一只哑铃，将其放在大腿上，掌心彼此相对。 然后，用大腿将哑铃举起来，每次举起一只，将其放到胸前，掌心向前，与肩同宽。同时向后躺到靠背上，挺胸收腹。这是动作的起始位置。 然后用胸部力量将哑铃推举起来，同时呼气，记住全程要保持对哑铃的掌控力度，并且挺胸收腹。 手臂自然伸直，到达顶点的时候稍适停留，然后开始缓慢下降，下降的时候吸气。理想情况下，下降所花费的时间应为举起所花费时间的两倍。 以上是一次完整动作，重复动作至推荐次数。 完成训练后，将哑铃放回腿部，然后再放到地板上。这是安全放置哑铃的方式。 站姿哑铃肩上推举站姿哑铃推举也叫直立推举，是一个锻炼上肢的复合动作。 锻炼部位：主要要锻炼肩部三角肌和肱三头肌！ 组数: 4*8 动作要领 选择合适的重量，两手稍宽于肩，双手握住哑铃，哑铃起始位置可以在最顶端也可以在胸前肩上方 背部保持挺直，腹部收紧，集中三角肌前束力量，尽量少借助身体其他部位的辅助，向上推举哑铃至双臂完全伸直，使哑铃停于头上方，约1秒钟后，缓慢放落哑铃于准备姿势的胸前肩高位置。 动作技巧 1.动作过程中切忌身体摇晃、摆动，更不得借力上推。为杜绝借力， 2.你也可以采用坐姿进行。坐姿更稳定，也更集中的锻炼三角肌 3.也可以采用杠铃来完成站姿推举， 4.你还可以交替单臂进行推举 哑铃锤式弯举哑铃锤式弯举（Hammer Curl），和哑铃弯举（Dumbbell Curl）区别在于：弯举不外旋手臂，集中锻炼二头肌的外侧。哑铃锤式弯举一般也是交替进行，即交替哑铃侧弯举（Alternating Hammer Curl） 主要锻炼部位：主要是锻炼肱二头肌外侧 组数: 3*10 动作要领: 1.立姿（或坐姿），手持哑铃垂于体侧，掌心相对，上臂紧贴体侧，肘关节是惟一运动的关节。 2.用力向上弯举，可感受到肱二头肌外侧膨胀隆起。最高点进行顶峰收缩，并坚持片刻，然后缓慢还原，最低点时手臂完全伸直。 3.做完一侧换侧再做。为避免动作过程中身体借力，躯干可稍前倾。 注意事项： 1.注意与哑铃弯举（如下图）的区别：哑铃锤式弯举并不外旋手臂，始终保持掌心相对 哑铃交替弯举 主要肌肉群 : 肱二头肌, 肱肌 组数: 3*10 动作要领: 保持大臂垂直地面并且贴近身体，将哑铃放在体前，稍停片刻，这将是你的起始位置。 保持肘关节和肩关节稳定，掌心朝上，交替将哑铃弯举到最高点。 起始姿势处于手臂接近伸直(二头肌拉长的阶段)，左手完成一次弯举，然后交替右手，右手交替进行动作。 注意事项： 1.交替弯举时，尽量避免身体前后晃悠，容易造成腰部受伤，尤其冲击大重量时，有必要带上腰带进行必要保护。 2.向上弯举时，前臂外旋掌心朝上，否则就变成主要锻炼二头肌外侧的哑铃侧弯举(哑铃锤式弯举)这个动作了. 详细讲解 斜上哑铃推举 主要训练部位: 上胸部，三角肌前束 角度：把斜板的角度设置在30度，以达到刺激胸肌的较好姿势。大于30度，就会使重量过多地作用在三角肌前束上。 组数: 约(3-4)*(12-15) 动作要领:上举时应遵循三角形的运动轨迹，哑铃应在胸肩上方身体中线处相遇。哑铃几乎要互相接触的位置，进行顶峰收缩动作要平缓流畅，特别关注对胸肌的挤压，还原时动作要慢。 技巧： 1.向上推举哑铃时，要试着让哑铃轻轻地“漂上去”，好像不受意识的支配。这样胸肌必须立即开始工作，以防哑铃坠落。 .2下放哑铃时要注意控制速度，以缓慢而稳定为佳。但不要停留，增加训练的密度，当举到最高点时，就立即将杠铃下放，保持动作的流畅。 3.在整个推举动作过程中始终保持胸部紧张。想象你的胸肌在推举哑铃，而不是你的臂三头肌或肩三角肌，尽管在动作中的确需要少许借助这两个肌肉群。 俯身哑铃划船 俯身哑铃划船是锻炼中背部最好的动作! 主要锻炼：中背部，还会锻炼肱二头肌、肩部。 动作要领: 双手各握一支哑铃。膝盖略微，身体向前倾，弯腰，背挺直，让身体几乎和地板平行。提示：保持抬头正视前方。 握住哑铃的双臂自然下垂，与地面和身体保持垂直。这是动作的起始位置。 身躯保持固定，将哑铃拉至身体两侧，肘部贴紧身体。 在收缩动作最高点，收紧背部肌肉，保持1秒。然后缓慢地将哑铃放下还原至起始位置。 注意事项： 1.保证动作的标准，背部一定不能弯，否则会造成背部损伤。 2.要注意使用的哑铃重量，不确定请使用较轻的哑铃。 3.呼吸方法：哑铃拉至身体两侧时吐气，回到起始吸气。 俯身哑铃飞鸟哑铃俯身飞鸟是锻炼后背的经典动作，对增加后背宽度有很好的效果，同时对背阔肌、斜方肌和三角肌后束也有明显的刺激作用。 主要锻炼部位: 三角肌后束和上背肌群 组数: 3* 10 动作要领: 两脚分开站立同肩宽，持哑铃，上体向前屈体至与地面平行，两腿稍屈，使下背部没有拉紧感。 两手持铃向两侧举起，直至上臂与背部平行（或略为超过）,与北部呈T型，稍停，然后放下哑铃还原。重复做。 提肘：即先要提起和外张两个手肘来起动；如果在持铃向两侧举起时，使肘和腕部稍微弯屈，你会感到能使三角肌群获到更好的收缩. 控肩：双肩发力控制上举； 挥腕：随着肩肘的动势，向侧挥腕做大幅度举起。肩肘腕三关节要伸中有缩，直中有曲，放中有收，发劲含蕴。在整个动作过程中，思想要集中在目标收缩的肌肉群上不要借力。开臂时吸气，合臂时呼气。 动作技巧: 哑铃返回时，手臂保持微曲，不要放松背部。 不要用惯性提起哑铃。 拉起哑铃吸气，放下哑铃呼气。 详细讲解 固定器械 | 无器械绳索下压 主要训练部位: 三头肌 身体姿态: 如果是大重量身体可适当前倾,后者双脚一前一后,微屈髋关节. 含胸-避免胸肌发力, 新手采用正握, 后期慢慢换成反握刺激效果更好 组数: 3*10 动作技巧 并脚站立，膝盖微曲，挺胸收腹，腰杆挺直，身体略向前倾，上臂夹紧肋部并保持不动。 抓紧绳索，腕关节放松，肩胛骨下沉，双手握住绳索根部，慢慢向下压至双臂将近伸直，然后向两侧分开至体侧，同时前臂内旋，把绳索拉直。稍微停顿紧缩1-2秒，缓慢还原。 注意事项 在做绳索下压动作时，上身千万不要晃动，要完全靠三头肌力量下压绳索，而不能靠身体重力拉伸。 肘关节要始终夹紧肋部(上臂固定)，不能前后移动。只是前臂运动 下压至胳膊将近伸直时，要马上旋转前臂，让肱三头肌外侧极度收紧，增加训练效果。 也可以改变肘的方向 比如向两侧打开做下压来侧重三头肌外侧头 绳索飞鸟高位绳索飞鸟 主要训练部位: 下胸 组数 4*12 重量: 不必追求大重量 高位滑索飞鸟是一个下胸训练动作，一个挺难掌握好的一个动作。 动作要领 立于拉力器架中央，调节好拉索长度，挺胸！身体略前倾45度，弓步支撑，双手持环，微微屈肘，手臂向身前向下伸展，手掌相对。 打开时注意控制动作，感受胸肌被拉伸，合拢时尽力挤压胸肌，略做停顿进行顶峰收缩。不要为了拉起更大的重量，身体前倾得厉害。此动作的要点是使胸部肌肉较充分拉伸与尽可能挤压，采用的重量是次要的。 当双手打开时，手肘应保持轻微屈曲并指向滑索末端的滚轴方向。 最常见的毛病 手肘升起时慢慢由指向上方转到后方，以肩内旋肌肉作代偿。 很多初学者会把力量集中手柄位置不断用力往下推，以为推得离开胸部越远越好。然而当肩部不能稳定时，代偿令肩推前形成圆肩状态，令肩关节压力增加，亦大大限制了胸大肌的使用。 平行绳索飞鸟 主要训练部位: 中胸 组数: 4*12 重量: 适中, 不要追求大重量, 易受伤 动作要领 把拉力器调整到最平行于肩部！立于拉力器架中央，调节好拉索长度，挺胸！身体略前倾45度，弓步支撑，双手持环，微微屈肘，手臂向身前向下伸展，手掌相对。 打开时注意控制动作，感受胸肌被拉伸，合拢时尽力挤压胸肌，略做停顿进行顶峰收缩。不要为了拉起更大的重量，身体前倾得厉害。此动作的要点是使胸部肌肉较充分拉伸与尽可能挤压，采用的重量是次要的。 低位绳索飞鸟 主要训练部位: 上胸 组数 4*12 重量: 适中 动作要领 把拉力器调整到最低端，立于拉力器架中央，调节好拉索长度，挺胸。身体略前倾45度，弓步支撑，双手持环，微微屈肘，手臂向身前向下伸展，手掌相对。 打开时注意控制动作，感受胸肌被拉伸，合拢时尽力挤压胸肌，略做停顿进行顶峰收缩。不要为了拉起更大的重量，身体前倾得厉害。此动作的要点是使胸部肌肉较充分拉伸与尽可能挤压，采用的重量是次要的。 高位下拉 主要训练: 背阔肌 组数: 3*10 动作要领: 在我们做高位下拉的过程中，要想让背部发力有感觉，首先要做到肩部下沉这个动作。沉肩，就是要我们的脖子伸出来，肩膀往下压。 在下拉的时候，一定要做到先沉肩后下拉，顺序不要搞乱，否则就会造成严重的手臂代偿，影响背部训练效果。 具体下拉幅度能有多大，就要看我们的关节灵活程度，有些人不一定非得拉到锁骨位置，拉到鼻子位置也可以。还有些人在负载大重量的时候，下拉幅度也会有所减少，可能只能拉到额头位置，但这就够了，对背部的训练效果依旧，如果强行下拉的话，可能会造成腰部受伤。 动作技巧:正确的发力顺序应该是：起动前应先做好肩胛骨下沉→然后大臂内收(伸展)→顺势屈肘将负重拉起。 详细讲解1 详细讲解2 引体向上 主要训练部位: 背部 组数: 约(3-4)*(12-15) 根据自身情况选择合适的引体向上动作-循序渐进 引体向上一上来就是自重，一开始的难度是很大的，就像卧推一样，你一个70KG的人，一上来就想卧推70KG，谈何容易？很多人得努力锻炼个一年半载才能卧推自重呢！下面几个动作，从最简单，到最艰难，你按照我说的去做，绝对可以很快就轻松做起来引体向上。引体向上是复合动作！握力不强，肱桡肌不强，肱二头肌不强，都会严重影响引体向上！很多做不了一两个引体向上的人，其实不是背部没有肌肉，而是握力和肱二头肌太弱，同时并不会使用背部肌肉发力！ 反向划船反向划船本身也分难易……杆子离地面越高，你双脚和身体与地面的角度越大，反向划船也就越简单越容易；杆子离地面越低，你双脚和身体与地面的角度越小，反向划船也就会越艰难……像下图这种杠铃拉触腹部的难度也挺大（一般是触胸）。 练一段时间反向划船，直到你将杆子下降到很低，做起来依旧感觉很轻松，这表示你双臂的力量和背部肌群的力量，已经增强了很多，就可以开始使用 对握引体向上了 对握引体向上对握引体向上，也就是双手掌心相对，各抓着一根杆子做引体向上。主要是练中下背的，差不多是各种引体向上里面最容易的（比反握引体向上还要容易）。所以当你正握引体向上还做不了几个的时候，你必须要练这个动作。 正握引体向上 握距: 与肩同宽 你练了一段时间反向划船之后，再做对握引体向上，你肯定能够轻松做上几个，并且进步非常明显。 反握引体向上反握引体向上的难度，要比对握引体向上大一点，所以我将反握引体向上放在了对握引体向上之后。 动作技巧 从易到难，当你对握引体向上能做六七个之后，还要继续练习对握引体向上，但是在练对握引体向上的同时，你可以开始尝试反握引体向上。 背部肌肉的恢复时间，本身就要比其他一些肌群久一点，第一组和第二组数量差距很大，这不是你的问题，其他人也都差不多。拉不上去了你就练反握引体向上啊，反握引体向上做了两三组又拉不动了，那你就做对握引体向上，你会发现，咦，对握引体向上还能做几个！ 详细讲解1 详细讲解2 深蹲 主要训练: 臀部集群 组数: 4*12 徒手深蹲 双手前平举有助于维持身体的平衡，可以帮助我们在大腿后侧肌肉链条控制能力不足的情况下蹲得下去。 双脚打开与肩同宽，双脚外八分开，腰背挺直，核心收紧，双臂下垂臀部向后坐并屈膝下蹲至大腿与地面平行或者稍低后起身还原下蹲时双臂前平举，起身时双臂还原。 宽距手触地深蹲 宽距深蹲会在动作过程中注重内收肌的锻炼，同时还可以增高对于臀大肌的刺激。 双脚打开约两倍肩宽，双脚呈外八状，腰背挺直，核心收紧，双臂垂于体前臀部后移屈膝下蹲至大腿与地面平行后起身下蹲时双臂随之向下使双手尽量接触地面。 开合跳深蹲 跳跃的形式不但可以有效锻炼臀腿，还会锻炼到爆发力与身体稳定性。 双腿微微分开，腰背挺直，核心收紧，双臂下垂臀部后移下蹲至大腿与地面平行后起身，起身的同时向上跳起，同时双腿向外打开以约两倍肩宽的站距落地然后了屈膝下蹲至大腿与地面平行后起身并跳起，跳起时双腿向内收回。 抱头深蹲 抱头动作可以增加身体的不稳定性，需要更多地调动核心力量，并且起立过程中大腿后侧及内侧参与伸髋的肌肉能更好发力，让髋关节能充分打开。 双脚打开与肩同宽，腰背挺直，挺胸收腹臀部向后坐屈膝下蹲至大腿与地面平行后起身还原。 90度转体深蹲跳 在深蹲动作过程中加入转体跳跃动作，可以在有效锻炼臀腿的同时锻炼爆发力以及身体的平衡性、协调性与稳定性。 双脚打开与肩同宽站立，挺胸收腹，腰背挺直，双臂下垂臀部向后坐屈膝下蹲至大腿与地面平行后起身，起身的同时向上跳起并向一侧转体90度双脚落地后再次下蹲，起身时跳回还原 并腿深蹲 并腿深蹲可以更有针对性地锻炼股四头肌，但难度比较大，同时膝关节有伤的朋友应该避免这个动作。 双腿并拢站立，腰背挺直，核心收紧，双臂下垂臀部后移下蹲至自己动作顶点后起身还原下蹲时双臂前平举，起身时双臂还原 侧弓步 侧蹲可以有效锻炼股四头肌、腘绳肌与臀部肌群，同时还会锻炼到内收肌。 双腿分开站立，背部挺直，核心收紧，双手叉腰重心向身体一侧移动，同时弯曲这一侧膝盖至大腿与地面平行，另一侧腿伸直后起身还原还原后重心向另一侧移动并下蹲 左右平移深蹲 在标准深蹲的基础上加入身体的其他动作，看起来在难度上没有什么变化，但是可以更有效地锻炼身体的协调性。 双腿打开比肩略宽，腰背挺直，核心收紧，双臂自然下垂臀部向后坐下蹲至大腿与地面平行后起身起身的同时一条腿向内收回，另一条腿向另一侧迈开后再次下蹲下蹲时双臂前平举，起身时双臂还原 深蹲注意事项 在动作过程中，要全程保持腰背挺直； 注意屈髋动作，也就是臀部向后坐，先屈髋再屈膝下蹲； 不要过于纠结膝盖要不要超过脚尖，但要注意膝盖与脚尖方向一致 起身过程中，虽然双腿伸直，但膝关节不但锁死 以上动作如果可以整组进行的话，每一个动作12-15次，动作间休息45秒左右，每次做2-3组，但动作前后的热身与拉伸不能被省略。 腿举深蹲由于下蹲时腰部压力大，而斜卧负重腿举则可避免这一不足，因此可用来冲击大重量。仰卧角度一般不小于45度，如果偏小阻力损耗较大，这时应采用水平蹬腿。现在45度斜卧腿举机是主流，当然也有90度仰卧推举。 主要锻炼部位：股四头肌 辅助锻炼：股四头肌、臀大肌（股四头肌中尤其是锻炼内侧头、其次外侧头，而股直肌基本不参与） 动作要领 坐在腿举器上，将你的髋部靠住斜垫，并把你的双脚以肩宽的距离踏在脚台上。抓住手柄并从脚跟发力来释放安全栓。在动作的开始时你的膝关节应该微弯。 吸气并慢慢降低负重，直到膝关节成90度时停止动作。稍作停留，然后通过你的脚后跟强力地向上推举重量返回至初始位置，当向上推举通过动作中点时开始呼气。 注意事项： 1、练习时不应养用推膝借力或双臂交叉胸前限制动作的距离等不良习惯。此外，动作还原时两脚不要完全伸直，膝关节呈微屈状态。 2、腿举重量必须大大高于深蹲，才能真正达到效果。腿举训练量也必须高于其他项目，训练量标准是：直到完全移动不了器械为止。因为腿举是固定运动轨迹的项目，难以刺激肌肉深层，而且腿举非常安全。 3、健美爱好者一般开始就认真练习深蹲和腿举两个动作。千万不要认为：深蹲已经足够了，不需要腿举了。认真练习腿举，不仅能增大你的股四头肌，还能增大你全身各部位的肌肉。练此动作之前最好先做深蹲，直至腰感到不能承受重压时再腿举。 腿弯举把腘绳肌伺候好了，能大大减少膝盖伤病，尤其是前十字韧带拉伤。即便是运动健将，大多数也是股四头肌更壮，前后肌群发展不平衡，很容易受伤 主要训练部位: 腘绳肌（大腿后侧肌群) 组数: 4*(10-12) 动作要领 根据身高调整器械，俯卧在腿弯举器上，膝盖正好刚刚超过俯卧板的末端。调整阻力滚垫，使脚踝后面正好卡在滚垫下。抓住手柄并深吸气, 绷直脚尖。 保持躯干平直，确保双腿完全伸展，呼气的同时收缩股二头肌使滚垫朝臀部运动，但不要把大腿抬起离开垫子，当动作到达终点时，在动作的顶端，努力挤压股二头肌，停顿两秒。 再吸气时将腿慢慢反向返回至初始位置。 注意事项 1、弯举时只用腿部力量，防止用腹部力量借力。 2、股二头肌收缩用力时臀部不可抬起，避免借力。如出现这一情形说明负重过重，应减轻弯举重量，意念要集中在主动肌的收缩和伸展上。 3、勾起重量时小腿不宜超过垂直面，还原时股二头肌要用力控制，两腿不完全伸直，保持张紧状态，动作过程不能靠惯性。如出现这一情形说明负重过轻，应适当增加试举重量，并注意控制动作节奏，如向心收缩稍快，离心收缩稍慢。 腿屈伸 主要锻炼部位: 小腿后侧集群 组数: 4*(12-15) 动作要领 选择好重量后，坐在腿部伸展器上，脚踝在垫子后面，脚面紧贴脚踝垫，钩脚尖，坐直，臀部、下背部紧靠座垫，头抬起，挺胸、收腹，面向前，握着座椅边沿或握柄。这是动作的起始位置。 呼气的同时，用你的股四头肌，全范围地缓慢伸展你的膝关节，最大限度伸展双腿。确保你身体的其他部分在座位上保持不动。在肌肉紧张的位置保持1秒。 3. 吸气的同时，慢慢将重量降低至最初的位置，确保在小腿之前不要超过90度夹角的限制。 重复动作至推荐的重复次数。 动作技巧 坐下后要让背和靠椅紧密接触，并且大腿的下侧也是如此，尤其是靠近膝盖的部分，不要让其离开了座椅，这样股四才能收紧。 膝盖要和器械的轴心，就是转动部分的中心，他们二者要在一条线上，如果不是这样的话，你会发现转动起来会很吃力，并且容易对膝盖形成较大的伤害。 做的时候全身保持绷紧，才能让它受到最有效的刺激，如果你身体是疲软的状态，抬腿会具有难度，你做几次就会不想做了，为了保持泵感，绷紧是很重要的。 在抬起小腿之后，让转轴回到原位的过程中，你需要控制它下落，这会为你制造更大的张力，而如果自由下落的话，不仅没有张力还容易受伤。 站姿提臀 主要肌肉群 : 小腿肌群 组数: 4*(12-15) 动作要领 身体直立，双手各持一只哑铃，手臂在身体两侧自然下垂。双脚前脚掌跖骨踩在一个高约5-8公分的固定物上，脚跟落地。这是动作的起始位置。 脚跟离开地面，尽可能的向上踮起脚尖，同时呼气。在顶部停留1秒，感受小腿肌肉的紧绷。要注意的是，双脚脚尖向前，锻炼部位相对均匀；脚尖向内，锻炼重点是小腿外侧；脚尖向外，锻炼重点是小腿内侧。 最后慢慢降低脚跟，回到起始位置，同时吸气。 以上是一次完整动作，重复动作至推荐次数。 注意事项： 1.注意完成动作时不要屈膝、屈体。 2.控制重心不要有意前移，否则效果极差，可在前脚掌下垫一块铃片防止重心前移。 3.脚尖向内扣站法侧重于锻炼腓肠肌的内侧头，而普通练法内外侧都能练到。 健身饮食主要食物种类 个人认为一天吃2-4个全蛋并没有什么问题，没必要只吃蛋清，要知道鸡蛋的营养成分大部分都在蛋黄当中，如果一个不吃就太浪费了。 饮食原则：少量多餐增肌训练者应该遵循少量多餐的饮食原则，与我们日常的一日三餐不同，增肌训练者一天应该至少有六次的食物摄入，有证据表明，每天六餐或者六餐以上，身体能够更有效的吸收消化食物，也就使身体摄入更多的营养物质，在促进身体肌肉合成，减少脂肪等方面比多吃少餐的效果要好。当然，这里说的多餐也不是毫无节制的吃，什么都能吃，还是要遵循科学的饮食原则，避免摄入高热量的食物。 总之减脂餐食谱一日三餐的安排是低脂碳水的，不要完全排除不吃脂肪和碳水这样是不科学的规划自己饮食，保持平常心加强运动很快你就能看到自己健身减肥自律的效果了 健身饮食注意事项增肌训练的饮食除了保证全天碳水，蛋白质，水果，蔬菜的摄入以外，还要注意以下几点： 1、运动前一定要补充一点碳水，但也不用太多，比如吃两片面包片，吃个苹果都可以，空腹运动除了会使血糖降低，引起眩晕外，还会由于机体缺乏足够的碳水，导致蛋白质供能，引起肌肉的流失， 使锻炼效果大大折扣。 2、增肌饮食建议控制脂肪摄入，但不是不吃，可以吃一点坚果类食品，但不要太多。 3、运动中，一定要补充水分，最好准备些淡盐水，不要喝各种碳酸饮料。 4、增肌训练中，各种补给品，比如蛋白粉，增肌粉，肌酸，氮泵等，不是必需的，应该根据自身情况进行选择。 5、训练后，建议30分钟内补充一定的蛋白质，此时的吸收效率比较高，单次蛋白质摄入不要超过30g。 早餐：1.水果麦片+冻牛奶+鸡蛋+生菜 2.吐司+香蕉+培根+西蓝花 3.面包+煎蛋+鸡肉+生菜 4.玉米+牛奶+虾仁+紫甘蓝 5.番薯+水煮蛋+牛奶+生菜 6.八宝粥+牛奶+椰菜花 7.欧包+酸奶+生菜 8.荞麦面+西红柿+荷包蛋 9.小米粥+菜圃+牛肉 10.山药+红薯+鳕鱼 午饭或晚餐：\\1. 米饭+青菜+真菌烤鸡肉 \\2. 土豆+苹果+莴笋炒鸡肉 \\3. 意大利面+胡萝卜牛肉+芦笋+蘑菇 \\4. 通心粉+西红柿炒牛肉+魔芋 \\5. 荞麦面+芦笋+秋葵炒牛肉 \\6. 米饭+鱼类+大白菜+茄子 \\7. 紫薯+黄豆炒肉+荷兰豆炒牛肉 \\8. 八宝粥+红烧牛肉+生菜 \\9. 白薯米饭南瓜粥+胡萝卜炒鸡肉 \\10. 南瓜+河粉+白萝卜炒牛肉或鸡肉 早中晚加餐：\\1. 酸奶一杯 \\2. 白薯一条 \\3. 玉米一根 \\4. 无糖馒头一个 \\5. 鸡肉100g \\6. 坚果（核桃，花生，夏威夷果）20g \\7. 香蕉，浆果类水果50g \\8. 南瓜100-200g \\9. 青瓜黄瓜一根 \\10. 土豆一小个","categories":[{"name":"健身","slug":"健身","permalink":"/categories/%E5%81%A5%E8%BA%AB/"}],"tags":[]},{"title":"小白快速上手DockerFile","slug":"小白快速上手DockerFile","date":"2019-12-04T00:00:00.000Z","updated":"2019-12-04T07:05:04.000Z","comments":true,"path":"2019/12/04/小白快速上手DockerFile/","link":"","permalink":"/2019/12/04/%E5%B0%8F%E7%99%BD%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BDockerFile/","excerpt":"","text":"小白快速上手DockerFile什么是dockerfile?Dockerfile是一个包含用于组合映像的命令的文本文档。可以使用在命令行中调用任何命令。 Docker通过读取Dockerfile中的指令自动生成映像。 docker build命令用于从Dockerfile构建映像。可以在docker build命令中使用-f标志指向文件系统中任何位置的Dockerfile。 例： 1docker build -f /path/to/a/Dockerfile Dockerfile的基本结构Dockerfile 一般分为四部分：基础镜像信息、维护者信息、镜像操作指令和容器启动时执行指令，’#’ 为 Dockerfile 中的注释。 Dockerfile文件说明Docker以从上到下的顺序运行Dockerfile的指令。为了指定基本映像，第一条指令必须是 FROM。一个声明以＃字符开头则被视为注释。可以在Docker文件中使用RUN，CMD，FROM，EXPOSE，ENV等指令。 用一张图解释常用指令的意义^-^ 常用指令在这里列出了一些常用的指令。 FROM制定基础镜像, 必须为第一个命令 12345678格式： FROM &lt;image&gt; FROM &lt;image&gt;:&lt;tag&gt; FROM &lt;image&gt;@&lt;digest&gt;示例： FROM mysql:5.6注： tag或digest是可选的，如果不使用这两个值时，会使用latest版本的基础镜像 MAINTAINER维护者信息 123456格式： MAINTAINER &lt;name&gt;示例： MAINTAINER Jasper Xu MAINTAINER sorex@163.com MAINTAINER Jasper Xu &lt;sorex@163.com&gt; RUN 构建镜像时执行的命令 12345678910111213RUN用于在镜像容器中执行命令，其有以下两种命令执行方式：shell执行格式： RUN &lt;command&gt;exec执行格式： RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]示例： RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] RUN apk update RUN [&quot;/etc/execfile&quot;, &quot;arg1&quot;, &quot;arg1&quot;]注： RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache ADD将本地文件添加到容器中，tar类型文件会自动解压(网络压缩资源不会被解压)，可以访问网络资源，类似wget 12345678格式： ADD &lt;src&gt;... &lt;dest&gt; ADD [&quot;&lt;src&gt;&quot;,... &quot;&lt;dest&gt;&quot;] 用于支持包含空格的路径示例： ADD hom* /mydir/ # 添加所有以&quot;hom&quot;开头的文件 ADD hom?.txt /mydir/ # ? 替代一个单字符,例如：&quot;home.txt&quot; ADD test relativeDir/ # 添加 &quot;test&quot; 到 `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # 添加 &quot;test&quot; 到 /absoluteDir/ COPY功能类似ADD，但是是不会自动解压文件，也不能访问网络资源** CMD构建容器后调用，也就是在容器启动时才进行调用。 1234567格式： CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (执行可执行文件，优先) CMD [&quot;param1&quot;,&quot;param2&quot;] (设置了ENTRYPOINT，则直接调用ENTRYPOINT添加参数) CMD command param1 param2 (执行shell内部命令)示例： CMD echo &quot;This is a test.&quot; | wc - CMD [&quot;/usr/bin/wc&quot;,&quot;--help&quot;]注： CMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令。 当有多个CMD的时候，只有最后一个生效。 ENTRYPOINT配置容器，使其可执行化。配合CMD可省去”application”，只使用参数。 1234567格式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (可执行文件, 优先) ENTRYPOINT command param1 param2 (shell内部命令)示例： FROM ubuntu ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;] CMD [&quot;-c&quot;]注： ENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT，而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令。 一定要注意！ CMD和ENTRYPOINT同样作为容器启动时执行的命令，区别有以下几点： CMD的命令会被 docker run 的命令覆盖而ENTRYPOINT不会 如使用CMD [&quot;/bin/bash&quot;]或ENTRYPOINT [&quot;/bin/bash&quot;]后，再使用docker run -ti image启动容器，它会自动进入容器内部的交互终端，如同使用 docker run -ti image /bin/bash。 但是如果启动镜像的命令为docker run -ti image /bin/ps，使用CMD后面的命令就会被覆盖转而执行bin/ps命令，而ENTRYPOINT的则不会，而是会把docker run 后面的命令当做ENTRYPOINT执行命令的参数。 以下例子比较容易理解 Dockerfile中为 1ENTRYPOINT [\"/user/sbin/nginx\"] 然后通过启动build之后的容器 1docker run -ti image -g \"daemon off\" 此时-g &quot;daemon off&quot;会被当成参数传递给ENTRYPOINT，最终的命令变成了 1/user/sbin/nginx -g \"daemon off\" CMD和ENTRYPOINT都存在时 CMD和ENTRYPOINT都存在时，CMD的指令变成了ENTRYPOINT的参数，并且此CMD提供的参数会被 docker run 后面的命令覆盖，如： 123...ENTRYPOINT [\"echo\",\"hello\",\"i am\"]CMD [\"docker\"] 之后启动构建之后的容器 使用docker run -ti image 输出“hello i am docker” 使用docker run -ti image world 输出“hello i am world” 指令比较多，可以通过分类(如开头的表格)的办法去记忆 LABEL用于为镜像添加元数据 123456格式： LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...示例： LABEL version=&quot;1.0&quot; description=&quot;这是一个Web服务器&quot; by=&quot;IT笔录&quot;注： 使用LABEL指定元数据时，一条LABEL指定可以指定一或多条元数据，指定多条元数据时不同元数据之间通过空格分隔。推荐将所有的元数据通过一条LABEL指令指定，以免生成过多的中间镜像。 ENV设置环境变量 1234567格式： ENV &lt;key&gt; &lt;value&gt; #&lt;key&gt;之后的所有内容均会被视为其&lt;value&gt;的组成部分，因此，一次只能设置一个变量 ENV &lt;key&gt;=&lt;value&gt; ... #可以设置多个变量，每个变量为一个&quot;&lt;key&gt;=&lt;value&gt;&quot;的键值对，如果&lt;key&gt;中包含空格，可以使用\\来进行转义，也可以通过&quot;&quot;来进行标示；另外，反斜线也可以用于续行示例： ENV myName John Doe ENV myDog Rex The Dog ENV myCat=fluffy EXPOSE指定于外界交互的端口 12345格式： EXPOSE &lt;port&gt; [&lt;port&gt;...]示例： EXPOSE 80 443 EXPOSE 8080 EXPOSE 11211/tcp 11211/udp注： EXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口 VOLUME用于指定持久化目录 12345678910格式： VOLUME [&quot;/path/to/dir&quot;]示例： VOLUME [&quot;/data&quot;] VOLUME [&quot;/var/www&quot;, &quot;/var/log/apache2&quot;, &quot;/etc/apache2&quot;注： 一个卷可以存在于一个或多个容器的指定目录，该目录可以绕过联合文件系统，并具有以下功能：1 卷可以容器间共享和重用2 容器并不一定要和其它容器共享卷3 修改卷后会立即生效4 对卷的修改不会对镜像产生影响5 卷会一直存在，直到没有任何容器在使用它 WORKDIR工作目录，类似于cd命令 123456格式： WORKDIR /path/to/workdir示例： WORKDIR /a (这时工作目录为/a) WORKDIR b (这时工作目录为/a/b) WORKDIR c (这时工作目录为/a/b/c)注： 通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行。在使用docker run运行容器时，可以通过-w参数覆盖构建时所设置的工作目录。 USER指定运行容器时的用户名或 UID，后续的 RUN 也会使用指定用户。使用USER指定用户时，可以使用用户名、UID或GID，或是两者的组合。当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户 12345678910格式: USER user USER user:group USER uid USER uid:gid USER user:gid USER uid:group示例： USER www 注： 使用USER指定用户后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT都将使用该用户。镜像构建完成后，通过docker run运行容器时，可以通过-u参数来覆盖所指定的用户。 ARG用于指定传递给构建运行时的变量 12345格式： ARG &lt;name&gt;[=&lt;default value&gt;]示例： ARG site ARG build_user=www ONBUILD用于设置镜像触发器 12345格式： ONBUILD [INSTRUCTION]示例： ONBUILD ADD . /app/src ONBUILD RUN /usr/local/bin/python-build --dir /app/src注： 当所构建的镜像被用做其它镜像的基础镜像，该镜像中的触发器将会被钥触发 案例配置nginx 案例12345678910111213141516171819202122232425262728293031323334353637383940414243444546# This my first nginx Dockerfile# Version 1.0# Base images 基础镜像FROM centos#MAINTAINER 维护者信息MAINTAINER yeung #ENV 设置环境变量ENV PATH /usr/local/nginx/sbin:$PATH#ADD 文件放在当前目录下，拷过去会自动解压ADD nginx-1.8.0.tar.gz /usr/local/ ADD epel-release-latest-7.noarch.rpm /usr/local/ #RUN 执行以下命令# 先更换掉源 安装软件更快WORKDIR /etc/yum.repos.d/RUN mv CentOS-Base.repo CentOS-Base.repo_bak# 获取国内yum源（阿里、163二选一）RUN wget -O CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# wget -O CentOS-Base.repo http://mirrors.163.com/.help/CentOS7-Base-163.repo# 清理yum缓存RUN yum clean all# 重建缓存RUN yum makecache# 升级Linux系统RUN yum -y update#安装自定义软件RUN rpm -ivh /usr/local/epel-release-latest-7.noarch.rpmRUN yum install vim -y wget lftp gcc gcc-c++ make openssl-devel pcre-devel pcre &amp;&amp; yum clean allRUN useradd -s /sbin/nologin -M www#WORKDIR 相当于cdWORKDIR /usr/local/nginx-1.8.0 RUN ./configure --prefix=/usr/local/nginx --user=www --group=www --with-http_ssl_module --with-pcre &amp;&amp; make &amp;&amp; make installRUN echo \"daemon off;\" &gt;&gt; /etc/nginx.conf#EXPOSE 映射端口EXPOSE 80#CMD 运行以下命令CMD [\"nginx\", \"-g\", \"daemon off;\"] Ubuntu案例1234567891011FROM ubuntuMAINTAINER yeung xxxx@qq.comWORKDIR /usr/local/dockerADD temp.zip ./add/COPY temp.zip ./copy/EXPOSE 22RUN groupadd -r yeung &amp;&amp; useradd -r -g yeung yeungUSER yeungENTRYPOINT [&quot;/bin/bash&quot;]","categories":[{"name":"docker","slug":"docker","permalink":"/categories/docker/"}],"tags":[{"name":"dockerfile","slug":"dockerfile","permalink":"/tags/dockerfile/"}]},{"title":"Linux 高频文件操作","slug":"linux-高频文件操作","date":"2019-12-04T00:00:00.000Z","updated":"2019-12-04T07:56:00.000Z","comments":true,"path":"2019/12/04/linux-高频文件操作/","link":"","permalink":"/2019/12/04/linux-%E9%AB%98%E9%A2%91%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/","excerpt":"","text":"Linux 高频文件操作原文链接：https://blog.csdn.net/gexiaoyizhimei/article/details/100122368 0.新建操作：12mkdir abc #新建一个文件夹touch abc.sh #新建一个文件 1.查看操作查看目录：123ll #显示目录文件详细信息du -h 文件/目录 #查看大小pwd #显示路径 查看文件内容：cat|head|tail命令123456789101112cat abc.txt #查看abc的内容head -5 abc.txt #查看abc前5行内容。默认是10行tail [选项] 文件名各选项的含义如下：+num：从第num行以后开始显示-num：从距文件尾num行处开始显示。如果省略num参数，系统默认值为10.-f: 循环读取,例如查看服务器日志时，可以实时观察#filename 文件里的最尾部的内容显示在屏幕上，并且不断刷新。tail -f filename #查看最后20行tail -f filename more命令：12345more命令一次显示一屏信息，若信息未显示完屏幕底部将出现“-More-（xx%）”。此时按Space键，可显示下一屏内容；按“回车”键，显示下一行内容；按B键，显示上一屏；按Q键，可退出more命令。 less命令：和more命令类似，但是比more命令更强大。在很多时候，必须使用less,比如管道。例如： 1ll /etc | less stat 命令：查看文件的详细信息，比如创建修改时间，大小等 12345678910[root@localhost zx]# stat index.html 文件：&quot;index.html&quot; 大小：29006 块：64 IO 块：4096 普通文件设备：fd00h/64768d Inode：17589607 硬链接：1权限：(0644/-rw-r--r--) Uid：( 0/ root) Gid：( 0/ root)环境：unconfined_u:object_r:home_root_t:s0最近访问：2019-09-02 21:47:41.824053666 +0800最近更改：2019-09-02 21:44:33.588587500 +0800最近改动：2019-09-02 21:44:33.588587500 +0800创建时间：- du 命令：选项：-h 以合适的单位显示（会根据文件的大小自动选择kb或M等单位） 12[root@localhost zx]# du -h index.html 32K index.html 2.删除操作12rm -f aa.txt #强制删除aa.txtrm -rf fileDir #强制删除fileDir文件夹和里边的所有文件 3.复制操作同一机器的复制：123cp:复制文件或目录语法：cp [options] source dest 123456789-a：此选项通常在复制目录时使用，它保留链接、文件属性，并复制目录下的所有内容。其作用等于dpR参数组合。-d：复制时保留链接。这里所说的链接相当于Windows系统中的快捷方式。-f：覆盖已经存在的目标文件而不给出提示。-i：与-f选项相反，在覆盖目标文件之前给出提示，要求用户确认是否覆盖，回答&quot;y&quot;时目标文件将被覆盖。-p：除复制文件的内容外，还把修改时间和访问权限也复制到新文件中。-r：若给出的源文件是一个目录文件，此时将复制该目录下所有的子目录和文件。-l：不复制文件，只是生成链接文件。##### 举例：1234567891011#将../html/index.html 复制到当前目录cp ../html/index.html . #将../html/ 目录下的文件及子目录复制到当前的tt目录下，如果tt不存在，会自动创建cp -r ../html/ tt/ #将文件file复制到目录/usr/men/tmp下，并改名为file1cp file /usr/men/tmp/file1 #如果dir2目录已存在，则需要使用cp -r dir1/. dir2#如果这时使用cp -r dir1 dir2,则也会将dir1目录复制到dir2中，明显不符合要求。ps:dir1、dir2改成对应的目录路径即可。 远程复制1234567891011#将当前目录下的test.txt复制到远程111.12机器的/zx目录下scp test.txt root@192.168.111.12:/zx#将test.txt复制到远程用户的根目录，并命名为textA.txtscp test.txt root@192.168.111.12:testA.txt#也可以不指定用户，在后续提示中再输入，如下：scp test.txt 192.168.111.12:/zx#从远程复制到本地： -r用于递归整个目录scp -r remote_user@remote_ip:remote_folder local_path 4.移动操作：移动操作可以理解成复制文件后，删除原文件。 eg1: 123mv /zx/soft/* . #复制/zx/soft目录中的所有文件到当前目录mv a.txt ./test/a.txt #复制当前目录a.txt到当前的test目录下。mv /zx/soft/ /tmp/soft #复制文件夹到/tmp/下，必须保证tmp是存在的文件夹 5.重命名操作：重命名还是用的移动操作命令，比如： 123456#将目录(文件)A重命名为Bmv A B#将/a目录(文件)移动到/b下，并重命名为c。要保证b目录存在。mv /a /b/c#将当前test1目录移动到当前的test目录并命名为bmv ./test1 ./test/b 6.解压压缩操作tar 123456789101112-c: 建立压缩档案-x：解压-t：查看内容-r：向压缩归档文件末尾追加文件-u：更新原压缩包中的文件这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。-z：有gzip属性的-j：有bz2属性的-Z：有compress属性的-v：显示所有过程-O：将文件解开到标准输出 下面的参数-f是必须的-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名。 举例说明：12345678tar -cf all.tar *.jpg这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。tar -tf all.tar这条命令是列出all.tar包中所有文件，-t是列出文件的意思tar -xf all.tar这条命令是解出all.tar包中所有文件，-x是解开的意思压缩tar –cvf jpg.tar *.jpg //将目录里所有jpg文件打包成jpg.tar eg2:12tar -xzf nginx-1.14.0.tar.gz //解压到当前目录 tar -zxf nginx-1.14.0.tar.gz -C /usr/local/nginx #解压到对应目录 eg3:1tar -zxvf nginx...tar.gz #解压并显示过程 注意：有些压缩程序提示命令找不到，需要进行安装，例如：yum install unzip或在ubuntu上：apt-get install unzip 总结1、*.tar 用 tar –xvf 解压2、*.gz 用 gzip -d或者gunzip 解压3、*.tar.gz和.tgz 用 tar –xzf 解压4、\\.bz2 用 bzip2 -d或者用bunzip2 解压5、*.tar.bz2用tar –xjf 解压6、*.Z 用 uncompress 解压7、*.tar.Z 用tar –xZf 解压8、*.rar 用 unrar e解压9、*.zip 用 unzip 解压 解压的时候，有时候不想覆盖已经存在的文件，那么可以加上-n参数 12unzip -n test.zipunzip -n -d /temp test.zip 只看一下zip压缩包中包含哪些文件，不进行解压缩 1unzip -l test.zip 查看显示的文件列表还包含压缩比率 1unzip -v test.zip 检查zip文件是否损坏 1unzip -t test.zip 如果已有相同的文件存在，要求unzip命令覆盖原先的文件 1unzip -o test.zip -d /tmp/ 示例：1eg1: unzip mydata.zip -d mydatabak #解压到mydatabak目录 xz这是两层压缩，外面是xz压缩方式，里层是tar压缩,所以可以分两步实现解压 12$ xz -d node-v6.10.1-linux-x64.tar.xz$ tar -xvf node-v6.10.1-linux-x64.tar 7.上传文件工具从本地windows上传一些文件到远程Linux服务器可以通过xshell的xftp也可以通过下面这个小工具lrzsz，使用更加方便。 1yum install lrzsz #安装工具 常用命令：12sz dist.zip #下载文件dist.zip到本地rz #会打开窗口，上传文件到远程服务器 8.ln、file和touch命令ln命令：名用于创建链接文件，包括硬链接(Hard Link)和符号链接（Symbolic Link) 。我们常用的是符号链接，也称软连接。软连接就类似windows里的快捷方式。 示例：#在当前目录创建一个软连接，指向/etc/fastab，名称也是fastab 1ln -s /etc/fastab #在当前目录创建一个指向/boot/grub的软连接，命名为gb 1ln -s /boot/grub gb 注意：删除软连接 正确方式是： 1rm -rf ./gb 错误方式 1rm -rf ./gb/ 这样会删除了原有grub下的内容。特别是针对系统文件的软连接，删除一定要慎重。 file命令 用于识别文件的类型 Linux中文件后缀只是方便使用者识别，没有实质的约束作用。file命令可以查看文件的实质类型： file [-bcLz] 文件|目录选项说明： 文件|目录：需要识别的文件或目录 1234-b: 显示识别结果时，不显示文件名-c: 显示执行过程-L: 直接显示符号链接文件指向的文件类型-z: 尝试去解读压缩文件的内容 示例： 可以看出，index.mp4本质是一个HTML而非一个mp4文件 示例： 可以看出，index.mp4本质是一个HTML而非一个mp4文件 12[root@VM_0_13_centos soft]# file index.mp4 index.mp4: HTML document, UTF-8 Unicode text, with very long lines touch命令： 用于改变文件或目录的访问时间和修改时间。 12345touch [-am] [-t&lt;日期时间&gt;] [目录|文件]如果指定目录文件不存在，则会直接创建一个空文件，所以touch也常用来创建一个空白文件#创建一个新文件aa.txttouch aa.txt 选项说明： 123-a: 只修改访问时间-m : 只修改 修改时间-t : 使用指定日期时间，而非系统时间 。例如要修改为2019年10月20日16：38分13秒。参数就是：‘20191020163813’ 示例：修改之前可以先查看文件的时间戳: 用stat 命令查看 123456789[root@VM_0_13_centos soft]# stat index.html File: ‘index.html’ Size: 17215 Blocks: 40 IO Block: 4096 regular fileDevice: fd01h/64769d Inode: 529352 Links: 1Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2019-10-23 15:15:37.280616254 +0800Modify: 2019-10-23 15:15:37.280616254 +0800Change: 2019-10-23 15:15:37.290616257 +0800 Birth: - 开始修改：将index.html文件的访问和修改时间修改成当前系统的时间。 touch index.html 9.查找操作命令：对于要用到的文件，目录等，经常有忘记的时候，所以查找命令就显得极为必要： find:查找文件或目录 (常用) 语法如下： 1234find [目录…] [-amin &lt;分钟&gt;] [-atime &lt;24小时数&gt;] [-cmin &lt;分钟&gt;] [-ctime&lt;24小时数&gt;][-empty][-exec&lt;执行命令&gt;][-fls&lt;列表文件&gt;][-follow] [-fstype &lt;系统文件类型&gt;] [-gid &lt;组编号&gt;] [-group &lt;组名称&gt;] [-nogroup] [-mmin &lt;分钟&gt;] [-mtime &lt;24小时数&gt;] [-name &lt;查找内容&gt;] [-nogroup] [-nouser] [-perm &lt;权限数值&gt;] [-size &lt;文件大小&gt;] [-uid &lt;用户编号&gt;] [-user &lt;用户名称&gt;] [-nouser] 几个常用选项说明：123456789101112131415-size &lt;文件大小&gt;：查找符合指定大小的文件。文件大小单位可以是“c”表示Byte；“k”表示KB。如配置为“100k”，find命令会查找文件大小正好100KB的文件；配置为“+100k”，find命令会查找文件大小大于100KB的文件；配置为“-100k”，find命令会查找文件大小小于100KB的文件。-user&lt;用户名称&gt;：查找所有者是指定用户的文件或目录，也能以用户编号指定-name &lt;查找内容&gt;：查找指定的内容，在查找内容中使用“*” 表示任意个字符；使用“?”表示任何一个字符-mtime &lt;24小时数&gt;：查找在指定时间曾更改过内容的文件或目录，单位以24小时计算。如配置为2，find命令会查找刚好在48小时之前更改过内容的文件；配置为+2，find命令会查找超过在48小时之前更改过内容的文件；配置为-2，find命令会查找在48小时之内更改过内容的文件。-mmin &lt;分钟&gt;：查找在指定时间曾被更改过内容的文件或目录，单位以分钟计算。cmin &lt;分钟&gt;：查找在指定时间曾被更改过权限属性的文件或目录，单位以分钟计算。-ctime对应小时。-amin &lt;分钟&gt;：查找的是指定时间访问过的文件或目录。-atim对应小时。-perm &lt;权限数值&gt;：查找符合指定权限数值（有关权限数值见第6章）的文件或目录。如配置为“0700”，find命令会查找权限数值正好是“0700”的文件或目录；配置为“+0700”，find命令会查找权限数值大于 “0700”的文件或目录；配置为“-0700”， find选项大概有以下几类： 1.按时间范围查找 2.按文件大小查找 3.按文件名称查找 4.按其他：比如权限、用户组、类型等 示例：#从根目开始，查找名称以nginx开头的目录和文件 1234567find / -name nginx* #查找文件大小超过100M的文件find / -size +100M#查找/home/zx目录下，10分钟内被修改过的文件和目录find /home/zx/ -mmin -10 locate：查找文件或目录(不常用) 1locate 查找内容 例如：locate nginx 会将所有包含nginx的目录和文件都列出来。可以用* 或？等匹配符。 locate的查找速度非常快，因为该命令查找的是数据库，所以有些刚修改的文件和目录，可能无法找到。可以采用：updatedb 命令更新数据库。 which: 查找文件(不常用) 1which [文件] which命令只会在PATH环境变量定义的路径及命令别名中查找，所以范围有限。 whereis :查找文件(不常用) 1whichis [-bu] [-B&lt;目录&gt;] [-M&lt;目录&gt;] [-S&lt;目录&gt;] [文件] 常用选项： 1234567891011文件：要查找的命令-b: 只查找二进制文件-u: 查找不包含指定类型的文件-B&lt;目录&gt;： 只在指定目录下查找二进制文件-M&lt;目录&gt;：只在指定目录查找帮助文件-S&lt;目录&gt;：只在指定目录查找源码目录 例如： 默认只会在指定目录查找（/bin ,/etc ,/usr) 12[root@VM_0_13_centos soft]# whereis nginxnginx: /usr/local/nginx /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak","categories":[{"name":"Linux","slug":"Linux","permalink":"/categories/Linux/"}],"tags":[]},{"title":"springboot 打war包,部署到tomcat 404错误解决","slug":"springboot-打war包,部署到tomcat-404错误解决","date":"2019-12-03T00:00:00.000Z","updated":"2019-12-03T07:09:40.000Z","comments":true,"path":"2019/12/03/springboot-打war包,部署到tomcat-404错误解决/","link":"","permalink":"/2019/12/03/springboot-%E6%89%93war%E5%8C%85,%E9%83%A8%E7%BD%B2%E5%88%B0tomcat-404%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3/","excerpt":"","text":"springboot打war包,部署到tomcat 404错误解决本文链接：https://blog.csdn.net/yswKnight/article/details/80054284SpringBoot打war包，并且部署到Tomcat服务器，运行报错404（springboot专属404页面）【完美解决】参考文档：idea下springboot打包成jar包和war包，并且可以在外部tomcat下运行访问到 修改pom文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465661、packaging由jar变成war &lt;groupId&gt;com.wangys&lt;/groupId&gt; &lt;artifactId&gt;wechatProject&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;war&lt;/packaging&gt;2、剔除springboot本身Tomcat配置spring boot 本身是内置了tomcat的，但是我们在外面的tomcat部署项目的时候，就必须把springboot本身的tomcat剔除掉，否则会形成冲突！&lt;scope&gt;provided&lt;/scope&gt; 作用：provided表明该包只在编译和测试的时候用！&lt;!-- 排除内置tomcat容器，导出成war包可以让外部容器运行spring-boot项目--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;!-- provided表明该包只在编译和测试的时候用 --&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- 添加jsp支持 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt; &lt;!-- 如果不注释，你会发现main线程运行报404错误，必须使用 spring-boot:run这个命令运行才行。所以我们注释掉--&gt;&lt;/dependency&gt;143、将pom文件中的servlet-api.jar包设置为只有在编译和测试的时候用！因为tomcat本身也有servlet-api.jar包，结果会产生冲突！ &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;!-- provided表明该包只在编译和测试的时候用 --&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;jsp-api&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;4、如果有下面这个插件spring-boot-maven-plugin，请将其注释，否则会打包错误！&lt;build&gt; &lt;!-- 修改war包名称 --&gt; &lt;finalName&gt;weChatProject&lt;/finalName&gt; &lt;plugins&gt; &lt;!-- 在打jar包时，用来指定主类的插件，打war包将其注释掉 --&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;configuration&gt; &lt;mainClass&gt;$&#123;start-class&#125;&lt;/mainClass&gt; &lt;layout&gt;ZIP&lt;/layout&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 2、继承SpringBootServletInitializer方法SpringBoot启动类继承SpringBootServletInitializer方法，并重写configure方法点解：springboot项目打成war包部署到tomcat时需要改变启动方式，否则运行tomcat时war包只会解压，但是加载不了 123456789101112131415161718192021@RestController@EnableAutoConfiguration@ComponentScan@SpringBootApplicationpublic class WeChatApplication extends SpringBootServletInitializer&#123;@RequestMapping(\"/hello\")public String index() &#123; return \"Hello World\";&#125;/** * 需要把web项目打成war包部署到外部tomcat运行时需要改变启动方式 */@Overrideprotected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(WeChatApplication.class);&#125;public static void main(String[] args) &#123; SpringApplication.run(WeChatApplication.class, args);&#125;&#125; 默认配置下的访问路径（至关重要）默认配置下，将war包发布到tomcat服务器，需要加war包的名称！！！！http://localhost:8080/war包名/@RequestMapping.value访问了————————————————版权声明：本文为CSDN博主「詠聖wK」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/yswKnight/article/details/80054284","categories":[{"name":"spribgboot","slug":"spribgboot","permalink":"/categories/spribgboot/"}],"tags":[{"name":"tomcat","slug":"tomcat","permalink":"/tags/tomcat/"}]},{"title":"docker 安装tomcat","slug":"docker-安装tomcat","date":"2019-12-02T00:00:00.000Z","updated":"2019-12-03T03:27:08.000Z","comments":true,"path":"2019/12/02/docker-安装tomcat/","link":"","permalink":"/2019/12/02/docker-%E5%AE%89%E8%A3%85tomcat/","excerpt":"","text":"docker安装tomcat1. 安装tomcat一下代码请一行一行的执行, 按照文字说明操作 123docker pull tomcat #如果不指定版本默认拉取最新版docker images #查看拉取的imagedocker run --name tomcat -p 7899:8080 -it -d tomcat(或者imageid号) 2. 配置tomcat-user.xml和 manager.xmltomcat-user.xml是配置管理用户的信息, 用于上传war包, 在文件中添加如下信息 roles加manager-sript是因为远程maven部署用, 注意对于tomcat9来说，不能同时赋予用户manager-script和manager-gui角色。 123456&lt;role rolename=\"manager\"/&gt; &lt;role rolename=\"manager-gui\"/&gt; &lt;role rolename=\"admin\"/&gt; &lt;role rolename=\"admin-gui\"/&gt; &lt;role rolename=\"manager-script\"/&gt; &lt;user username=\"tomcat\" password=\"tomcat\" roles=\"admin-gui,admin,manager-gui,manager,manager-script\"/&gt; 其中admin-gui是为了能访问manger的界面，manager-secret是为了可以上传war文件 做完了以上的步骤，可以用localhost:8080/manager,或者127.0.0.1:8080/manager要远程访问manager的页面，但是换为IP:8080/manager被拒绝了 配置远程访问manager： tomat_home/conf/Catalina/localhost/下 添加manager.xml 12345&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;Context privileged=\"true\" antiResourceLocking=\"false\" docBase=\"$&#123;catalina.home&#125;/webapps/manager\"&gt; &lt;Valve className=\"org.apache.catalina.valves.RemoteAddrValve\" allow=\"^.*$\" /&gt; &lt;/Context&gt; 如果还是不生效,则编辑/usr/local/tomcat# vi webapps/manager/META-INF/context.xml 更改其中的RemoteAddrValve地址 1&lt;Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot; allow=&quot;^.*$&quot; /&gt;","categories":[{"name":"docker","slug":"docker","permalink":"/categories/docker/"}],"tags":[{"name":"tomcat","slug":"tomcat","permalink":"/tags/tomcat/"}]},{"title":"docker安装Jenkins最新版本","slug":"docker安装Jenkins最新版本","date":"2019-11-29T00:00:00.000Z","updated":"2019-11-29T06:35:42.000Z","comments":true,"path":"2019/11/29/docker安装Jenkins最新版本/","link":"","permalink":"/2019/11/29/docker%E5%AE%89%E8%A3%85Jenkins%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC/","excerpt":"","text":"http://localhost:8001/pluginManager/advanced docker安装jenkins最新版本1.pull一个jenkins镜像 docker pull jenkins/jenkins:lts; 这个是安装最新版的jenkins,如果安装旧版本，很多插件安装不上，docker环境下升级又比较麻烦。 image.png 2.查看已经安装的jenkins镜像 docker images; image.png 查看是否是最新版 docker inspect ba607c18aeb7 image.png 3.创建一个jenkins目录 mkdir /home/jenkins_home; 4.启动一个jenkins容器 docker run -d –name jenkins_01 -p 8081:8080 -v /home/jenkins_01:/home/jenkins_01 jenkins/jenkins:lts ; 5.查看jenkins服务 docker ps | grep jenkins; 6.启动服务端 。localhost:8081; 7.进入容器内部docker exec -it jenkins_01 bash； 8.执行：cat /var/jenkins_home/secrets/initialAdminPassword，得到密码并粘贴过去 9.输入密码之后，重启docker镜像 docker restart {CONTAINER ID}，安装完毕( 不重启也没有什么关系)。 安装插件 !!!! 很重要( 这是的插件安装的源地址是国外的, 可能会导致插件无法访问安装失败) 这时先不要急着选择 安装插件的方式, 首先新开一个浏览器标签页访问 http://localhost:8001/pluginManager/advanced (主机地址和端口号是自己设置的 之前在docker启动的时候影射了端口号)更改掉默认的插件源地址 进入页面之后点击 advance(高级) 然后滑动页面到最底下, 更改源地址为清华的镜像地址,然后点击提交 https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json 然后再切换到之前选择插件安装方式的页面,自己选择安装的方式 –推荐选择第一个安装推荐的插件– (避免因为国外的源地址导致不能访问插件安装失败,后续手动安装时非常的麻烦–因为各个插件安装时会有依赖,安装顺序错了就会安装失败)","categories":[{"name":"docker","slug":"docker","permalink":"/categories/docker/"},{"name":"Jenkins","slug":"docker/Jenkins","permalink":"/categories/docker/Jenkins/"}],"tags":[{"name":"-Jenkins -docker","slug":"Jenkins-docker","permalink":"/tags/Jenkins-docker/"}]},{"title":"GIT-项目中的实际使用分析","slug":"GIT项目中的实际使用分析","date":"2019-11-19T00:00:00.000Z","updated":"2019-11-19T05:50:46.916Z","comments":true,"path":"2019/11/19/GIT项目中的实际使用分析/","link":"","permalink":"/2019/11/19/GIT%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%99%85%E4%BD%BF%E7%94%A8%E5%88%86%E6%9E%90/","excerpt":"","text":"GIT-项目中的实际使用分析本文转载自看完，我不信你不会GIT！！！ 写得非常的详细,分析很到位,很容易理解 一、常用命令介绍 1.1 命令行介绍1.1.1 Git 全局设置12$ git config --global user.name \"knight\"$ git config --global user.email \"knight@dayuan.com\" 1.1.2 创建一个新仓库（本地）123456$ git clone http://git.dayuan.cc/practice/git-exmple.gitcd git-exmple$ touch README.md$ git add README.md$ git commit -m \"add README\"$ git push -u origin master 1.1.3 在已存在的目录中创建仓库123456cd existing_folder$ git init$ git remote add origin http://git.dayuan.cc/practice/git-exmple.git$ git add .$ git commit -m \"Initial commit\"$ git push -u origin master 1.1.4 将本地已存在的仓库推送到远程仓库12345cd existing_repo$ git remote rename origin old-origin$ git remote add origin http://git.dayuan.cc/practice/git-exmple.git$ git push -u origin --all$ git push -u origin --tags 1.1.5 查看分支相关命令123$ git branch -r; //查看远程分支$ git branch; //查看本地分支$ git branch -a; //查看所有分支 1.1.6 拉取远程分支并创建本地分支12// dev2为远程分支,dev1为本地分支$ git checkout -b dev1 origin/dev2; 从远程分支dev拉取到本地并且创建本地分支dev，且俩者之间建立映射关系,同时当前分支会切换到dev1 12//dev2为远程分支,dev1为本地分支$ git fetch origin dev2:dev1; 使用该方式会在本地新建分支dev1，但是不会自动切换到该本地分支dev1，需要手动checkout。采用此种方法建立的本地分支不会和远程分支建立映射关系。 1.1.7 建立本地分支与远程分支的映射关系（或者为跟踪关系track） 这样使用git pull或者git push时就不必每次都要指定从远程的哪个分支拉取合并和推送到远程的哪个分支了。 1$ git branch -vv 输出映射关系 12// dev为远程分支名$ git branch -u origin/dev 将当前本地分支与远程分支建立映射关系 1$ git branch --unset-upstream 撤销当前本地分支与远程分支的映射关系 1.1.8 切换当前本地分支12// dev为本地分支名$ git checkout dev; 1.1.9 拉取远程分支代码1$ git pull 使用的前提是当前分支需要与远程分支之间建立映射关系 1.1.10 推送本地分支代码到远程分支1$ git push 使用的前提是当前分支需要与远程分支之间建立映射关系 1.1.11 合并分支 场景:现在有dev本地分支与远程分支，master本地分支与远程分支 现在将dev的分支代码合并到master主干上 思路步骤 : 1.切换到本地分支dev上，并且pull拉取一下远程dev分支上的改动地方 2.将所有本地修改进行commit并且push到远程dev分支上，保证没有遗漏的，确保当前本地dev与远程dev是一致的 3.将当前本地分支切换到本地master上 4.将本地分支dev合并到本地master上 5.将本地已经合并了dev分支的master进行push到远程master上 大概思路就是这样。需要注意的是在进行merge(合并)的时候需要禁用fast-forward模式 具体的合并命令: git merge --no-ff dev (dev为本地被合并的分支名字) 二、Gitflow总览 git_flow流程图.png 从上图可以看到主要包含下面几个分支： master : 主分支，主要用来版本发布。 develop：日常开发分支，该分支正常保存了开发的最新代码。 feature：具体的功能开发分支，只与 develop 分支交互。 release：release分支可以认为是master 分支的未测试版。比如说某一期的功能全部开发完成，那么就将 develop 分支合并到 release 分支，测试没有问题并且到了发布日期就合并到master 分支，进行发布。 hotfix：线上 bug 修复分支。 除此之后还可以有 fast-track 等分支。 2.1 主分支主分支包括 master 分支和 develop 分支。master 分支用来发布，HEAD 就是当前线上的运行代码。develop分支就是我们的日常开发。使用这两个分支就具有了最简单的开发模式：develop分支用来开发功能，开发完成并且测试没有问题则将 develop分支的代码合并到 master分支并发布。 master-develop.png 这引入了几个问题： develop分支只有发布完了才能进行下一个版本开发，开发会比较缓慢。 线上代码出现 bug 如何进行 bug 修复。 带着这两个问题往下看。 2.2 辅助分支主要介绍的辅助分支如下： feature分支 release分支 hotfix分支 通过这些分支，我们可以做到：团队成员之间并行开发，feature track更加容易，开发和发布并行以及线上问题修复。 2.2.1 Feature 分支feature分支用来开发具体的功能，一般 fork 自 develop分支，最终可能会合并到develop分支。比如我们要在下一个版本增加功能1、功能2、功能3。那么我们就可以起三个feature分支：feature1，feature2，feature3。（feature分支命名最好能够自解释，这并不是一种好的命名。）随着我们开发，功能1和功能2都被完成了，而功能3因为某些原因完成不了，那么最终 feature1 和 feature2分支将被合并到 develop分支，而 feature3分支将被干掉。 develop-feature.png 我们来看几个相关的命令。 2.2.1.1 新建feature分支从 develop 分支建一个 feature 分支，并切换到 feature 分支 12$ git checkout -b myfeature developSwitched to a new branch \"myfeature\" 2.2.1.2 合并feature 分支到 develop12345678$ git checkout developSwitched to branch 'develop'$ git merge --no-ff myfeatureUpdating ea1b82a..05e9557(Summary of changes)$ git branch -d myfeatureDeleted branch myfeature$ git push origin develop 上面我们 merge 分支的时候使用了参数 --no-ff，ff 是fast-forward的意思，--no-ff就是禁用fast-forward。关于这两种模式的区别如下图。（可以使用 sourceTree 或者命令git log –graph查看。） fast-forward模式的效果.png 看了上面的图，那么使用非fast-forward模式来 merge 的好处就不言而喻了：我们知道哪些commit 是某些feature 相关的。虽然 git merge的时候会自动判断是否使用fast-farward模式，但是有时候为了更明确，我们还是要加参数--no-ff或者--ff。 2.2.2 Release 分支release分支在我看来是 pre-master。release分支从 develop 分支 fork 出来，最终会合并到 develop分支和 master分支。合并到 master分支上就是可以发布的代码了。有人可能会问那为什么合并回 develop分支呢？很简单，有了 release分支，那么相关的代码修复就只会在 release分支上改动了，最后必然要合并到 develop分支。下面细说。 我们最初所有的开发工作都在 develop分支上，当我们这一期的功能开发完毕的时候，我们基于 develop分支开一个新的release分支。这个时候我们就可以对 release分支做统一的测试了，另外做一些发布准备工作：比如版本号之类的。 如果测试工作或者发布准备工作和具体的开发工作由不同人来做，比如国内的 RD 和 QA，这个 RD 就可以继续基于develop分支继续开发了。再或者说公司对于发布有严格的时间控制，开发工作提前并且完美的完成了，这个时候我们就可以在develop 分支上继续我们下一期的开发了。同时如果测试有问题的话，我们将直接在release分支上修改，然后将修改合并到develop分支上。 待所有的测试和准备工作做完之后，我们就可以将 release分支合并到master 分支上，并进行发布了。 一些相关命令如下。 新建 release 分支1234567$ git checkout -b release-1.2 developSwitched to a new branch \"release-1.2\"$ ./bump-version.sh 1.2File modified successfully, version bumped to 1.2.$ git commit -a -m \"Bumped version number to 1.2\"[release-1.2 74d9424] Bumped version number to 1.21 files changed, 1 insertions(+), 1 deletions(-) 2.2.2.1 release 分支合并到 master 分支123456$ git checkout masterSwitched to branch 'master'$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes)$ git tag -a 1.2 2.2.2.2 release 分支合并到 develop 分支12345$ git checkout developSwitched to branch 'develop'$ git merge --no-ff release-1.2Merge made by recursive.(Summary of changes) 2.2.2.3 最后，删除 release 分支12$ git branch -d release-1.2Deleted branch release-1.2 (was ff452fe). 2.2.3 Hotfix 分支顾名思义，hotfix分支用来修复线上 bug。当线上代码出现 bug 时，我们基于 master分支开一个 hotfix分支，修复 bug 之后再将 hotfix分支合并到master分支并进行发布，同时 develop分支作为最新最全的代码分支，hotfix分支也需要合并到 develop分支上去。仔细想一想，其实 hotfix分支和 release分支功能类似。hotfix的好处是不打断develop 分支正常进行，同时对于生产代码的修复貌似也没有更好的方法了（总不能直接修改 master代码吧）。 hotfixes.png 一些相关的命令。 2.2.3.1 新建 hotfix 分支1234567$ git checkout -b hotfix-1.2.1 masterSwitched to a new branch \"hotfix-1.2.1\"$ ./bump-version.sh 1.2.1Files modified successfully, version bumped to 1.2.1.$ git commit -a -m \"Bumped version number to 1.2.1\"[hotfix-1.2.1 41e61bb] Bumped version number to 1.2.11 files changed, 1 insertions(+), 1 deletions(-) 2.2.3.2 Fix bug123$ git commit -m \"Fixed severe production problem\"[hotfix-1.2.1 abbe5d6] Fixed severe production problem5 files changed, 32 insertions(+), 17 deletions(-) 2.2.3.3 buffix 之后，hotfix 合并到 master123456$ git checkout masterSwitched to branch 'master'$ git merge --no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes)$ git tag -a 1.2.1 2.2.3.4 hotfix 合并到 develop 分支12345$ git checkout developSwitched to branch 'develop'$ git merge --no-ff hotfix-1.2.1Merge made by recursive.(Summary of changes) 2.2.3.5 删除 hotfix 分支12$ git branch -d hotfix-1.2.1Deleted branch hotfix-1.2.1 (was abbe5d6). 三、Git 分支管理和冲突解决 3.1 合并分支间的修改 Merge合并操作将两条或多条分支合并到一起，实际上有好几种分支合并方法，下面介绍主要的三种： 3.1.1 直接合并(straight merge)：把两条分支上的历史轨迹合并，交汇到一起。比如要把dev分支上的所有东东合并到master分支： 首先先到master分支：git checkout master 然后把dev给合并过来：git merge dev 注意没参数的情况下merge是fast-forward的，即Git将master分支的指针直接移到dev的最前方。 换句话说，如果顺着一个分支走下去可以到达另一个分支的话，那么Git在合并两者时，只会简单移动指针，所以这种合并成为快进式(Fast-forward)。 3.1.2 压合合并(squashed commits)：将一条分支上的若干个提交条目压合成一个提交条目，提交到另一条分支的末梢。 把dev分支上的所有提交压合成主分支上的一个提交，即压合提交： 12$ git checkout master$ git merge --squash dev 此时，dev上的所有提交已经合并到当前工作区并暂存，但还没有作为一个提交，可以像其他提交一样，把这个改动提交到版本库中： 1$ git commit –m “something from dev” 3.1.3 拣选合并(cherry-picking)：拣选另一条分支上的某个提交条目的改动带到当前分支上。每一次提交都会产生一个全局唯一的提交名称，利用这个名称就可以进行拣选提交。 比如在dev上的某个提交叫：321d76f 把它合并到master中： 12$ git checkout master$ git cherry-pick 321d76f 要拣选多个提交，可以给git cherry-pick命令传递-n选项，比如： 1$ git cherry-pick –n 321d76f 这样在拣选了这个改动之后，进行暂存而不立即提交，接着可以进行下一个拣选操作，一旦拣选完需要的各个提交，就可以一并提交。 3.2 冲突处理当两条分支对同一个文件的同一个文本块进行了不同的修改，并试图合并时，Git不能自动合并的，称之为冲突(conflict)。解决冲突需要人工处理。 比如当前在master分支，想把dev分支merge过来，结果产生了一个冲突，打开文件内容可以看到这么一个冲突： 123456789&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADtest in master=======test in dev&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev &lt;&lt;&lt;&lt;&lt;&lt;&lt;标记冲突开始，后面跟的是当前分支中的内容。 HEAD指向当前分支末梢的提交。 =======之后，&gt;&gt;&gt;&gt;&gt;&gt;&gt;之前是要merge过来的另一条分支上的代码。 &gt;&gt;&gt;&gt;&gt;&gt;&gt;之后的dev是该分支的名字。 对于简单的合并，手工编辑，然后去掉这些标记，最后像往常的提交一样先add再commit即可。 3.3 删除分支有些分支没有必要长期保存，比如分支中的代码已经打了标签并已发布，或者实验分支已经成功完成工作或中途废弃等等。 注意：打了标签的分支，Git在删除该分支时，从版本树起始到此标签间的全部历史轨迹均会保留，此时删除分支操作只是删除分支本身的名称，因此可以说该分支没有必要长期保存。 而在其他版本控制工具中，删除分支通常意味着删除分支上的所有历史轨迹，所以不能因为打了标签就认为其没有必要保存。 删除一个分支dev2： 1$ git branch –d dev2 注意不能删除当前所在分支，需要转到别的分支上。 如果要删除的分支已经成功合并到当前分支，删除分支的操作会直接成功。 如果要删除的分支没有合并到当前所在分支，则会出现提示，如果确定无须合并而要直接删除，则执行命令： 1$ git branch –D dev2 进行强删。 四、Git 版本回退 在版本迭代开发过程中，相信很多人都会有过错误提交的时候。这种情况下，菜鸟程序员可能就会虎驱一震，紧张得不知所措。而资深程序员就会微微一笑，摸一摸锃亮的脑门，然后默默的进行版本回退。 对于版本的回退，我们经常会用到两个命令： 12$ git reset$ git revert 那这两个命令有何区别呢？先不急，我们后文详细介绍。 4.1 git reset假如我们的系统现在有如下几个提交： 错误提交.png 其中：A 和 B 是正常提交，而 C 和 D 是错误提交。现在，我们想把 C 和 D 回退掉。而此时，HEAD 指针指向 D 提交（5lk4er）。我们只需将 HEAD 指针移动到 B 提交（a0fvf8），就可以达到目的。 只要有 git 基础的朋友，一定会想到 git reset 命令。完整命令如下： 1$ git reset --hard a0fvf8 命令运行之后，HEAD 指针就会移动到 B 提交下，如下图示： reset以后效果.png 而这个时候，远程仓库的 HEAD 指针依然不变，仍在 D 提交上。所以，如果直接使用命令的话，将无法将更改推到远程仓库。此时，只能使用 选项将提交强制推到远程仓库： 1$ git push -f 采用这种方式回退代码的弊端显而易见，那就是会使 HEAD 指针往回移动，从而会失去之后的提交信息。将来如果突然发现，C 和 D 是多么绝妙的想法，可它们已经早就消失在历史的长河里了。 而且，有些公司明令禁止使用 git reset命令去回退代码，原因与上述一样。所以，我们需要找到一个命令，既可以回退代码，又可以保存错误的提交。这时，git revert命令就派上用场了。 4.2 git revertgit revert的作用通过反做创建一个新的版本，这个版本的内容与我们要回退到的目标版本一样，但是HEAD指针是指向这个新生成的版本，而不是目标版本。 使用 git revert命令来实现上述例子的话，我们可以这样做：先 revert D，再 revert C （有多个提交需要回退的话需要由新到旧进行 revert）： 12$ git revert 5lk4er$ git revert 76sdeb 反向创建版本回退.png 这里只有两个提交需要 revert，我们可以一个个回退。但如果有几十个呢？一个个回退肯定效率太低而且容易出错。我们可以使用以下方法进行批量回退： 1$ git revert OLDER_COMMIT^..NEWER_COMMIT 这时，错误的提交 C 和 D 依然保留，将来进行甩锅的时候也有依可循。而且，这样操作的话 HEAD 指针是往后移动的，可以直接使用 git push命令推送到远程仓库里。而这种做法，正是企业所鼓励的。 我们再举个更难一点的例子。 假如现在有三个提交，但很不巧的是，那个错误的提交刚好位于中间。如下图示： 位于中间的错误提交.png 这时，直接使用 git reset命令将 HEAD 指针重置到 A 提交显然是不行的，因为 C 提交是正确的，需要保留的。先把 C 提交 及 B 提交全部回退，再使用 cherry-pick 命令将 C 提交重新再生成一个新的提交 C’’，这样就实现了将 B提交回退的需求。完整的过程如下： 1.png 通过以上对比可以发现，与 最大的差别就在于，会失去后面的提交，而是通过反做的方式重新创建一个新的提交，而保留原有的提交。在企业里，应尽量使用 命令，能不用 命令尽量不用。 五、命名规范 5.1 主版本.次版本.修订号1.0.0 版本号主要有3部分构成（由两个.分割成三部分）主版本、次版本、修订号： 主版本：程序的主版本号，除非系统做整体重构，一般不变化 次版本：功能版本号，一般为功能迭代的版本号，每次版本号为上一次正常按迭代计划发版的次版本 + 1;主版本发生变更，次版本需重置为0 比如：上次按正常按迭代计划发版的版本号为v1.9.0，本次版本号为 v1.(9+1).0，即 v1.10.0 修订号：每次线上BUG修复，该版本号相对上次修订号+1 （前提：相同主版本以及次版本）;主版本和次版本发生变更，修订号需重置为0 比如：上次修订号为v1.9.3，本次版本号为 v1.9.(3+1)，即 v1.9.4 5.2 分支命名规范5.2.1 主分支：master：master 分支就叫 master 分支 develop：develop 分支就叫 develop 分支 5.2.2 辅助分支：5.2.2.1 Feature 分支feature/v1.16.0_xxx feature/v1.16.0_yyy feature/v1.16.0_zzz v1.16.0 表示当前迭代的版本号，xxx、yyy、zzz 表示当前迭代的功能或业务单元的名称 5.2.2.2 Release 分支release/v1.17.0 release/v1.18.0 v1.17.0、v1.18.0 根据上线需求和系统上线计划，合理规划版本号，每个大版本号表示一次上线正常上线过程。 5.2.2.3 Hotfix 分支hotfix/v1.17.1 hotfix/v1.17.2 v1.17.1、v1.17.2 表示v1.17.0 这个版本做了2次线上问题热修复。 六、总结 并行开发：依据迭代的发版计划和任务分解，创建feature（不同迭代需通过版本号隔离，同一个迭代内要上线的功能需要通过feature隔离） 保持迭代内代码的可预见性&amp;可控制性： 迭代内，只允许主迭代的feature代码提交到develop分支 哪里有问题改哪里，改完后及时合并到主分支: release(fit)环境的问题修复：应从release分支拉出分支进行问题修复，修复后及时合并到develop主分支 master环境的问题修复：应从生产环境对应的tag（一般为最新的版本号）拉出分支进行问题修复，问题修复后及时合并代码至develop主分支和master主分支","categories":[{"name":"git","slug":"git","permalink":"/categories/git/"}],"tags":[]},{"title":"mysql数据库字符乱码分析","slug":"mysql数据库字符乱码分析","date":"2019-11-13T03:45:01.776Z","updated":"2019-11-15T02:53:48.353Z","comments":true,"path":"2019/11/13/mysql数据库字符乱码分析/","link":"","permalink":"/2019/11/13/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%97%E7%AC%A6%E4%B9%B1%E7%A0%81%E5%88%86%E6%9E%90/","excerpt":"","text":"docker - mysql - utf8 中文编码问题原文链接：https://blog.csdn.net/m0_37639542/article/details/72852875 手把手教你如何在mysql 中使用中文编码1.首先在docker中拉取好一个最新的mysql镜像以后，创建一个容器： 1docker run -d -p 13306:3306 -e MYSQL_ROOT_PASSWORD=xxxxxx--name MYDB mysql1 参数的解释： -d 设置detach为true -p port 映射端口 13306 -e environment 设置密码 xxxxx 2. docker ps 查看mysql容器是否启动，进去容器 1docker exec -ti xxx(容器id) /bin/bash1 理论上应该启动正常 进去容器内部 3.查看mysql密码 是否正常1mysql -u root -p1 在提示下输入密码 xxxxx 正常情况下，应该出现以下提示符mysql&gt; ！！重点来了！！1.前期工作 查看当前mysql字符集情况1mysql&gt;SHOW VARIABLES LIKE &apos;character_set_%&apos;;//查看数据库字符集1 基本上都如下图所示：默认就是瑞典latin1 1SHOW VARIABLES LIKE &apos;collation_%&apos;;1 图上的第一个 connection 就是我们通过workbench等客户端连接的时候指定的编码。外部访问数据乱码的问题就出在这个connection连接层上 1.先解决外部访问数据乱码的问题1SET NAMES &apos;utf8&apos;;1 它相当于下面的三句指令： 123SET character_set_client = utf8;SET character_set_results = utf8;SET character_set_connection = utf8;123 2.创建数据库，创建表的时候，包括设置字段的时候也要加上字符集设置：例如 12345678910create database MYDB character set utf8;use JSPDB;create table t_product(pid varchar(20),pname varchar(20),price double,address varchar(30)) DEFAULT CHARSET=utf8;12345678910 3.如果你应经有建立了数据库，也可以通过以下语句修改字符集当然 如果是刚刚建容器的时候 我想你肯定是没有数据库的，所有此步跳过 123alter database name character set utf8;#修改数据库成utf8的.alter table type character set utf8;#修改表用utf8.alter table type modify type_name varchar(50) CHARACTER SET utf8;#修改字段用utf8123 4.这一步是很重要的来修改配置文件mysql容器的配置文件在哪呢？我们进去容器。不输入mysql -u root -p（即登录数据库）配置文件在etc/mysql/mysql.conf.d/mysql.cnf这个文件里头 我们找到这个文件 编辑他vi mysql.cnf,在使用docker容器时键入vim命令时提示：vim: command not found这个时候就需要安装vim这时候需要敲： 1apt-get update1 等更新完毕以后再敲命令： 1apt-get install vim1 然后你发现vim 编辑器可以使用以后，在此文件中添加如下字段在 [mysqld] 标签下加上三行 12default-character-set = utf8character_set_server = utf812 在 [mysql] 标签下加上一行 1default-character-set = utf81 在 [mysql.server]标签下加上一行 1default-character-set = utf81 在 [mysqld_safe]标签下加上一行 1default-character-set = utf81 在 [client]标签下加上一行 1default-character-set = utf81 修改结果如下： 那么如何检验成功了呢？还记得我们怎么检查mysql字符集编码的吗 ？1SHOW VARIABLES LIKE &apos;character_set_%&apos;;1 我们发现有很多都变成utf8了 1SHOW VARIABLES LIKE &apos;collation_%&apos;;1 以上基本的工作都做完了。 如果以上方法在docker作中不行 尝试执行以下更改1vi /etc/mysql/conf.d/mysql.cnf 将其中的内容改为如下信息 123456789101112[client]default-character-set=utf8[mysql]default-character-set=utf8[mysqld]init_connect=&apos;SET collation_connection = utf8_unicode_ci&apos;init_connect=&apos;SET NAMES utf8&apos;character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshake 那么现在让我们建一个表，看看是否可以读取写入中文了吧！！咚！！首先我使用的是mysql-workbench客户端我解释一下： connection name:连接名字随便起，这个无所谓 hostname:是你的ip地址，如果你是云服务器，你填上自己的公网地址就行（我是某云） port:记得填上你的映射端口号 比如我们例子使用的是：13306 username :是root password:你可以先输入上，如果不输入的时候，会提示你输入密码的 你会问哪来的用户和密码 ，你傻了，你创建容器的时候自己设置的，还记得吗？1docker run -d -p 13306:3306 -e MYSQL_ROOT_PASSWORD=xxxxxx--name MYDB mysql1 好，以下是基本的建表啊，数据库啊的语句，我就不展开了，很容易上手！！ 12345678910create database MYDB character set utf8;1use MYDB;create table t_product(pid varchar(20),pname varchar(20),price double,address varchar(30)) DEFAULT CHARSET=utf8;12345678insert into t_product values (&quot;A01&quot;,&quot;苹果&quot;,2.5,&quot;yantai&quot;);1 后来 我尝试插入多条： 12345678910insert into t_product values(&quot;A02&quot;,&quot;橘子&quot;,4.5,&quot;yantai&quot;),(&quot;A03&quot;,&quot;香江&quot;,8.5,&quot;rizhao&quot;),(&quot;A04&quot;,&quot;彩电&quot;,200,&quot;japan&quot;),(&quot;A05&quot;,&quot;哈哈&quot;,13.5,&quot;shandong&quot;)，(&quot;A06&quot;,&quot;你好&quot;,8.5,&quot;rizhao&quot;),(&quot;A07&quot;,&quot;手机&quot;,100,&quot;japan&quot;),(&quot;A08&quot;,&quot;电视&quot;,13.5,&quot;linyi&quot;)，(&quot;A09&quot;,&quot;数数&quot;,4.5,&quot;yantai&quot;),(&quot;A10&quot;,&quot;书店&quot;,8.5,&quot;rizhao&quot;);12345678910 然后你可以尝试将自己建好的表查看一下： 1select * from t_product;1 这样就成功了！！！如果你还不放心，我们可以尝试运行一个项目试一试哦！我这也不展开讲了！！我自己的一个JSP项目运行截图： 新增：我在使用的过程中，发现在jsp的servlet中使用1ps = conn.prepareStatement(&quot;insert into emp(empno,ename,job,sal,deptno) values(?,?,?,?,?)&quot;);1 首先 我在数据库是事先插入的3条记录，都没有问题，然后再jsp中，明明插入的是中文，但是在数据库中显示乱码我肯定是：JSP设置的出的问题 所以 我给每个servlet设置 1request.setCharacterEncoding(&quot;UTF-8&quot;);1 但是并没什么卵用….你可以试试…. 方案1:首先jsp页面设置编码设置为utf8&lt;%@ page language=”java” pageEncoding=”utf8”%&gt;接受对象是中文的时候对其处理String str=new String(request.getParameter(“name”).getBytes(“ISO-8859-1”),”utf8”); 方案2:连接数据库时候指定Encoding,我使用的是这个方法解决的下面是连接数据库的两种不同形式： 123456DriverManager.getConnection(&quot;jdbc:mysql://localhost/SKDB?useUnicode=true&amp;characterEncoding=utf8&quot;,&quot;root&quot;,&quot;xxxx&quot;);1private static final String DRIVERNAME = &quot;com.mysql.jdbc.Driver&quot;; private static final String URL = &quot;jdbc:mysql://123.207.149.102:4444/testjdbc?useUnicode=true&amp;characterEncoding=utf8&quot;; private static final String USERNAME = &quot;root&quot;; private static final String PASSWORD = &quot;xxxx&quot;; private static Connection conn = null;12345","categories":[{"name":"mysql","slug":"mysql","permalink":"/categories/mysql/"},{"name":"字符乱码","slug":"mysql/字符乱码","permalink":"/categories/mysql/%E5%AD%97%E7%AC%A6%E4%B9%B1%E7%A0%81/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"}]},{"title":"浅析计算机网络七层网络模型","slug":"浅析计算机网络七层网络模型","date":"2019-11-01T00:00:00.000Z","updated":"2019-11-01T06:04:35.624Z","comments":true,"path":"2019/11/01/浅析计算机网络七层网络模型/","link":"","permalink":"/2019/11/01/%E6%B5%85%E6%9E%90%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B8%83%E5%B1%82%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"计算机网络七层网络模型算机网络七层网络模型浅析计算机网络七层网络模型七层网络模型OSI 包括 7 层，它们分别是: 物理层、数据链路层、网络层、传输层、会话层、表示层和应用层 各层主要功能 物理层：实现透明的比特流传输。 数据链路层：在网络的相邻节点之间建立、维持和释放数据链路。 网络层：在两个端系统之间的通信子网上建立、维持和终止网络连接。 传输层：为会话层实体间的通信提供端到端的透明的数据传送。 会话层：组织、管理和同步两个实体之间的会话连接和它们之间的数据交换。 表示层：为应用层实体提供信息的公共表示方面的服务。 应用层：为应用进程提供访问 OSI 的手段。 各层的主要协议","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"面试","slug":"面试","permalink":"/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"英语语法/副词","slug":"英语语法/副词","date":"2019-10-30T06:56:02.543Z","updated":"2019-10-30T08:40:17.276Z","comments":true,"path":"2019/10/30/英语语法/副词/","link":"","permalink":"/2019/10/30/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E6%B3%95/%E5%89%AF%E8%AF%8D/","excerpt":"","text":"123title: 英语语法-副词date: 2019-10-22 19:17:37categories: 英语语法 副词1.副词的种类 时间副词: 如 often 地点副词: 如here 方式副词: 如hard 程度副词: 如very 疑问副词(放在特殊疑问句的句首): 如how 关系副词(放在定语从句举手): 如 when 连接副词(放在名词性从句句首): 如 how 2.副词的作用副词修饰动词, 形容词, 名词, 副词获全句, 在句子中作状语, 标语, 定语和补语 3.副词的位置 修饰形容词和副词是的位置: 副词修饰形容词和副词是, 一般位于被修饰词的前面 时间副词和地点副词: 表示确定时间的副词和表示地点的副词一般放在句尾, 若句子中溶蚀有地点副词和时间副词, 地点副词通常在前, 时间副词在后 程度副词: 一般放在被修饰的形容词和副词之前, 当修饰使实义动词时, 放在实义动词之后 enough作副词, 总实位语被修饰的形容词获副词之后 方式副词: 修饰不及物动词时, 要放在被修饰词之后 修饰及物动词时, 可放在被修饰动词之前获病愈之后, 如果并与较长, 也可以放在动词和宾语之间 “动词”+”副词”短语: 在这种情况下, 如果并于时人称代词, 则代次要放在副词之前, 宾语是名词时, 该名词放在副词前后均可, 宾语时句子是, 句子要放在副词之后.","categories":[],"tags":[]},{"title":"Java的四种引用方式","slug":"Java的四种引用方式","date":"2019-10-22T21:36:06.000Z","updated":"2019-10-22T13:55:46.631Z","comments":true,"path":"2019/10/23/Java的四种引用方式/","link":"","permalink":"/2019/10/23/Java%E7%9A%84%E5%9B%9B%E7%A7%8D%E5%BC%95%E7%94%A8%E6%96%B9%E5%BC%8F/","excerpt":"","text":"Java的四种引用方式java内存管理分为内存分配和内存回收，都不需要程序员负责，垃圾回收的机制主要是看对象是否有引用指向该对象。 java对象的引用包括 强引用 软引用 弱引用 虚引用 Java中提供这四种引用类型主要有两个目的： 第一是可以让程序员通过代码的方式决定某些对象的生命周期； 第二是有利于JVM进行垃圾回收。 四种类型引用的概念1. 强引用 指创建一个对象并把这个对象赋给一个引用变量。 12Object object =new Object();String str =\"hello\"; 强引用有引用变量指向时永远不会被垃圾回收，JVM宁愿抛出OutOfMemory错误也不会回收这种对象。 123456789&lt;pre name=\"code\" class=\"java\"&gt;public class Main &#123; public static void main(String[] args) &#123; new Main().fun1(); &#125; public void fun1() &#123; Object object = new Object(); Object[] objArr = new Object[1000]; &#125; 当运行至Object[] objArr = new Object[1000];这句时，如果内存不足，JVM会抛出OOM错误也不会回收object指向的对象。不过要注意的是，当fun1运行完之后，object和objArr都已经不存在了，所以它们指向的对象都会被JVM回收。 如果想中断强引用和某个对象之间的关联，可以显示地将引用赋值为null，这样一来的话，JVM在合适的时间就会回收该对象。 比如Vector类的clear方法中就是通过将引用赋值为null来实现清理工作的： 2. 软引用 如果一个对象具有软引用，内存空间足够，垃圾回收器就不会回收它； 如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。 软引用可用来实现内存敏感的高速缓存,比如网页缓存、图片缓存等。使用软引用能防止内存泄露，增强程序的健壮性。SoftReference的特点是它的一个实例保存对一个Java对象的软引用， 该软引用的存在不妨碍垃圾收集线程对该Java对象的回收。也就是说，一旦SoftReference保存了对一个Java对象的软引用后，在垃圾线程对 这个Java对象回收前，SoftReference类所提供的get()方法返回Java对象的强引用。 一旦垃圾线程回收该Java对象之 后，get()方法将返回null。 12MyObject aRef = new MyObject(); SoftReference aSoftRef=new SoftReference(aRef); 此时，对于这个MyObject对象，有两个引用路径，一个是来自SoftReference对象的软引用，一个来自变量aReference的强引用，所以这个MyObject对象是强可及对象。 随即，我们可以结束aReference对这个MyObject实例的强引用: 1aRef = null; 此后，这个MyObject对象成为了软引用对象。如果垃圾收集线程进行内存垃圾收集，并不会因为有一个SoftReference对该对象的引用而始终保留该对象。Java虚拟机的垃圾收集线程对软可及对象和其他一般Java对象进行了区别对待:软可及对象的清理是由垃圾收集线程根据其特定算法按照内存需求决定的。也就是说，垃圾收集线程会在虚拟机抛出OutOfMemoryError之前回收软可及对象，而且虚拟机会尽可能优先回收长时间闲置不用的软可及对象，对那些刚刚构建的或刚刚使用过的“新”软可反对象会被虚拟机尽可能保留。在回收这些对象之前，我们可以通过: 1MyObject anotherRef=(MyObject)aSoftRef.get(); 重新获得对该实例的强引用。而回收之后，调用*get()方法就只能得到*null了。使用ReferenceQueue清除失去了软引用对象的SoftReference：** 3. 弱引用（WeakReference） 弱引用也是用来描述非必需对象的，当JVM进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。 1234567891011121314151617181920212223public class test &#123; public static void main(String[] args) &#123; WeakReference&lt;People&gt;reference=new WeakReference&lt;People&gt;(new People(\"zhouqian\",20)); System.out.println(reference.get()); System.gc();//通知GVM回收资源 System.out.println(reference.get()); &#125; &#125; class People&#123; public String name; public int age; public People(String name,int age) &#123; this.name=name; this.age=age; &#125; @Override public String toString() &#123; return \"[name:\"+name+\",age:\"+age+\"]\"; &#125; &#125; 输出结果：name:zhouqian,age:20]null 第二个输出结果是null，这说明只要JVM进行垃圾回收，被弱引用关联的对象必定会被回收掉。不过要注意的是，这里所说的被弱引用关联的对象是指只有弱引用与之关联，如果存在强引用同时与之关联，则进行垃圾回收时也不会回收该对象（软引用也是如此）。 若进行如下修改,结果则不同 1234567891011121314151617181920212223public class test &#123; public static void main(String[] args) &#123; People people=new People(\"zhouqian\",20); WeakReference&lt;People&gt;reference=new WeakReference&lt;People&gt;(people);//关联强引用 System.out.println(reference.get()); System.gc(); System.out.println(reference.get()); &#125; &#125; class People&#123; public String name; public int age; public People(String name,int age) &#123; this.name=name; this.age=age; &#125; @Override public String toString() &#123; return \"[name:\"+name+\",age:\"+age+\"]\"; &#125; &#125;//结果发生了很大的变化 [name:zhouqian,age:20] [name:zhouqian,age:20] 4. 虚引用 虚引用和前面的软引用、弱引用不同，它并不影响对象的生命周期。在java中用java.lang.ref.PhantomReference类表示。如果一个对象与虚引用关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。 要注意的是，虚引用必须和引用队列关联使用，当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会把这个虚引用加入到与之 关联的引用队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 123456public class Main &#123; public static void main(String[] args) &#123; ReferenceQueue&lt;String&gt; queue = new ReferenceQueue&lt;String&gt;(); PhantomReference&lt;String&gt; pr = new PhantomReference&lt;String&gt;(new String(\"hello\"), queue); System.out.println(pr.get()); &#125; 软引用和弱引用 对于强引用，我们平时在编写代码时经常会用到。而对于其他三种类型的引用，使用得最多的就是软引用和弱引用，这2种既有相似之处又有区别。它们都是用来描述非必需对象的，但是被软引用关联的对象只有在内存不足时才会被回收，而被弱引用关联的对象在JVM进行垃圾回收时总会被回收。 Java是如何判断对象是否需要回收 常见的两种判断的算法： 引用计数算法 可达性分析算法（Java使用的这一种） 引用计数算法引用计数算法是在对象中加入一个计数器，当对象被引用，计数器+1，当引用失效，计数器-1 这种算法实现简单，效率高，但是有一个严重的问题会导致内存泄漏，那就是对象之间循环引用，比如说A对象持有B对象的引用，B对象持有A对象的引用，那么A和B的计数器值永远&gt;=1，也就是说这两个对象永远不会被回收 可达性分析算法Java中定义了一些起始点，称为GC Root，当有对象引用它的时候，就把对象挂载在它下面，形成一个树状结构，当一个对象处于一个这样的树里时，就认为此对象是可达的，反之是不可达，如下图 如图 Object1,Object2,Object3是对象可达的，Object4,Object5,Object6是不可达的 那么有哪些可以作为GC Root呢？ 静态属性（被static修饰的属性） 常量（被static final修饰的属性） 虚拟机栈（本地变量表）中引用的对象 本地方法栈中引用的对象","categories":[{"name":"Java","slug":"Java","permalink":"/categories/Java/"}],"tags":[{"name":"java","slug":"java","permalink":"/tags/java/"}]},{"title":"用时态数轴搞定完成时","slug":"英语语法/用时态数轴搞定完成时","date":"2019-10-22T19:17:37.000Z","updated":"2019-11-15T02:15:16.209Z","comments":true,"path":"2019/10/23/英语语法/用时态数轴搞定完成时/","link":"","permalink":"/2019/10/23/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E6%B3%95/%E7%94%A8%E6%97%B6%E6%80%81%E6%95%B0%E8%BD%B4%E6%90%9E%E5%AE%9A%E5%AE%8C%E6%88%90%E6%97%B6/","excerpt":"","text":"用“时态数轴”搞定完成时原文链接 一般时的动作发生在一个时间段内，完成时就不一样了，它强调的是动作在某个时间点之前已经完成，不用管在什么时间段发生的。 例如： Everybody had left before she got to the station. 这个句子中的谓语动词是leave，它在从句表明的时间之前就已经发生了，但不知道具体是在哪个时间段发生的，所以用过去完成时来表明“动作已经完成”。 那么怎样用“时态数轴”来解释完成时呢？ 在一般时中，我们用一个有界区间来表示动作发生的时间段。既然完成时只要求“截止时间”，那就可以考虑用一个左侧无界区间来表示动作发生的时间。 与一般时相同，我们可以从过去、现在、将来三个时间分别探讨一下完成时的用法。 一、过去完成时过去完成时的动作当然是发生在过去，并且动作的截止时间也是在过去。 在原点now的左边找到一个点，以这个点画出一个左侧无界区间，那么这个区间就是动作发生的时间。 例如： 过去完成时 截止时间是she got to the station，这是一个一般过去时的动作，所以它的落点在过去半轴，而主句的动作leave在它之前就已经发生了，所以是过去完成时had left。 再如： The students had been preparing for the college entrance exam for three years when the Education Ministry suddenly announced a change of regulations. 用传统语法的观点看，这是一个典型的过去完成进行时。但如果我们现在将be动词看作谓语动词，将后面的现在分词短语看作是主语补语，那么它就是一个过去完成时。 过去完成进行时 这个句子令人困惑的一点是，它有两处表示时间的地方：一个是for three years这个介词短语，另外一个是when引导的从句。 咋一看three years是一个明确的时间段，好像应该用一般过去时；但再看后面when引导的从句，又好像应该用过去完成时。 那么问题来了，我们到底应该以哪一个时间为准来判断句子的时态呢？ 判断的方法很简单，关键在于弄清楚句子的结构。我们要判断的是主句的时态，那当然要看主句的谓语动词发生在什么时间。 这个句子中，主句的谓语动词是be，preparing for the college entrance exam for three years是主语补语，when引导的从句修饰的是主句的动词be。 因此，for three years只是用来修饰分词短语中prepare这个动作的，主句动作发生的时间是由when引导的从句确定的，所以主句应该用的是过去完成时。 过去完成进行时经常会出现这种情况 ，我们只要能够搞清楚哪个时间修饰的是谓语动词，就能轻松地确定主句的时态。 另外，传统语法中说“过去完成时和过去完成进行时不能单独存在，一定要与一个一般过去时从句或者表示过去时间的从句、副词短语等连用。” 什么意思呢？用“时态数轴”来看的话，其实很容易理解。 完成时要求有一个“截止时间”，如果是过去完成时或过去完成进行时的话，这个截止点必须要落在过去半轴，那么假如一个句子没有给出过去的“截止时间”点，那就当然没法使用过去完成时，因为此时根本就画不出来一个落在过去半轴的左侧无界区间。 二、现在完成时如果没有明确交待“截止时间”，只强调动作已经完成，那通常就需要用到现在完成时，表示到现在为止，动作已经完成。 例如： I have seen that movie. 这个句子只说“我已经看过那部电影了”，但是具体什么时间看的没有说明，可以理解为在说这句话的时候动作已经完成，这个时间节点就是now。 现在完成时 由于完成时要用到无界区间，所以只需要以now为右端点画一个左侧无界区间即可，动作see发生在这个区间中，所以要用现在完成时have seen。 再来看一个现在完成进行时的例子： I have been living here for 20 years. 同样地，我们可以将be动词看成谓语动词，把其后的现在分词短语视为主语补语，将其转换为现在完成时。 现在完成进行时 那么这个句子的时间怎么确定？ 表面上看句子中有一个介词短语for 20 years，但通过分析句子结构可知，这个介词短语修饰的是live而不是be动词。be动词发生的时间没有明确，但从句意可知指的是“现在”，所以要用现在完成时。 三、将来完成时如果一个动作的截止时间是在将来，那么就需要用到将来完成时。 例如： By next month I will have worked at the company for 30 years. 不难看出这个句子动作的“截止时间”是next month，它强调“完成”的意思，所以要用将来完成时。 将来完成时 同理，将来完成进行时也可以通过将其视为将来完成时理解： By next month I will have been working at the company for 30 years. 将来完成进行时 至此，我们已经用“时态数轴”法解释了英语中所有常见的时态，所列举的例子都是比较典型的、简单的，目的是为了快速理解。熟练此方法之后，我们也可以去试着分析一些长难句，以增进对时态问题的掌握。 例如： Drenched and sputtering,Ron staggered sideways into Harry, just as a second water bomb dropped — narrowly missing Hermione, it burst at Harry’s feet,sending a wave of cold water over his sneakers into his socks.（Harry Potter and The Goblet of Fire） 你能试着用“时态数轴”分析这个句子的时态吗？","categories":[{"name":"英语语法","slug":"英语语法","permalink":"/categories/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E6%B3%95/"}],"tags":[{"name":"英语","slug":"英语","permalink":"/tags/%E8%8B%B1%E8%AF%AD/"}]},{"title":"查看mysql主从配置的状态及修正 slave不启动问题","slug":"mysql主从配置的状态及修正-slave不启动问题","date":"2019-10-22T19:17:37.000Z","updated":"2019-10-24T13:00:02.245Z","comments":true,"path":"2019/10/23/mysql主从配置的状态及修正-slave不启动问题/","link":"","permalink":"/2019/10/23/mysql%E4%B8%BB%E4%BB%8E%E9%85%8D%E7%BD%AE%E7%9A%84%E7%8A%B6%E6%80%81%E5%8F%8A%E4%BF%AE%E6%AD%A3-slave%E4%B8%8D%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98/","excerpt":"","text":"查看mysql主从配置的状态及修正 slave不启动问题原文链接 1、查看master的状态123show master status; //Position不应该为0show processlist; //state状态应该为Has sent all binlog to slave; waiting for binlog to be updated 2、查看slave状态123456show slave status;//Slave_IO_Running 与 Slave_SQL_Running 状态都要为Yesshow processlist;//应该有两行state值为：Has read all relay log; waiting for the slave I/O thread to update itWaiting for master to send event 3、错误日志MySQL安装目录 /usr/local/mysql MySQL日志目录 /usr/local/mysql/data/ 形如，Hostname.err 4、Change master to如果从库的Slave未启动，Slave_IO_Running为NO。可能是主库是的master的信息有变化，查看主库show master status;记录下File,Position字段，假设为‘mysql-bin.000004’,98;在从库执行： 123mysql&gt;stop slave; mysql&gt;change master to master_log_file=&apos;mysql-bin.000004&apos;,master_log_pos=98; mysql&gt;start slave; 5、SET global sql_slave_skip_counter=n;如果从库的slave_sql_running为NO。Err文件中记录：Slave:Error “Duplicate entry ‘1’ for key 1” on query…..可能是master未向slave同步成功，但slave中已经有了记录。造成的冲突可以在从库上执行set global sql_slave_skip_counter=n;跳过几步。再restart slave就可以了。 6、同步错误处理发现mysql slave服务器经常因为一些特殊字符或者符号产生的更新语句报错，整个同步也会因此而卡在那，最初的办法只是手动去出错的机器执行下面三条SQL语句，跳过错误即可。 123mysql&gt;slave stop; mysql&gt;set GLOBAL SQL_SLAVE_SKIP_COUNTER=1; mysql&gt;slave start; PS：本人多次遇到从数据库的同步进程自动停掉的问题，有时简单通过slave stop,slave start即可解决。有时slave start启动后又会自动停掉，这时使用 change master重设主数据库信息的方式解决了问题。 说明：Slave_IO_Running：连接到主库，并读取主库的日志到本地，生成本地日志文件 Slave_SQL_Running:读取本地日志文件，并执行日志里的SQL命令。","categories":[{"name":"Docker","slug":"Docker","permalink":"/categories/Docker/"},{"name":"Mysql","slug":"Docker/Mysql","permalink":"/categories/Docker/Mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"},{"name":"Docker","slug":"Docker","permalink":"/tags/Docker/"}]},{"title":"Have/Has 的主要用法","slug":"英语语法/havehas的用法","date":"2019-10-22T19:17:37.000Z","updated":"2019-11-01T07:25:43.648Z","comments":true,"path":"2019/10/23/英语语法/havehas的用法/","link":"","permalink":"/2019/10/23/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E6%B3%95/havehas%E7%9A%84%E7%94%A8%E6%B3%95/","excerpt":"","text":"Have/Has 的主要用法 主语为第三人称单数时用has ，其余人称都用have 。 当“有”讲，后面跟名词作宾语，如have a book 当“做某种动作”讲，后面跟与动词同形的名词，如have a look、have a talk、have a rest 当“进行”讲，后面可跟meeting（看会）、class（上课）、a meal（吃饭）、tea（喝茶）等名词 用作使役动词，后面跟复合宾语，如have my hair cut、have the door keep open 用作助动词，后面跟过去分词表示动作的完成","categories":[{"name":"英语语法","slug":"英语语法","permalink":"/categories/%E8%8B%B1%E8%AF%AD%E8%AF%AD%E6%B3%95/"}],"tags":[{"name":"英语","slug":"英语","permalink":"/tags/%E8%8B%B1%E8%AF%AD/"}]},{"title":"docker容器内网络请求缓慢问题","slug":"docker容器内网络请求缓慢问题","date":"2019-10-22T19:17:37.000Z","updated":"2019-10-24T12:10:58.073Z","comments":true,"path":"2019/10/23/docker容器内网络请求缓慢问题/","link":"","permalink":"/2019/10/23/docker%E5%AE%B9%E5%99%A8%E5%86%85%E7%BD%91%E7%BB%9C%E8%AF%B7%E6%B1%82%E7%BC%93%E6%85%A2%E9%97%AE%E9%A2%98/","excerpt":"","text":"docker容器内网络请求缓慢问题解决 原文链接:https://www.embbnux.com/2016/10/06/docker-container-network-too-slow-timeout/ 一、docker的网络模式1、none当配置为none时，docker容器网络无法输入输出，与世隔绝。 2、bridge默认为bridge模式，docker有自己的虚拟网卡，通过桥接的方式从主机获得网络。 3、host当指定为host时，主机的网卡直接暴露给了容器，直接通过主机的网络上网，比如要拿主机上的redis服务127.0.0.1:6357,就得通过这种方法，不过就比较不安全了。 4、container使用其他容器的网络 二、docker的dns解析 docker容器本质上也是个linux，所以dns的解析方法和linux一样，优先是找/etc/hosts文件，像localhost这种域名就是写在这个文件里，比如: 123456789101112127.0.0.1 localhost如果docker容器link了其他容器，这里也会多出link的域名，比如:docker run --name app --link app-redis:redis -d ubuntu就会在hosts里多出172.17.0.3 app-redis 038c8388e4a1找完/etc/hosts文件，然后是/etc/resolv.conf文件:domain localnameserver 192.168.65.1nameserver 192.168.65.10 三、解决docker容器里网络请求慢的问题 经过抓包测试等分析，发现网络请求慢，主要发生在dns解析中，所以主要采取dns优化:如果请求的是自己内网的api, 可以直接修改/etc/hosts文件，如果是外网的请求可以通过更改/etc/resolv.conf里的nameserver实现。docker容器肯定不是直接通过修改文件实现的，可以通过run的命令 1234# 添加hostdocker run --name app --add-host='api.embbnux.com:10.98.10.98' -d ubuntu# 指定dns serverdocker run --name app --dns=223.5.5.5 --dns=8.8.8.8 -d ubuntu 这样在docker容器里dns解析阶段的时间就被加速了","categories":[{"name":"Docker","slug":"Docker","permalink":"/categories/Docker/"}],"tags":[]},{"title":"Docker实用技巧之更改软件包源提升构建速度","slug":"Docker实用技巧之更改软件包源提升构建速度","date":"2019-10-22T19:17:37.000Z","updated":"2019-11-03T05:46:28.036Z","comments":true,"path":"2019/10/23/Docker实用技巧之更改软件包源提升构建速度/","link":"","permalink":"/2019/10/23/Docker%E5%AE%9E%E7%94%A8%E6%8A%80%E5%B7%A7%E4%B9%8B%E6%9B%B4%E6%94%B9%E8%BD%AF%E4%BB%B6%E5%8C%85%E6%BA%90%E6%8F%90%E5%8D%87%E6%9E%84%E5%BB%BA%E9%80%9F%E5%BA%A6/","excerpt":"","text":"Docker实用技巧之更改软件包源提升构建速度本文在原文的基础上做了一些修改和补充,有问题的地方欢迎指出 原文链接: https://www.cnblogs.com/stulzq/p/9339250.html 一. 问题说明我的一个开源项目提供了在线示例，项目代码在github，提交代码以后通过Jenkins持续集成，以Docker的方式运行。大家用过.NET Core的人应该都知道，.NET Core 默认是不带 System.Drawing的，前期有第三方的实现，为我们提供了一个解决方案，现在官方也提供了一个，以nuget包的方式发布，名为 System.Drawing.Common，实用这个包可以正常的进行各种图片操作，比如生成图片验证码，二维码等等，但是如果我们将其发布到linux，将会出现异常：Unable to load DLL &#39;libgdiplus&#39;。解决办法是，我们在构建Docker镜像的时候，可以通过命令装上libgdiplus，但是如果直接写命令apt-get install -y libgdiplus ，你会发现构建会出错，找不到这个包，我们需要在执行这个命令之前，执行apt-get update更新软件包源，那么问题来了，我在第一次构建Docker镜像（没有使用Cache）的执行 apt-get update命令时，非常的慢。最后整个构建过程花了12分钟。 构建的程序为 ASP.NET Core 2.1 应用程序，使用的基础镜像为微软官方提供的：microsoft/dotnet:2.1-aspnetcore-runtime 这不能忍啊，简直是太慢了，查看日志，发现这里执行非常慢： 123456After this operation, 38.8 MB of additional disk space will be used.Get:1 http://cdn-fastly.deb.debian.org/debian stretch/main amd64 libxau6 amd64 1:1.0.8-1 [20.7 kB]Get:2 http://cdn-fastly.deb.debian.org/debian stretch/main amd64 sgml-base all 1.29 [14.8 kB]Get:3 http://cdn-fastly.deb.debian.org/debian stretch/main amd64 libxml2 amd64 2.9.4+dfsg1-2.2+deb9u2 [920 kB]Get:4 http://cdn-fastly.deb.debian.org/debian stretch/main amd64 ucf all 3.0036 [70.2 kB]...此处省略28个，一共32个 应该是从 http://cdn-fastly.deb.debian.org/debian获取数据太慢导致的，所以，准备替换构建所使用的基础镜像的软件包源，准备替换为网易提供的包源 http://mirrors.163.com/ 二.问题解决–替换软件包源软件包源的配置文件在基础镜像所用的Linux系统中路径为 /etc/apt/sources.list，我们只需在执行 apt-get update命令之前，将我们编写好的使用网易包源的配置文件进行替换就行了。 使用网易包源的配置文件： sources.list 12345678deb http://mirrors.163.com/debian/ jessie main non-free contribdeb http://mirrors.163.com/debian/ jessie-updates main non-free contribdeb http://mirrors.163.com/debian/ jessie-backports main non-free contribdeb-src http://mirrors.163.com/debian/ jessie main non-free contribdeb-src http://mirrors.163.com/debian/ jessie-updates main non-free contribdeb-src http://mirrors.163.com/debian/ jessie-backports main non-free contribdeb http://mirrors.163.com/debian-security/ jessie/updates main non-free contribdeb-src http://mirrors.163.com/debian-security/ jessie/updates main non-free contrib 那么有一个问题,初始的时候docker的linux连vim都没有, 怎么更改文件的中内容呢??? 换个思路,不更改文件了 mv /etc/apt/sources.list /etc/apt/sources.list.bak 将原来的源文件拷贝一份 找到适合自己容器中linux的版本, 然后去阿里源找相应的替换源, 比如我的docker的linux版本是deblian, 使用echo 命令 123456789echo \"deb http://mirrors.163.com/debian/ jessie main non-free contribdeb http://mirrors.163.com/debian/ jessie-updates main non-free contribdeb http://mirrors.163.com/debian/ jessie-backports main non-free contribdeb-src http://mirrors.163.com/debian/ jessie main non-free contribdeb-src http://mirrors.163.com/debian/ jessie-updates main non-free contribdeb-src http://mirrors.163.com/debian/ jessie-backports main non-free contribdeb http://mirrors.163.com/debian-security/ jessie/updates main non-free contribdeb-src http://mirrors.163.com/debian-security/ jessie/updates main non-free contrib\" &gt; sources.listmv sources.list /etc/apt/sources.list 重启一下容器, 然后apt-get update 一下, 哈哈哈哈哈哈哈, 这速度,安装软件开心到飞起😁😘 Dockerfile: 123456FROM microsoft/dotnet:2.1-aspnetcore-runtimeWORKDIR /appCOPY . .RUN mv /etc/apt/sources.list /etc/apt/sources.list.bak &amp;&amp; mv sources.list /etc/apt/ &amp;&amp; apt-get update -y &amp;&amp; apt-get install -y libgdiplus &amp;&amp; apt-get clean &amp;&amp; ln -s /usr/lib/libgdiplus.so /usr/lib/gdiplus.dllEXPOSE 80ENTRYPOINT [&quot;dotnet&quot;, &quot;Alipay.Demo.PCPayment.dll&quot;] 主要是这两句命令： 1234#备份旧的配置文件mv /etc/apt/sources.list /etc/apt/sources.list.bak#替换为我们自定义的配置文件mv sources.list /etc/apt/ 需要注意的是，sources.list 需要放在我们打包的目录，保证能复制到镜像里面。 然后构建时间由12分钟缩短到37秒，这个过程是没有使用Docker Cache所花的时间： 三.其他加速1.腾讯云我的服务器是使用的腾讯云，腾讯云也提供了软件包源，分为内网和外网，外网是所有人都能使用，内网只能腾讯云的服务器使用。使用内网的包源将会获得更快的速度。详细说明：https://cloud.tencent.com/document/product/213/8623 使用内网的腾讯云包源配置文件： 12345678deb http://mirrors.tencentyun.com/debian/ jessie main non-free contribdeb http://mirrors.tencentyun.com/debian/ jessie-updates main non-free contribdeb http://mirrors.tencentyun.com/debian/ jessie-backports main non-free contribdeb-src http://mirrors.tencentyun.com/debian/ jessie main non-free contribdeb-src http://mirrors.tencentyun.com/debian/ jessie-updates main non-free contribdeb-src http://mirrors.tencentyun.com/debian/ jessie-backports main non-free contribdeb http://mirrors.tencentyun.com/debian-security/ jessie/updates main non-free contribdeb-src http://mirrors.tencentyun.com/debian-security/ jessie/updates main non-free contrib 2.阿里云阿里云作为一个全球第三的云平台运营商，也是具有此项服务的。其包源地址为：https://mirrors.aliyun.com 配置文件： 12345678deb https://mirrors.aliyun.com/debian/ jessie main non-free contribdeb https://mirrors.aliyun.com/debian/ jessie-updates main non-free contribdeb https://mirrors.aliyun.com/debian/ jessie-backports main non-free contribdeb-src https://mirrors.aliyun.com/debian/ jessie main non-free contribdeb-src https://mirrors.aliyun.com/debian/ jessie-updates main non-free contribdeb-src https://mirrors.aliyun.com/debian/ jessie-backports main non-free contribdeb https://mirrors.aliyun.com/debian-security/ jessie/updates main non-free contribdeb-src https://mirrors.aliyun.com/debian-security/ jessie/updates main non-free contrib 四.其他Linux系统镜像我用的Docker镜像所使用的Linux系统为 debian，如果你是用的不是 debian，那么你可以通过以下几个步骤来进行包源的更改。 方法一1.通过你所使用镜像官方提供的资料，查询出镜像所使用的Linux系统包源路径以及配置文件内容 2.替换加速地址 方法二1.使用你需要使用的镜像构建一个简单的程序，然后运行。 2.通过Docker交互模式，进入容器。 3.查询出使用的系统Linux镜像版本 4.找到并查看包源配置文件 5.复制配置文件内容，然后将地址替换为对应的加速地址","categories":[{"name":"Docker","slug":"Docker","permalink":"/categories/Docker/"}],"tags":[]},{"title":"部署 MySQL 8.0 主从复制","slug":"mysql8-0主从集群配置","date":"2019-10-22T19:17:37.000Z","updated":"2019-11-11T09:47:40.140Z","comments":true,"path":"2019/10/23/mysql8-0主从集群配置/","link":"","permalink":"/2019/10/23/mysql8-0%E4%B8%BB%E4%BB%8E%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE/","excerpt":"","text":"部署 MySQL 8.0 主从复制 镜像版本 mysql:8.0 参考文献: https://blog.csdn.net/why15732625998/article/details/80463041 MySQL 主从复制（也称 A/B 复制） 的原理 Master将数据改变记录到二进制日志(binary log)中，也就是配置文件log-bin指定的文件，这些记录叫做二进制日志事件(binary log events)； Slave 通过 I/O 线程读取 Master 中的 binary log events 并写入到它的中继日志(relay log)； Slave 重做中继日志中的事件， 把中继日志中的事件信息一条一条的在本地执行一次，完成数据在本地的存储， 从而实现将改变反映到它自己的数据(数据重放)。 MySQL安装基础1. 安装msyqlmsyql 的一些常用配置信息 在解压的路径下查看是否含有my.ini的文件，如果没有则新建一个 12345678910111213141516171819202122232425[mysqld]# 设置3306端口port=8307# 设置mysql的安装目录basedir=D:\\Program Files\\MySQL# 设置mysql数据库的数据的存放目录datadir=D:\\Program Files\\MySQL\\Data# 允许最大连接数max_connections=200# 允许连接失败的次数。max_connect_errors=10# 服务端使用的字符集默认为UTF8character-set-server=utf8# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB# 默认使用“mysql_native_password”插件认证#mysql_native_passworddefault_authentication_plugin=mysql_native_password[mysql]# 设置mysql客户端默认字符集default-character-set=utf8[client]# 设置mysql客户端连接服务端时默认使用的端口port=8307default-character-set=utf8 1.1 linux dockers设置初始参数1docker run --name mysql_master -p 3306:3306 -e MYSQL_ROOT_PASSWORD=admin -d mysql:8.0 1docker run --name mysql_slave -p 3307:3306 -e MYSQL_ROOT_PASSWORD=admin -d mysql:8.0 2. 更改数据库默认密码并授权访问更改密码如果设置处初始密码,可以不更改, 视情况而定 输入mysql -u root -p后会让你输入密码，密码为前面让你记住的密码，输入正确后就会出现如下界面，表示进入了MySQL命令模式。 新用户需要修改密码先，输入 123use mysql;ALTER USER &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;admin&apos; PASSWORD EXPIRE NEVER; #修改加密规则, 8.0 以后navicat连接会报错, 加密方案不支持ALTER USER &apos;root&apos;@&apos;%&apos; IDENTIFIED WITH mysql_native_password BY &apos;admin&apos;; #更新一下用户的密码 出现如下界面表示更改成功。 到此，MySQL8.0.12数据库就安装完成了。 更改权限查询用户权限情况 12use mysql;select User,authentication_string,Host from user; 可选配置：使用root远程登录输入 12grant all privileges on *.* to 'root'@'%';flush privileges; 3. 数据库相关配置查看默认数据库： 1show databases; 选择mysql数据库： 1use mysql; 查看默认MySQL用户的权限与加密情况： 1select host, user, authentication_string, plugin from user; 创建新用户并添加权限创建新用户： 1CREATE USER &apos;slave&apos;@&apos;%&apos; IDENTIFIED WITH mysql_native_password BY &apos;admin&apos;; 给新用户授权： 1GRANT ALL PRIVILEGES ON *.* TO &apos;slave&apos;@&apos;%&apos;; l刷新权限： 1FLUSH PRIVILEGES; 基础安装问题详解1. MySQL8.0.12不能连接Navicat原因：MySQL8.0与MySQL5.0所采用的加密方式规则不一样，所以导致 Navicat打不开。可一下命令查看密码的规则。 1select host, user, authentication_string, plugin from user; 如上图，plugin这一列就是对应用户的加密规则，可以看到我的root用户的加密规则是：mysql_native_password，这是因为我已经设置过了，默认的是：caching_sha2_password，所以我们只需要将默认的caching_sha2_password改为mysql_native_password即可。 解决方案：输入ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘新密码’;即可修改root用户的加密规则以及密码。 1ALTER USER &apos;root&apos;@&apos;%&apos; IDENTIFIED WITH mysql_native_password BY &apos;admin&apos;; 123ALTER USER &apos;root&apos;@&apos;%&apos; IDENTIFIED WITH mysql_native_password BY &apos;Aqgj,123&apos;;ALTER USER &apos;slave&apos;@&apos;%&apos; IDENTIFIED WITH mysql_native_password BY &apos;Aqgj,123&apos;;FLUSH PRIVILEGES; 2. 授权出错显示You are not allowed to create a user with GRANT 原因：在网上有很多教程说当出现The user specified as a definer (‘root’@’%’) does not exist时表示root用户权限不足，只需要执行GRANT ALL ON *.* TO ‘root’@’%’;就可以了，但是往往又会出现You are not allowed to create a user with GRANT的错误提示。这是因为GRANT ALL ON *.* TO ‘root’@’%’;这条语句中@’%’中的百分号其实是root用户对应host的名称，很多人并没有注意到他的root用户对应的其实是localhost，直接就执行了上面的语句，所以才会报错。 解决方案：只要将GRANT ALL ON *.* TO ‘root’@’%’;中的%改为对应的host名称即可，最后还要刷新一下权限FLUSH PRIVILEGES; 。 12GRANT ALL ON *.* TO &apos;root&apos;@&apos;%&apos;;FLUSH PRIVILEGES; 特别说明：网上说%表示通配所有的host，但是操作时并不成功，不明白是为什么，我猜想可能与MySQL8.0版本有关系。 配置数据库的主从复制 1.主从服务器操作系统版本和位数一致； 2.Master 和 Slave 数据库的版本要一致； 3.Master 和 Slave 数据库中的数据要一致； 4.Master 开启二进制日志， Master 和 Slave 的 server_id 在局域网内必须唯一； !!!! 如果docker 容器缺少vi软件, 可参考这篇文章Docker实用技巧之更改软件包源提升构建速度 1. 配置主库 注意！注意！注意！：配置文件一定要保存成ASNI存储方式，不然会报错 在主从的服务器上分别找到以下路径的my.ini文件 1.1 主库conf配置文件的mysqld节点下加入a) 普通模式配置123456789[mysqld]log-bin=master-bin #开启二进制日志 binlog#binlog-do-db=mytest #用于读写分离的具体数据库，这里我创建了mytest作测试log-bin-index=master-bin.index#binlog_ignore_db=mysql #不用于读写分离的具体数据库#binlog_ignore_db=information_schema #和binlog-do-db一样，可以设置多个#选择row模式 #binlog-format=ROWserver-id=1 #配置唯一的server-id b) git模式配置123456-----使用gtid模式 才使用如下配置---------[mysqld]server-id=1 #除了server-id不同外，其他配置主从都相同log-bin=master-bingtid-mode = onenforce-gtid-consistency = on 1.2 重启mysql服务1.3 在主数据库中创建用于从数据连接的用户123CREATE USER &apos;slave&apos;@&apos;%&apos; IDENTIFIED WITH mysql_native_password BY &apos;admin&apos;;GRANT REPLICATION SLAVE ON *.* TO &apos;slave&apos;@&apos;%&apos;;FLUSH PRIVILEGES; 1.4 获取主节点当前binary log文件名和位置（position）查询主数据库状态，并记录下File和Position字段的值 从数据库中会用到 1234567mysql&gt; show master status;+------------------+----------+--------------+--------------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+--------------------------+-------------------+| mysql-bin.000002 | 155 | mytest | mysql,information_schema | |+------------------+----------+--------------+--------------------------+-------------------+row in set (0.02 sec) 2. 配置从库2.1 从数据库conf配置从数据库 (同主库，必须指定中继日志的名称) , 从数据库配置的mysqld节点下加入 a) 传统模式12345678910111213[mysqld]#在[mysqld]中添加：server-id=2relay-log=slave-relay-binrelay-log-index=slave-relay-bin.index#replicate-do-db=test#备注：#server-id 服务器唯一标识，如果有多个从服务器，每个服务器的server-id不能重复，跟IP一样是唯一标识，如果你没设置server-id或者设置为0，则从服务器不会连接到主服务器。#relay-log 启动MySQL二进制日志，可以用来做数据备份和崩溃恢复，或主服务器挂掉了，将此从服务器作为其他从服务器的主服务器。#replicate-do-db 指定同步的数据库，如果复制多个数据库，重复设置这个选项即可。若在master端不指定binlog-do-db，则在slave端可用replication-do-db来过滤。#replicate-ignore-db 不需要同步的数据库，如果有多个数据库，重复设置这个选项即可。#其中需要注意的是，replicate-do-db和replicate-ignore-db为互斥选项，一般只需要一个即可。 修改完server-id 之后最好重启一下从库, 因为从库默认的server-id可能是1 与之前主库设置的id冲突了, 从库还没有读取新的设置项,需要重启更新一下 b) GIT模式1234567-----使用gtid模式 才使用如下配置---------[mysqld]server-id=2 #除了server-id不同外，其他配置主从都相同log-bin=master-bingtid-mode=onenforce-gtid-consistency=on 开启从数据库服务，手动创建测试读写分离的库，这边不会帮你自动创建，同时也创建和主库一样的用户，我这里还是先停止从库 2.2 在从数据库的msyql命令模式种执行以下命令1stop slave; 2.3 从（Slave）节点上设置主节点参数a)传统模式(下面的信息,一定要和自己的master的信息一直才行) 1234567mysql&gt; change master to master_host=&apos;138.138.0.95&apos;, master_port=3307, master_user=&apos;slave&apos;, master_password=&apos;admin&apos;, master_log_file=&apos;master-bin.000006&apos;, master_log_pos=822; 注：这里的 master_log_file和master_log_pos就是配置主数据库查询到的File和Position b) GIT模式如果使用了gtid模式, 则改为 12345mysql&gt; change master to MASTER_HOST=&apos;121.42.14.45&apos;, master_port=3306, MASTER_USER=&apos;slave&apos;, MASTER_PASSWORD=&apos;Aqgj,123&apos;, MASTER_AUTO_POSITION=1; 上面的命令是为了示例的完整性，如果使用GITD配置之前，两台服务器已经是传统的binlog主从服务器，则命令可以更简单： 如果上述报错了 关于@@global.gtid_mode=off, 则先执行 1set @@GLOBAL.GTID_MODE = OFF_PERMISSIVE; c) 传统该GIT模式如果设置gtid模式以前已经是常规的binlog模式,则只执行以下语句即可 1mysql&gt; change master to MASTER_AUTO_POSITION=1; #自动跟踪日志名称和位置 2.4 (Slave节点)启动从库1start slave; 2.5 (Slave节点)检查是否启动成功1show slave status\\G; 如果Slave_IO_State字段显示 Waiting for master to send event说明成功，当然你也可以在主库表中插入一条数据，看看从库是否有同步，到这里，已经配置好主从同步了。 Slave_IO_Running和Slave_SQL_Running的状态都为YES才表示同步成功！！！！ 般io的报错，都是server_id重复了 查看server_id的方法 1show variables like &apos;server_id&apos;; 至此，主从同步完成。可自行测试同步效果。 ==注意,从节点的同步,只会同步postion之后的操作, 必须在进行同步之前保证主从的数据是一直的, 否则就会导致主节点中的操作在从节点中不存在某表或者某数据导致异常== 3. MySQL主从复制的复制方式MySQL的主从复制并不完美，存在着几个由来已久的问题，首先一个问题是复制方式： 基于SQL语句的复制（statement-based replication,SBR） 基于行的复制（row-based replication,RBR） 混合模式复制（mixed-based replication,MBR） 全局事务标识符 GTID（Global Transaction Identifier,GTID） 基于SQL语句的方式是最古老的方式，也是目前默认的复制方式，后来的三种是MySQL 5以后才出现的复制方式。 3.1 SBR方式的优缺点SBR的优点 历史悠久，技术成熟binlog文件较小binlog中包含了所有数据库更改信息，可以据此来审核数据库的安全等情况binlog可以用于实时的还原，而不仅仅用于复制主从版本可以不一样，从服务器版本可以比主服务器版本高SBR的缺点： 不是所有的UPDATE语句都能被复制，尤其是包含不确定操作的时候复制需要进行全表扫描(WHERE 语句中没有使用到索引)的 UPDATE 时，需要比 RBR 请求更多的行级锁对于一些复杂的语句，在从服务器上的耗资源情况会更严重，而 RBR 模式下，只会对那个发生变化的记录产生影响数据表必须几乎和主服务器保持一致才行，否则可能会导致复制出错执行复杂语句如果出错的话，会消耗更多资源 3.2 RBR方式的优缺点RBR的优点 任何情况都可以被复制，这对复制来说是最安全可靠的和其他大多数数据库系统的复制技术一样多数情况下，从服务器上的表如果有主键的话，复制就会快了很多RBR 的缺点：binlog 大了很多 复杂的回滚时 binlog 中会包含大量的数据主服务器上执行 UPDATE 语句时，所有发生变化的记录都会写到 binlog 中，而 SBR 只会写一次，这会导致频繁发生 binlog 的并发写问题无法从 binlog 中看到都复制了写什么语句 3.3 混合方式混合方式就是有mysql自动选择RBR方式和SBR方式，能够充分发挥两种方式的优点，一般情况下都使用该种方式实现主从复制 3.4 全局事务标识符 GTID这种方式虽然能够大大提高主从复制的效率，减小主从复制的延时，但也存在问题，具体请参看下面的博客。https://blog.csdn.net/guotao521/article/details/45483833http://hamilton.duapp.com/detail?articleId=47 主从配置异常解决1. server-id报错报错内容：The slave I/O thread stops because master and slave have equal MySQL server id问题重现 执行命令 show slave status\\G; 异常点: Last_IO_Error: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work 1.1 server-id排查 主库： 1234567mysql&gt; show variables like &apos;server_id&apos;;+---------------+-------+| Variable_name | Value |+---------------+-------+| server_id | 1 |+---------------+-------+1 row in set (0.01 sec) 从库： 1234567mysql&gt; show variables like &apos;server_id&apos;;+---------------+-------+| Variable_name | Value |+---------------+-------+| server_id | 2 |+---------------+-------+1 row in set (0.01 sec) 若此处id重复了, 则是配置错了, 去修改配置即可, 参考上文第三节的配置 1.2 File 排查主库: 1234567mysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 154 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) 从库： 1234567mysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 306 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) File一样，排除。 1.3 auto.cnf中的server-uuid排查最后检查发现他们的auto.cnf中的server-uuid是一样的。。。 123[root@localhost ~]# vim /var/lib/mysql/auto.cnf[auto]server-uuid=4f37a731-9b79-11e8-8013-000c29f0700f 问题解决停止从库的mysqld服务，删除他的auto.cnf文件，再启动数据库服务即可： 123[root@localhost mysql]# systemctl stop mysqld.service[root@localhost mysql]# mv /var/lib/mysql/auto.cnf /var/lib/mysql/auto.cnf.bak[root@localhost mysql]# systemctl start mysqld.service 此时再去查看从库auto.cnf，已自动生成新的server-uuid： 123[root@localhost mysql]# vim /var/lib/mysql/auto.cnf[auto]server-uuid=2682888d-994a-11e8-aaf0-000c298cdafc 再查看从库状态，已正常 2. 重置主库的bin-log手动删除 可以直接删除binlog文件，但是可以通过mysql提供的工具来删除更安全，因为purge会更新mysql-bin.index中的条目，而直接删除的话，mysql-bin.index文件不会更新。mysql-bin.index的作用是加快查找binlog文件的速度。 （1）直接删除 找到binlog所在目录，用rm binglog名 直接删除 例：rm mysql-bin.010 （2）通过mysql提供的工具来删除 删除之前可以先看一下purge的用法：help purge; 删除举例(3种方法)： RESET MASTER; #删除所有binlog日志，新日志编号从头开始 flush logs; 1234567892. 3. PURGE MASTER LOGS TO &apos;mysql-bin.010&apos;;[//删除mysql-bin.010之前所有日志](https://xn--mysql-bin-ku4om121d.xn--010-db9d86pgtxn1dj3icvc/)4. PURGE MASTER LOGS BEFORE &apos;2003-04-02 22:46:26&apos;;// 删除2003-04-02 22:46:26之前产生的所有日志### 3. 重置从库 MySQL彻底清除slave信息 在我们的MySQL，Master和Slave进行主从切换的时候，Slave成功升级为主库，那么这个时候就需要彻底清理从库的信息，不然监控系统会认为这台服务器是Slave，而且会报主从同步失败。其实非常的简单，只需要以下两步： mysql&gt; stop slave;mysql&gt; reset slave all; reset slave all;是清除从库的同步复制信息、包括连接信息和二进制文件名、位置。从库上执行这个命令后，使用show slave status将不会有输出。","categories":[{"name":"Mysql","slug":"Mysql","permalink":"/categories/Mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"},{"name":"MySQL集群","slug":"MySQL集群","permalink":"/tags/MySQL%E9%9B%86%E7%BE%A4/"}]},{"title":"mysql8.0连接报2059错误","slug":"mysql8-0连接报2059错误","date":"2019-10-22T18:51:44.000Z","updated":"2019-11-15T02:54:00.296Z","comments":true,"path":"2019/10/23/mysql8-0连接报2059错误/","link":"","permalink":"/2019/10/23/mysql8-0%E8%BF%9E%E6%8E%A5%E6%8A%A52059%E9%94%99%E8%AF%AF/","excerpt":"","text":"1.环境说明宿主机：win10 docker：Version 18.06.1-ce-win73 (19507) mysql：8.x（docker store中最新版） 2.报错描述使用navicat连接工具，连接报错如下： 3.报错原因说明由于新版本的MySQL新特性导致的，通过查询mysql 1mysql&gt; select Host,User,plugin from mysql.user; 查询结果+———–+——————+———————–+| Host | User | plugin |+———–+——————+———————–+| % | root | caching_sha2_password || % | vuluser | caching_sha2_password | 查询结果如上，在老版本里，一般使用加密方式为mysql_native_password，因为认证方式改变导致的。 4.修改方案 修改加密方式为老版本加密方式。 5.修改5.1 查看docker容器IDdocker ps -a 5.2 docker exec ：在运行的容器中执行命令 1docker exec -it 236b2624632d bash # 进入到自己的容器中操作 5.3 连接mysql1mysql -u root -p 5.4 修改加密方式执行命令，将用户的加密方式改为mysql_native_password，密码为root。 1alter user 'root'@'%' identified with mysql_native_password by 'admin'; 5.5 执行命令flush privileges使权限配置项立即生效1flush privileges;","categories":[{"name":"MySQL","slug":"MySQL","permalink":"/categories/MySQL/"},{"name":"Docker","slug":"MySQL/Docker","permalink":"/categories/MySQL/Docker/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"},{"name":"docker","slug":"docker","permalink":"/tags/docker/"},{"name":"避坑","slug":"避坑","permalink":"/tags/%E9%81%BF%E5%9D%91/"}]},{"title":"详解重载与重写","slug":"详解重载与重写","date":"2019-10-21T15:11:32.000Z","updated":"2019-10-21T07:32:42.119Z","comments":true,"path":"2019/10/21/详解重载与重写/","link":"","permalink":"/2019/10/21/%E8%AF%A6%E8%A7%A3%E9%87%8D%E8%BD%BD%E4%B8%8E%E9%87%8D%E5%86%99/","excerpt":"","text":"详解重载与重写 Overload是重载的意思，Override是覆盖的意思，也就是重写。 重载Overload:表示同一个类中可以有多个名称相同的方法，但这些方法的参数列表各不相同（即参数个数或类型不同) 重写Override:表示子类中的方法可以与父类中的某个方法的名称和参数完全相同，通过子类创建的实例对象调用这个方法时，将调用子类中的定义方法，这相当于把父类中定义的那个完全相同的方法给覆盖了，这也是面向对象编程的多态性的一种表现。 重写子类抛出的异常&lt;=父类** 子类覆盖父类的方法时，只能比父类抛出更少的异常，或者是抛出父类抛出的异常的子异常，因为子类可以解决父类的一些问题，不能比父类有更多的问题。 子类权限&gt;= 父类 子类方法的访问权限只能比父类的更大，不能更小。如果父类的方法是private类型，那么，子类则不存在覆盖的限制，相当于子类中增加了一个全新的方法。 返回值必须相同 至于Overloaded的方法是否可以改变返回值的类型这个问题，要看你倒底想问什么呢？这个题目很模糊。如果几个Overloaded的方法的参数列表不一样，它们的返回者类型当然也可以不一样。但我估计你想问的问题是：如果两个方法的参数列表完全一样，是否可以让它们的返回值不同来实现重载Overload。这是不行的，我们可以用反证法来说明这个问题，因为我们有时候调用一个方法时也可以不定义返回结果变量，即不要关心其返回结果，例如，我们调用map.remove(key)方法时，虽然remove方法有返回值，但是我们通常都不会定义接收返回结果的变量，这时候假设该类中有两个名称和参数列表完全相同的方法，仅仅是返回类型不同，java就无法确定编程者倒底是想调用哪个方法了，因为它无法通过返回结果类型来判断。 重写注意事项 1、覆盖的方法的标志必须要和被覆盖的方法的标志完全匹配，才能达到覆盖的效果； 2、覆盖的方法的返回值必须和被覆盖的方法的返回一致； 3、覆盖的方法所抛出的异常必须和被覆盖方法的所抛出的异常一致，或者是其子类； 4、被覆盖的方法不能为private，否则在其子类中只是新定义了一个方法，并没有对其进行覆盖。 重载overload 对我们来说可能比较熟悉，可以翻译为重载，它是指我们可以定义一些名称相同的方法，通过定义不同的输入参数来区分这些方法，然后再调用时，VM就会根据不同的参数样式，来选择合适的方法执行。 重载要注意以下的几点： 1、在使用重载时只能通过不同的参数样式。例如，不同的参数类型，不同的参数个数，不同的参数顺序（当然，同一方法内的几个参数类型必须不一样，例如可以是fun(int,float)，但是不能为fun(int,int)）； 2、不能通过访问权限、返回类型、抛出的异常进行重载； 3、方法的异常类型和数目不会对重载造成影响； 4、对于继承来说，如果某一方法在父类中是访问权限是priavte，那么就不能在子类对其进行重载，如果定义的话，也只是定义了一个新方法，而不会达到重载的效果。(其本质是private方法是属于父类的, 子类无法访问到,相当于子类根本没有该方法 不能对其重载) 总结重写: 关心方法的访问权限, 方法的异常抛出类型, 以及返回类型是否一致 重载: 只关心方法参数的个数, 类型, 顺序","categories":[{"name":"Java","slug":"Java","permalink":"/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"/tags/Java/"}]},{"title":"Main","slug":"Main","date":"2019-10-21T08:27:40.103Z","updated":"2019-10-21T08:31:50.215Z","comments":true,"path":"2019/10/21/Main/","link":"","permalink":"/2019/10/21/Main/","excerpt":"","text":"typora流程图画法1.1 流程图1.1 横向流程图源码格式:123456graph LRA[方形] --&gt; B(圆角) B --&gt; C&#123;条件a&#125; C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2] F[横向流程图] 1.2 竖向流程图源码格式:123456graph TDA[方形] --&gt; B(圆角) B --&gt; C&#123;条件a&#125; C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2]F[竖向流程图] 1.3 标准流程图源码格式:123456789st=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st-&gt;op-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op 1.4 标准流程图源码格式(横向):123456789st=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框(是或否?)sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st(right)-&gt;op(right)-&gt;condcond(yes)-&gt;io(bottom)-&gt;econd(no)-&gt;sub1(right)-&gt;op 1.2 UML时序图1.2.1 UML时序图源码样例:12345对象A-&gt;对象B: 对象B你好吗? (请求)Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B --&gt; 对象A: 我很好(响应)对象A --&gt; 对象B: 你真的好吗? 1.2.2 UML时序图源码复杂样例:1234567891011Title: 标题: 复杂使用对象A -&gt; 对象B: 对象B你好吗? (请求)Note right of 对象B: 对象B的描述Note right of 对象A: 对象A的描述(提示)对象B --&gt; 对象A: 我很好(响应)对象B --&gt; 小三: 你好吗?小三 -&gt; 对象A: 对象B找我了对象A --&gt; 对象B: 你真的好吗?Note over 小三, 对象B: 我们是朋友participant CNote right of C: 没人陪我玩 1.2.3 UML标准时序图样例:123456789101112%%时序图例子, -&gt; 实线, --&gt; 虚线, -&gt;&gt; 实线箭头 sequenceDiagram participant 张三 participant 李四 张三 -&gt; 王五: 王五你好吗? loop 健康检查 王五 -&gt; 王五: 与疾病战斗 end Note right of 王五: 合理饮食 &lt;br/&gt;看医生... 李四 -&gt;&gt; 张三: 很好! 王五 -&gt; 李四: 你怎么样? 李四 --&gt; 王五: 很好! 1.3 甘特图样例:12345678910111213141516171819202122%%语法示例 gantt dateFormat YYYY-MM-DD title 软件开发甘特图 section 设计 需求 :done, des1, 2014-01-06, 2014-01-08 原型 :active, des2, 2014-01-09, 3d UI设计 :des3, after des2, 5d 未来任务: :des4, after des3, 5d section 开发 学习准备理解需求 :crit, done, 2014-01-06, 24h 设计框架 :crit, done, after des2, 2d 开发 :crit, active, 3d 未来任务 :crit, 5d 耍 :2d section 测试 功能测试 :active, a1, after des3, 3d 压力测试 :after a1, 20h 测试报告 :48h","categories":[],"tags":[]},{"title":"Double和Float的区别","slug":"Double和Float的区别","date":"2019-10-20T20:55:32.000Z","updated":"2019-10-20T13:20:32.296Z","comments":true,"path":"2019/10/21/Double和Float的区别/","link":"","permalink":"/2019/10/21/Double%E5%92%8CFloat%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Double和Float的区别 对编程人员来说，double 和 float 的区别是double精度高， 单精度浮点数有效数字8位，双精度浮点数有效数字16位。 但double消耗内存是float的两倍，double的运算速度比float慢得多，C语言中数学函数名称double 和 float不同，不要写错，能用单精度时不要用双精度（以省内存，加快运算速度）。 Double与Float的存储方式java语言中，float类型数字在计算机中用4个字节（32位）来存储。double类型占用8个字节（64位）。 从存储结构和算法上来讲，double和float是一样的，不一样的地方仅仅是float是32位的，double是64位的，所以double能存储更高的精度。 按照IEEE制定的浮点数表示法来进行float,double运算。这种结构是一种科学计数法：用符号、指数和尾数来表示。指数可正可负，所以，IEEE规定，此处算出的次方必须减去127才是真正的指数。底数定为2，即把一个浮点数表示为尾数乘以2的指数次方再添上符号。 下面是具体的规格： 类型 符号位 指数 尾数 长度 float 1 8 23 32 double 1 11 52 64 以float为例： 因为指数需要减去127，所以float类型的指数可从-126到128。 科学计数法： SEEEEEEE EMMMMMMM MMMMMMMM MMMMMMMM S表示浮点数正负 E表示指数加上127的值后得到的二进制数据 ==M表示尾数，最高位固定为1== 举例：17.625在内存中的存储为： 首先要把17.625换算成二进制：10001.101。 整数部分：除以2，直到商为0，余数反转。（即：模2取余法） 17 % 2 = 8 —&gt; 1 低位 8 % 2 = 4 —&gt; 0 4 % 2 = 2 —&gt; 0 2 % 2 = 1 —&gt; 0 1 % 2 = 0 —&gt; 1 高位 所以整数部分的二进制表示= 10001 小数部分：乘以2，直到乘位为0，进位顺序取。（即：乘2取整法） 按如下算法进行： 1）首先给小数部分乘2，得到的数，如果小数点前为1；则计1，为0，则计0。 2）再对剩下的小数部分乘2，再计出1或0。 3）重复以上步骤，直至达到需要的精度。 0.625 x 2 = 1.25 —&gt; 1 -1位 0.25 x 2 = 0.5 —&gt; 0 -2位 0.5 x 2 = 1.0 —&gt; 1 -3位 0.0 x 2 = 0.0 —&gt; 0 -4位 ……. 所以小数部分得到的二进制表示= 0101 即101 以上得到17.625换算成二进制为10001.101。 再将10001.101右移，直到小数点前只剩1位，1.0001101 * 2^4 ，右移动了四位。 此时，底数和指数就出来了。 底数：因为小数点前必为1，所以IEEE规定只记录小数点后的就好。所以，此处的底数为：0001101 指数：实际为4，必须加上127(转出的时候，减去127)，所以为131。也就是10000011 符号：因为是正数，所以是0 综上所述，17.625在内存中的存储格式是： 0 (符号)+ 10000011 (指数)+ 0001101(底数)+00000000 00000000 (补0, 凑满32位) 即=&gt;01000001 10001101 00000000 00000000","categories":[{"name":"Java","slug":"Java","permalink":"/categories/Java/"},{"name":"基本类型","slug":"Java/基本类型","permalink":"/categories/Java/%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B/"}],"tags":[{"name":"Java","slug":"Java","permalink":"/tags/Java/"}]},{"title":"Java修饰符详解","slug":"Java修饰符详解","date":"2019-10-20T20:27:01.000Z","updated":"2019-10-20T13:32:12.113Z","comments":true,"path":"2019/10/21/Java修饰符详解/","link":"","permalink":"/2019/10/21/Java%E4%BF%AE%E9%A5%B0%E7%AC%A6%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"Java修饰符Java修饰符用来定义类、方法、变量，通常放在语句的最前端，修饰符主要分为两类： 访问控制修饰符 非访问修饰符 访问控制修饰符访问控制修饰符定义类、变量、方法的访问权限。Java支持4种访问控制修饰符：public, protected, default, private. public：修饰类、接口、方法、变量，所修饰的对象对所有类可见。 protected：修饰变量和方法，所修饰的对象对同一包内的类可见，若子类不在同一个包内，则该子类只能访问其继承的成员。（详情见Java protected 关键字详解） default：修饰类、接口、方法、变量，当缺省访问控制修饰符时默认为default，所修饰的对象在同一包内可见。 private：修饰变量和方法，所修饰的对象只在同一类内可见。用private修饰的变量不能被外部直接访问，通常需要通过getter和setter方法间接访问，实现对对象成员变量的保护。 修饰符 当前类 同一包内 子孙类（同一包） 子孙类（不同包） 其他包 public Y Y Y Y Y protected Y Y Y Y/N详解 N default Y Y Y N N private Y N N N N 非访问修饰符非访问修饰符实现一些其他功能 static 修饰符，用来修饰类方法和类变量。static修饰的变量和方法称为静态变量和静态方法，也叫类变量和类方法，静态变量和静态方法不随着创建对象而创建，它们属于类本身所有。 final 修饰符，用来修饰类、方法和变量。 final修饰的类不能够被继承。 final修饰的方法可以被继承，但是不能被子类重新定义。 修饰的变量为常量，是不可修改的，用final修饰的常量在创建时必须赋值。 注意!!! final修饰的变量如果是一个基本类型,那么它的值不能够再改变,否则编译报错, 如果final修饰的变量是一个引用类型,如数组,那么表示改变量不能再指向其他变量,但改变量里的值是可以改变的 如final x []=new int[10], 我们为x[1] 赋值是不是报错的. 12345678910public class Test&#123; final int value = 10; // 下面是声明常量的实例 public static final int BOXWIDTH = 6; static final String TITLE = \"Manager\"; public void changeValue()&#123; value = 12; //将输出一个错误 &#125;&#125; abstract修饰符，用来创建抽象类和抽象方法。 synchronized修饰符声明的方法同一时间只能被一个线程访问。 volatile修饰的成员变量在每次被线程访问时，都强制从共享内存中重新读取该成员变量的值。而且，当成员变量发生变化时，会强制线程将变化值回写到共享内存。这样在任何时刻，两个不同的线程总是看到某个成员变量的同一个值。 transient 序列化的对象包含被 transient 修饰的实例变量时，java 虚拟机(JVM)跳过该特定的变量。 ==一旦一个类被static修饰, 那么它就变成了一个顶级类==","categories":[{"name":"Java","slug":"Java","permalink":"/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"/tags/Java/"}]},{"title":"Java静态代理与动态代理的区别","slug":"Java静态代理与动态代理的区别","date":"2019-10-20T19:50:51.000Z","updated":"2019-10-20T12:05:48.145Z","comments":true,"path":"2019/10/21/Java静态代理与动态代理的区别/","link":"","permalink":"/2019/10/21/Java%E9%9D%99%E6%80%81%E4%BB%A3%E7%90%86%E4%B8%8E%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"Java中的静态代理和动态代理什么是代理?代理是设计模式的一种，代理类为委托类提供消息预处理，消息转发，事后消息处理等功能。Java中的代理分为三种角色： 代理类(ProxySubject) 委托类(RealSubject) 接口(Subject) 三者关系可以表示如下图: 代理模式 Java中的代理按照代理类生成时机不同又分为静态代理和动态代理。 静态代理: 代理类在编译期就生成 动态代理: 代理类则是在Java运行时动态生成。 静态代理Java中的静态代理要求代理类(ProxySubject)和委托类(RealSubject)都实现同一个接口(Subject)。静态代理中代理类在编译期就已经确定，而动态代理则是JVM运行时动态生成，静态代理的效率相对动态代理来说相对高一些，但是静态代理代码冗余大，一单需要修改接口，代理类和委托类都需要修改。举个例子： 接口(Subject)： 123interface HelloService &#123; void sayHello();&#125; 委托类： 123456class HelloServiceImpl implements HelloService &#123; @Override public void sayHello() &#123; System.out.println(\"Hello World!\"); &#125;&#125; 代理类： 1234567891011121314class HelloServiceProxy implements HelloService &#123; private HelloService helloService; public HelloServiceProxy(HelloService helloService) &#123; this.helloService = helloService; &#125; @Override public void sayHello() &#123; System.out.println(\"Before say hello...\"); helloService.sayHello(); System.out.println(\"After say hello...\"); &#125;&#125; 测试类： 12345678public class HelloServiceProxyTest &#123; public static void main(String[] args) &#123; HelloService helloService = new HelloServiceImpl(); HelloServiceProxy proxy = new HelloServiceProxy(helloService); proxy.sayHello(); &#125;&#125; 输出结果： 123Before say hello...Hello World!After say hello... 动态代理 实现原理是通过Java的反射机制,在运行的时候通过反射机制,动态的调用方法 Java中的动态代理依靠反射来实现，代理类和委托类不需要实现同一个接口。委托类需要实现接口(InvocationHandler)，否则无法创建动态代理。代理类在JVM运行时动态生成，而不是编译期就能确定。Java动态代理主要涉及到两个类：java.lang.reflect.Proxy和java.lang.reflect.InvocationHandler。代理类需要实现InvocationHandler接口或者创建匿名内部类，而Proxy用于创建动态动态。我们用动态代理来实现HelloService： 接口(Subject)： 123interface HelloService &#123; void sayHello();&#125; 委托类： 123456class HelloServiceImpl implements HelloService &#123; @Override public void sayHello() &#123; System.out.println(\"Hello World!\"); &#125;&#125; 动态代理类： 12345678910111213141516171819class HelloServiceDynamicProxy &#123; private HelloService helloService; public HelloServiceDynamicProxy(HelloService helloService) &#123; this.helloService = helloService; &#125; public Object getProxyInstance() &#123; return Proxy.newProxyInstance(helloService.getClass().getClassLoader(), helloService.getClass().getInterfaces(), new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(\"Before say hello...\"); Object ret = method.invoke(helloService, args); System.out.println(\"After say hello...\"); return ret; &#125; &#125;); &#125;&#125; 测试类： 1234567public class HelloServieDynamicProxyTest &#123; public static void main(String[] args)&#123; HelloService helloService = new HelloServiceImpl(); HelloService dynamicProxy = (HelloService) new HelloServiceDynamicProxy(helloService).getProxyInstance(); dynamicProxy.sayHello(); &#125;&#125; 输出结果： 123Before say hello...Hello World!After say hello... 总结 静态代理实现较简单，代理类在编译期生成，效率高。缺点是会生成大量的代理类, 并且需要特定的编译器。 JDK动态代理不要求代理类和委托类实现同一个接口，但是委托类需要实现接口，代理类需要实现InvocationHandler接口。 动态代理要求代理类InvocationHandler接口，通过反射代理方法，比较消耗系统性能，但可以减少代理类的数量，使用更灵活, 且不需要特定的编译器支持。 转载自: https://www.jianshu.com/p/f56e123817b5 , 本文对其进行了一些补充","categories":[{"name":"Java","slug":"Java","permalink":"/categories/Java/"},{"name":"代理模式","slug":"Java/代理模式","permalink":"/categories/Java/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Java","slug":"Java","permalink":"/tags/Java/"},{"name":"代理模式","slug":"代理模式","permalink":"/tags/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"}]},{"title":"排序算法","slug":"排序算法","date":"2019-10-20T19:36:56.000Z","updated":"2019-10-20T11:38:07.588Z","comments":true,"path":"2019/10/21/排序算法/","link":"","permalink":"/2019/10/21/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","excerpt":"","text":"排序算法插入排序原理算法原理：从整个待排序列中选出一个元素插入到已经有序的子序列中去，得到一个有序的、元素加一的子序列，直到整个序列的待插入元素为0，则整个序列全部有序。 具体的实现的时候，我们一般选择第一个元素作为有序的序列，将后面的元素插入到前面有序的序列直到整个序列有序。 时间复杂度：插入排序在最好情况下，需要比较n-1次，无需交换元素，时间复杂度为O(n);在最坏情况下，时间复杂度为O(n^2) 123456789101112131415161718192021222324252627282930#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cmath&gt;#include&lt;algorithm&gt;using namespace std;const int len=15;int main()&#123; int a[len]=&#123;1,87,64,19,53,14,57,62,23,37,48,9,91,45,81&#125;; for(int i=1;i&lt;len;i++) &#123; int j=i; int temp=a[i]; while(j&gt;0) &#123; if(a[j-1]&lt;a[i])//找到第一个比它小的数的位置 &#123; for(int k=i;k&gt;j;k--)//比它大的数全部后移 a[k]=a[k-1]; a[j]=temp;//将数值附到该位置上 break; &#125; j--; &#125; &#125; for(int i=0;i&lt;len;i++) printf(\"%d \",a[i]); printf(\"\\n\"); return 0;&#125; 选择排序原理算法原理：为每一趟从待排序的数据元素中选择最小（或最大）的一个元素作为首元素，直到所有元素排完为止 算法步骤： 首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置 再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。 重复第二步，直到所有元素均排序完毕。 时间复杂度：无论数组原始排列如何，比较次数是不变的；对于交换操作，在最好情况下也就是数组完全有序的时候，无需任何交换移动， 在最差情况下，也就是数组倒序的时候，交换次数为n-1次。综合下来，时间复杂度为O(n^2) 12345678910111213141516171819202122232425#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cmath&gt;#include&lt;algorithm&gt;using namespace std;const int len=15;int main()&#123; int a[len]=&#123;1,87,64,19,53,14,57,62,23,37,48,9,91,45,81&#125;; for(int i=0;i&lt;len-1;i++) &#123; int temp=i; for(int j=i+1;j&lt;len;j++) &#123; if(a[temp]&gt;a[j]) temp=j; &#125; if(temp!=i) swap(a[temp],a[i]); &#125; for(int i=0;i&lt;len;i++) printf(\"%d \",a[i]); printf(\"\\n\"); return 0;&#125; 冒泡排序原理算法原理：比较相邻的元素。如果第一个比第二个大，就交换他们两个 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 时间复杂度分析：最优时间O(n），最差时间O(n^2)。 12345678910111213141516171819202122232425262728#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cmath&gt;#include&lt;algorithm&gt;using namespace std;const int len=15;int main()&#123; int a[len]=&#123;1,87,64,19,53,14,57,62,23,37,48,9,91,45,81&#125;; for(int i=0;i&lt;len-1;i++)//进行几轮比较，确定位置 &#123; bool flag=false;//设定一个标记，若为true，则表示此次循环没有进行交换，也就是待排序列已经有序，排序已然完成。 for(int j=0;j&lt;len-1-i;j++) &#123; if(a[j]&gt;a[j+1]) &#123; swap(a[j],a[j+1]); flag=true; &#125; &#125; if(flag==false) break; &#125; for(int i=0;i&lt;len;i++) printf(\"%d \",a[i]); printf(\"\\n\"); return 0;&#125; 归并排序原理算法原理：是利用递归与分治的技术将数据序列划分为越来越小的半子表，再对半子表排序，最后再用递归方法将排好序的半子表合并成越来越大的有序序列。 时间复杂度：O(nlogn） 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;iostream&gt;#include &lt;cstdio&gt;using namespace std;int a[20]=&#123;5,6,9,15,4,-1,-9,5,-6,71,5,-36,2,15,48,-15,14,6,9,11&#125;;int l,r;void mergee(int l,int m,int r)&#123; int T[20]; int i=l,j=m+1; int k=0; while(i&lt;=m&amp;&amp;j&lt;=r) &#123; if(a[i]&lt;=a[j]) T[k++]=a[i++]; else T[k++]=a[j++]; &#125; while(i&lt;=m) T[k++]=a[i++]; while(j&lt;=r) T[k++]=a[j++]; for(int i=0;i&lt;k;i++) a[l+i]=T[i];&#125;int mergr_sort(int l,int r)&#123; if(r-l&gt;0) &#123; int mid=(l+r)/2; int p=l,q=mid,i=l; mergr_sort(l,mid); mergr_sort(mid+1,r); mergee(l,mid,r); &#125;&#125;int main()&#123; scanf(\"%d %d\",&amp;l,&amp;r); mergr_sort(l,r); for(int i=0;i&lt;20;i++) printf(\"%d \",a[i]); printf(\"\\n\"); return 0;&#125; 快速排序原理通过一轮的排序将序列分割成独立的两部分，其中一部分序列的关键字（这里主要用值来表示）均比另一部分关键字小。继续递归的对长度较短的序列进行同样的分割，最后到达整体有序。为了实现一次划分，我们可以从数组（假定数据是存在数组中）的两端移动下标，必要时交换记录，直到数组两端的下标相遇为止。为此，我们附设两个指针（下角标）i和j， 通过j 从当前序列的有段向左扫描，越过不小于基准值的记录。当遇到小于基准值的记录时，扫描停止。通过i从当前序列的左端向右扫描，越过小于基准值的记录。当遇到不小于基准值的记录时，扫描停止。交换两个方向扫描停止的记录 a[j] 与 a[i]。 然后，继续扫描，直至 i与j 相遇为止。它的平均时间复杂度为O(nlogn)。 当我们每次进行分区划分时，如果每次选择的基准元素都是当前序列中最大或最小的记录，这样每次分区的时候只得到了一个新分区，另一个分区为空，并且新分区只是比分区前少一个元素，这是快速排序的最坏情况，时间复杂度上升为O(n^2)。 1234567891011121314151617181920212223242526272829303132333435#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;cmath&gt;#include&lt;algorithm&gt;using namespace std;const int len=15;int a[len]=&#123;1,87,64,19,53,14,57,62,23,37,48,9,91,45,81&#125;;void quicksort(int l,int r)&#123; if(l&gt;r) return; int temp=a[l]; int i=l;int j=r; while(i!=j) &#123; while(a[j]&gt;=temp&amp;&amp;i&lt;j) j--;/*为什么要从右边，因为我们选择的基数是从左边开始选择的，开始的方向必须是要从基数的对面开始，如果你要从左边开始找那么选择基数的时候就从右边的数作为基数*/ while(a[i]&lt;=temp&amp;&amp;i&lt;j) i++; if(i&lt;j) swap(a[i],a[j]); &#125; a[l]=a[i]; a[i]=temp; quicksort(l,i-1); quicksort(i+1,r);&#125;int main()&#123; quicksort(0,14); for(int i=0;i&lt;len;i++) printf(\"%d \",a[i]); printf(\"\\n\"); return 0;&#125; 堆排序原理利用堆这种数据结构而设计的一种排序算法，堆排序是一种选择排序，它的最坏，最好，平均时间复杂度均为O(nlogn)，它也是不稳定排序。 该数组从逻辑上讲就是一个堆结构，我们用简单的公式来描述一下堆的定义就是： 大顶堆：arr[i] &gt;= arr[2i+1] &amp;&amp; arr[i] &gt;= arr[2i+2] 小顶堆：arr[i] &lt;= arr[2i+1] &amp;&amp; arr[i] &lt;= arr[2i+2] 算法步骤： 将无需序列构建成一个堆，根据升序降序需求选择大顶堆或小顶堆; 将堆顶元素与末尾元素交换，将最大元素”沉”到数组末端; 重新调整结构，使其满足堆定义，然后继续交换堆顶元素与当前末尾元素，反复执行调整+交换步骤，直到整个序列有序。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package sortdemo;import java.util.Arrays;public class HeapSort &#123; public static void main(String []args)&#123; int []arr = &#123;9,8,7,6,5,4,3,2,1&#125;; sort(arr); System.out.println(Arrays.toString(arr)); &#125; public static void sort(int []arr)&#123; //1.构建大顶堆 for(int i=arr.length/2-1;i&gt;=0;i--)&#123; //从第一个非叶子结点从下至上，从右至左调整结构 adjustHeap(arr,i,arr.length); &#125; //2.调整堆结构+交换堆顶元素与末尾元素 for(int j=arr.length-1;j&gt;0;j--)&#123; swap(arr,0,j);//将堆顶元素与末尾元素进行交换 adjustHeap(arr,0,j);//重新对堆进行调整 &#125; &#125; /** * 调整大顶堆（仅是调整过程，建立在大顶堆已构建的基础上） * @param arr * @param i * @param length */ public static void adjustHeap(int []arr,int i,int length)&#123; int temp = arr[i];//先取出当前元素i for(int k=i*2+1;k&lt;length;k=k*2+1)&#123;//从i结点的左子结点开始，也就是2i+1处开始 if(k+1&lt;length &amp;&amp; arr[k]&lt;arr[k+1])&#123;//如果左子结点小于右子结点，k指向右子结点 k++; &#125; if(arr[k] &gt;temp)&#123;//如果子节点大于父节点，将子节点值赋给父节点（不用进行交换） arr[i] = arr[k]; i = k; &#125;else&#123; break; &#125; &#125; arr[i] = temp;//将temp值放到最终的位置 &#125; /** * 交换元素 */ public static void swap(int []arr,int a ,int b)&#123; int temp=arr[a]; arr[a] = arr[b]; arr[b] = temp; &#125;&#125; 几种排序算法的比较示意图 排序稳定性的定义通俗地讲就是能保证排序前两个相等的数其在序列的前后位置顺序和排序后它们两个的前后位置顺序相同。在简单形式化一下，如果Ai = Aj，Ai原来在位置前，排序后Ai还是要在Aj位置前 二叉平衡树 左右子树的的高度差不超过1, 否则要进行树的调整 二叉查找树 1， 左子树上所有的节点的值均小于或等于他的根节点的值 2， 右子数上所有的节点的值均大于或等于他的根节点的值 3， 左右子树也一定分别为二叉排序树 传统的查找树的缺点: 如果树太高就会导致查找效率低下, 接近线性查找的时间复杂度 红黑树 红黑树就是一种平衡的二叉查找树，说他平衡的意思是他不会变成“瘸子”，左腿特别长或者右腿特别长。除了符合二叉查找树的特性之外，还具体下列的特性： \\1. 节点是红色或者黑色 \\2. 根节点是黑色 \\3. 每个叶子的节点都是黑色的空节点（NULL） \\4. 每个红色节点的两个子节点都是黑色的。 \\5. 从任意节点到其每个叶子的所有路径都包含相同的黑色节点。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"数据结构/算法","permalink":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"排序算法","slug":"排序算法","permalink":"/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"name":"数据结构","slug":"数据结构","permalink":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"深入理解读写锁ReentrantReadWriteLock","slug":"深入理解读写锁ReentrantReadWriteLock","date":"2019-10-20T19:19:36.000Z","updated":"2019-10-20T11:40:37.774Z","comments":true,"path":"2019/10/21/深入理解读写锁ReentrantReadWriteLock/","link":"","permalink":"/2019/10/21/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AF%BB%E5%86%99%E9%94%81ReentrantReadWriteLock/","excerpt":"","text":"深入理解读写锁ReentrantReadWriteLock 1.读写锁的介绍 在并发场景中用于解决线程安全的问题，我们几乎会高频率的使用到独占式锁，通常使用java提供的关键字synchronized（关于synchronized可以看这篇文章）或者concurrents包中实现了Lock接口的ReentrantLock。它们都是独占式获取锁，也就是在同一时刻只有一个线程能够获取锁。而在一些业务场景中，==大部分只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方==。针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写锁允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。在分析WirteLock和ReadLock的互斥性时可以按照WriteLock与WriteLock之间，WriteLock与ReadLock之间以及ReadLock与ReadLock之间进行分析。更多关于读写锁特性介绍大家可以看源码上的介绍（阅读源码时最好的一种学习方式，我也正在学习中，与大家共勉），这里做一个归纳总结： 公平性选择：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平； 重入性：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁； 锁降级：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁 要想能够彻底的理解读写锁必须能够理解这样几个问题： 读写锁是怎样实现分别记录读写状态的？ 写锁是怎样获取和释放的？ 读锁是怎样获取和释放的？我们带着这样的三个问题，再去了解下读写锁。 2.写锁详解2.1.写锁的获取同步组件的实现聚合了同步器（AQS），并通过重写重写同步器（AQS）中的方法实现同步组件的同步语义（关于同步组件的实现层级结构可以看这篇文章，AQS的底层实现分析可以看这篇文章）。因此，写锁的实现依然也是采用这种方式。在同一时刻写锁是不能被多个线程所获取，很显然写锁是独占式锁，而实现写锁的同步语义是通过重写AQS中的tryAcquire方法实现的。源码为: 1234567891011121314151617181920212223242526protected final boolean tryAcquire(int acquires) &#123; Thread current = Thread.currentThread(); // 1. 获取写锁当前的同步状态 int c = getState(); // 2. 获取写锁获取的次数 int w = exclusiveCount(c); if (c != 0) &#123; // (Note: if c != 0 and w == 0 then shared count != 0) // 3.1 当读锁已被读线程获取或者当前线程不是已经获取写锁的线程的话 // 当前线程获取写锁失败 if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); // Reentrant acquire // 3.2 当前线程获取写锁，支持可重复加锁 setState(c + acquires); return true; &#125; // 3.3 写锁未被任何线程获取，当前线程可获取写锁 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;&#125; 这段代码的逻辑请看注释，这里有一个地方需要重点关注，exclusiveCount(c)方法，该方法源码为： 1static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125; 其中EXCLUSIVE_MASK为: static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1; EXCLUSIVE _MASK为1左移16位然后减1，即为0x0000FFFF。而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数。同时还有一个方法值得我们注意： 1static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125; 该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。现在还记得我们开篇说的需要弄懂的第一个问题吗？读写锁是怎样实现分别记录读锁和写锁的状态的，现在这个问题的答案就已经被我们弄清楚了，其示意图如下图所示： 现在我们回过头来看写锁获取方法tryAcquire，其主要逻辑为：当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。 2.2.写锁的释放写锁释放通过重写AQS的tryRelease方法，源码为： 12345678910111213protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //1. 同步状态减去写状态 int nextc = getState() - releases; //2. 当前写状态是否为0，为0则释放写锁 boolean free = exclusiveCount(nextc) == 0; if (free) setExclusiveOwnerThread(null); //3. 不为0则更新同步状态 setState(nextc); return free;&#125; 源码的实现逻辑请看注释，不难理解与ReentrantLock基本一致，这里需要注意的是，减少写状态int nextc = getState() - releases;只需要用当前同步状态直接减去写状态的原因正是我们刚才所说的写状态是由同步状态的低16位表示的。 3.读锁详解3.1.读锁的获取看完了写锁，现在来看看读锁，读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。按照之前对AQS介绍，实现共享式同步组件的同步语义需要通过重写AQS的tryAcquireShared方法和tryReleaseShared方法。读锁的获取实现方法为： 123456789101112131415161718192021222324252627282930313233protected final int tryAcquireShared(int unused) &#123; Thread current = Thread.currentThread(); int c = getState(); //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前 // 线程获取读锁失败返回-1 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; int r = sharedCount(c); if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; //2. 当前线程获取读锁 compareAndSetState(c, c + SHARED_UNIT)) &#123; //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法 //返回当前获取读锁的次数 if (r == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; &#125; return 1; &#125; //4. 处理在第二步中CAS操作失败的自旋已经实现重入性 return fullTryAcquireShared(current);&#125; 代码的逻辑请看注释，需要注意的是 当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。另外，当前同步状态需要加上SHARED_UNIT（(1 &lt;&lt; SHARED_SHIFT)即0x00010000）的原因这是我们在上面所说的同步状态的高16位用来表示读锁被获取的次数。如果CAS失败或者已经获取读锁的线程再次获取读锁时，是靠fullTryAcquireShared方法实现的，这段代码就不展开说了，有兴趣可以看看。 3.2.读锁的释放读锁释放的实现主要通过方法tryReleaseShared，源码如下，主要逻辑请看注释： 1234567891011121314151617181920212223242526272829303132protected final boolean tryReleaseShared(int unused) &#123; Thread current = Thread.currentThread(); // 前面还是为了实现getReadHoldCount等新功能 if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count &lt;= 1) &#123; readHolds.remove(); if (count &lt;= 0) throw unmatchedUnlockException(); &#125; --rh.count; &#125; for (;;) &#123; int c = getState(); // 读锁释放 将同步状态减去读状态即可 int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) // Releasing the read lock has no effect on readers, // but it may allow waiting writers to proceed if // both read and write locks are now free. return nextc == 0; &#125;&#125; 4.锁降级读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中： 123456789101112131415161718192021222324252627void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; // Must release read lock before acquiring write lock rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; // Recheck state because another thread might have // acquired write lock and changed state before we did. if (!cacheValid) &#123; data = ... cacheValid = true; &#125; // Downgrade by acquiring read lock before releasing write lock rwl.readLock().lock(); &#125; finally &#123; rwl.writeLock().unlock(); // Unlock write, still hold read &#125; &#125; try &#123; use(data); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"/categories/Java/"},{"name":"ReentrantLock","slug":"Java/ReentrantLock","permalink":"/categories/Java/ReentrantLock/"}],"tags":[{"name":"Java","slug":"Java","permalink":"/tags/Java/"},{"name":"Lock","slug":"Lock","permalink":"/tags/Lock/"}]},{"title":"Java面试技术栈","slug":"Java面试技术栈","date":"2019-10-20T19:07:51.000Z","updated":"2019-10-20T11:18:07.345Z","comments":true,"path":"2019/10/21/Java面试技术栈/","link":"","permalink":"/2019/10/21/Java%E9%9D%A2%E8%AF%95%E6%8A%80%E6%9C%AF%E6%A0%88/","excerpt":"","text":"剑指offerjava基础知识： 1.1常用api； 1.2 java数据结构：ArrayList扩容，HashMap结构，Hashset/Hashtable原理，Collection.sort用法，BlockingQueue用法,单链表结构,红黑树； 1.3算法：单链表反转，排序算法。 java高级知识：1.1 JVM相关：JVM如何进行gc，JVM调优，垃圾回收算法，JVM内存模型，类加载机制，内存泄漏，内存溢出； 1.2jdk提供的命令； 1.3设计模式。 多线程相关： 多线程实现方式，对synchronized的理解，ConcurrentHashmap的结构和实现原理，ThreadPoolExcuter原理，volitale关键字； Spring框架相关：IOC和AOP，动态代理，Spring拦截机制。 数据库相关： 5.1索引原理，mysql的最左匹配原则； 5.2如何做读写分离，分库方法（ThreadLocal用法）； 5.3事务相关：事务的4个隔离级别，mysql默认的隔离级别，数据库锁（悲观锁，乐观锁），死锁的条件，事务注解的用法，OCID理解（原子性，有序性，可见性，幂等性）。 5.4 mysql调优。 开源框架： 6.1数据存储：memcache，redis利弊，redis锁，redis的aof和rdb落盘方式，redis集群部署，一致性哈希算法，Mongo数据库； 6.2RokectMq（MQ消息丢失，MQ怎样监听，RoketMq高可用部署 ）； 6.3 Dubbo服务（dubbo和HTTP的优劣），dubbo服务的调用过程； 6.4Elasticjob（架构/流程图，执行过程，选举算法 ），其他作业调度框架（结合项目中用的调度方式）， 6.5mybatis用法； 6.6关心的新技术有哪些。 常用linux高端命令：top，netstat等。 WEB服务器：tomcat用法，如何排除tomca无响应故障，apache，nginx，docker了解一下，nginx和lvs的区别。 网络编程方面： 常见的网络超时（CLOSE_WAIT,TIME_WAIT），如何避免重试的问题，http的cookie机制，CDN原理。 其他问题： 在项目中遇到的最大的问题是什么？怎样解决的？ 从项目中学到了什么？","categories":[{"name":"面试","slug":"面试","permalink":"/categories/%E9%9D%A2%E8%AF%95/"},{"name":"Java","slug":"面试/Java","permalink":"/categories/%E9%9D%A2%E8%AF%95/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"/tags/Java/"},{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"面试","slug":"面试","permalink":"/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"分布式","slug":"分布式","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T13:37:31.542Z","comments":true,"path":"2019/10/20/分布式/","link":"","permalink":"/2019/10/20/%E5%88%86%E5%B8%83%E5%BC%8F/","excerpt":"","text":"分布式单系统 所有的代码都在一个工程里，最多可能就是通过maven等构件工具拆分了一下代码工程模块，不同的模块可以放在不同的工程代码里。 在部署的时候，可能就是直接在线上的几台机器里直接放到里面的tomcat下来运行。很多流量很小的企业内部系统，比如OA、CRM、财务等系统，甚至可能就直接在一台机器的tomcat下部署一下。然后直接配置一下域名解析，就可以让这个系统的可能几十个，或者几百个用户通过访问域名来使用这个软件了。 只不过为了所谓的“高可用”，可能一般会部署两台机器，前面加一层负载均衡设备，这样其中一个机器挂了，另外一个机器上还有一个系统可以用。 但是团队越来越大，业务越来越复杂, 小团队你搞一个代码仓库，然后就一份代码，每个人都在自己本地写代码，最后把代码合并一下，做做测试，然后就直接部署基于Tomcat来就可以了。几十个人维护一个单块系统，大家在一个工程里写代码，大量的冲突以及代码合并都会让人崩溃。比如某个功能模块要上线了，但是他必须得把整个单块系统所有的功能都回归测试一遍才敢上线. 因为`大家的代码都在一个工程里，都是耦合在一起的，你修改了代码，必须全部测试一遍才能保证系统正常。 分布式系统因此分布式出现：庞大系统分而治之 这个时候就可以尝试把一个大的系统拆分为很多小的系统，甚至很多小的服务，然后几个人组成一个小组就专门维护其中一个小系统，或者每个人维护一个小服务。简单来说，就是分而治之，这样每个人可以专注维护自己的代码。 不同的小系统自己开发、测试和上线，都不会跟别人耦合在一起，可以自己独立进行，非常的方便，大大简化了大规模系统的开发成本。 不同的子系统之间，就是通过接口互相来回调用，每个子系统都有自己的数据库，大家看下面的图。 分布式系统所带来的技术问题 你的系统一旦分布式了之后，通信、缓存、消息、事务、锁、配置、日志、监控、会话，等等各种原来单块系统场景下很容易解决的问题，都会变得很复杂，需要引入大量外部的技术。 (1)分布式服务框架你如果要让不同的子系统或者服务之间互相通信，首先必须有一套分布式服务框架。也就是各个服务可以互相感知到对方在哪里，可以发送请求过去，可以通过HTTP或者RPC的方式。在这里，最常见的技术就是dubbo以及spring cloud，当然大厂一般都是自己有服务框架 (2)分布式事务一旦你的系统拆分为了多个子系统之后，那么一个贯穿全局的分布式事务应该怎么来实现? 这个你需要了解TCC、最终一致性、2PC等分布式事务的实现方案和开源技术。 (3)分布式锁不同的系统之间如果需要在全局加锁获取某个资源的锁定，此时应该怎么来做? 毕竟大家不是在一个JVM里了，不可能用synchronized来在多个子系统之间实现锁 (4)分布式缓存如果你原来就是个单块系统，那么你其实是可以在单个JVM里进行本地缓存就可以了，比如搞一个HashMap来缓存一些数据。但是现在你有很多个子系统，他们如果要共享一个缓存，你应该怎么办?是不是需要引入Redis等缓存系统? (5)分布式消息系统在单块系统内，就一个JVM进程内部，你可以用类似LinkedList之类的数据结构作为一个本地内存里的队列。 但是多个子系统之间要进行消息队列的传递呢?那是不是要引入类似RabbitMQ之类的分布式消息中间件? (6)分布式搜索系统如果在单块系统内，你可以比如在本地就基于Lucene来开发一个全文检索模块，但是如果是分布式系统下的很多子系统，你还能直接基于Lucene吗? 明显不行，你需要在系统里引入一个外部的分布式搜索系统，比如Elasticsearch。 (7)其他很多的技术比如说分布式配置中心、分布式日志中心、分布式监控告警中心、分布式会话，等等，都是分布式系统场景下你需要使用和了解的一些技术。因为沿用单块系统时代的那些技术已经不行了，比如说你单块系统的时候，直接在本地用一个properties文件存放自己的配置即可，日志也写到本地即可。","categories":[{"name":"分布式","slug":"分布式","permalink":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"分布式","slug":"分布式","permalink":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"ZooKeeper面试","slug":"ZooKeeper面试","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T06:06:22.160Z","comments":true,"path":"2019/10/20/ZooKeeper面试/","link":"","permalink":"/2019/10/20/ZooKeeper%E9%9D%A2%E8%AF%95/","excerpt":"","text":"ZooKeeper面试1. 什么是dubbo?Dubbo是阿里巴巴SOA服务化治理方案的核心框架，是一个分布式服务框架，致力于提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理方案。 Dubbo提供三个关键功能 包括基于接口的远程调用 容错和负载平衡 自动服务注册和发现。 2. 测试和生产公用一套zookeeper，怎么保证消费不冲突? dubbo白名单(Filter过滤器) 服务分组 1234567&lt;!--服务--&gt;&lt;dubbo:service group=\"feedback\" interface=\"com.xxx.IndexService\" /&gt;&lt;dubbo:service group=\"member\" interface=\"com.xxx.IndexService\" /&gt;&lt;!--引用--&gt;&lt;dubbo:reference id=\"feedbackIndexService\" group=\"feedback\" interface=\"com.xxx.IndexService\" /&gt;&lt;dubbo:reference id=\"memberIndexService\" group=\"member\" interface=\"com.xxx.IndexService\" /&gt; 多版本 1&lt;dubbo:service interface=\"com.foo.BarService\" version=\"1.0.0\" /&gt; 第一种方案：实现com.alibaba.dubbo.rpc.Filter接口： public class AuthorityFilter implements Filter { private static final Logger LOGGER = LoggerFactory.getLogger(AuthorityFilter.class); private IpWhiteList ipWhiteList; //dubbo通过setter方式自动注入 public void setIpWhiteList(IpWhiteList ipWhiteList) { this.ipWhiteList = ipWhiteList; } @Override public Result invoke(Invoker&lt;?&gt; invoker, Invocation invocation) throws RpcException { if (!ipWhiteList.isEnabled()) { LOGGER.debug(&quot;白名单禁用&quot;); return invoker.invoke(invocation); } String clientIp = RpcContext.getContext().getRemoteHost(); LOGGER.debug(&quot;访问ip为{}&quot;, clientIp); List&lt;String&gt; allowedIps = ipWhiteList.getAllowedIps(); if (allowedIps.contains(clientIp)) { return invoker.invoke(invocation); } else { return new RpcResult(); } } 注意：只能通过setter方式来注入其他的bean，且不要标注注解！dubbo自己会对这些bean进行注入，不需要再标注@Resource让Spring注入在resources目录下添加纯文本文件META-INF/dubbo/com.alibaba.dubbo.rpc.Filter，内容如下： xxxFilter=com.xxx.AuthorityFilter修改dubbo的provider配置文件，在dubbo:provider中添加配置的filter， 内容如下： &lt;dubbo:provider filter=&quot;xxxFilter&quot; /&gt; 这样就可以实现dubbo接口的IP白名单功能了。 出自https://www.cnblogs.com/wangzhongqiu/archive/2017/09/22/7575759.html 3. ZooKeeper是什么？ Zookeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的zookeeper机器来处理。对于写请求，这些请求会同时发给其他zookeeper机器并且达成一致后，请求才会返回成功。因此，随着zookeeper的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。有序性是zookeeper中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为zxid（==Zookeeper Transaction Id==）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个zookeeper最新的zxid。 应用场景Zookeeper的功能很强大，应用场景很多，结合我实际工作中使用Dubbo框架的情况，Zookeeper主要是做注册中心用。基于Dubbo框架开发的提供者、消费者都向Zookeeper注册自己的URL，消费者还能拿到并订阅提供者的注册URL，以便在后续程序的执行中去调用提供者。而提供者发生了变动，也会通过Zookeeper向订阅的消费者发送通知。 5. zookeeper是如何保证事务的顺序一致性的zookeeper采用了递增的事务Id(zxid)来标识，所有的proposal都在被提出的时候加上了zxid，zxid实际上是一个64位的数字，高32位是epoch用来标识leader是否发生改变，如果有新的leader产生出来，epoch会自增，低32位用来递增计数。当新产生proposal的时候，会依据数据库的两阶段过程，首先会向其他的server发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行 5. ZooKeeper提供了什么？ 5.1 文件系统Zookeeper提供一个多层级的节点命名空间（像树一样, 节点称为znode）。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得Zookeeper不能用于存放大量的数据，每个节点的存放数据上限为1M。 四种类型的znode1、PERSISTENT-持久化目录节点创建之后一直存在，除非有删除操作，创建节点的客户端会话失效也不影响此节点。2、PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点跟持久一样，就是父节点在创建下一级子节点的时候，记录每个子节点创建的先后顺序，会给每个子节点名加上一个数字后缀。3、EPHEMERAL-临时目录节点 ephemeral短暂的,临时的创建客户端会话失效（注意是会话失效，不是连接断了），节点也就没了。不能建子节点。4、EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 创建的临时节点什么时候会被删除，是连接一断就删除吗？延时是多少？连接断了之后，ZK不会马上移除临时数据，只有当SESSIONEXPIRED之后，才会把这个会话建立的临时数据移除。因此，用户需要谨慎设置Session_TimeOut 是否可以拒绝单个IP对ZK的访问操作 ?ZK本身不提供这样的功能，它仅仅提供了对单个IP的连接数的限制。你可以通过修改iptables来实现对单个ip的限制；当然，你也可以通过这样的方式来解决。 5.2 通知机制client端会对某个znode建立一个watcher事件，当该znode发生变化时，这些client会收到zookeeper的通知，然后client可以根据znode变化来做出业务上的改变等。 6. Zookeeper做了什么？ 命名服务（文件系统）命名服务是指通过指定的名字来获取资源或者服务的地址，利用zk创建一个全局的路径，即是唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。 zk的配置管理（文件系统、通知机制）程序分布式的部署在不同的机器上，将程序的配置信息放在zk的znode下，当有配置发生改变时，也就是znode发生变化时，可以通过改变zk中某个目录节点的内容，利用watcher通知给各个客户端，从而更改配置。 Zookeeper集群管理（文件系统、通知机制）所谓集群管理无在乎两点： 1. 是否有机器退出和加入 2. 选举leader。 对于第一点，所有机器约定在父目录下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount又有了. 对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。 Zookeeper分布式锁（文件系统、通知机制）有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过create znode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的distribute_lock 节点就释放出锁。 (思想就是: 去创建一个lock节点, 谁创建成功谁就能拿到这把锁, 用完后,删除掉自己创建的这个节点) 对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。 获取分布式锁的流程:在获取分布式锁的时候在locker节点下创建临时顺序节点，释放锁的时候删除该临时节点。客户端调用createNode方法在locker下创建临时顺序节点，然后调用getChildren(“locker”)来获取locker下面的所有子节点，注意此时不用设置任何Watcher。客户端获取到所有的子节点path之后，如果发现自己创建的节点在所有创建的子节点序号最小，那么就认为该客户端获取到了锁。如果发现自己创建的节点并非locker所有子节点中最小的，说明自己还没有获取到锁，==此时客户端需要找到比自己小的那个节点，然后对其调用exist()方法，同时对其注册事件监听器==。之后，让这个被关注的节点删除，则客户端的Watcher会收到相应通知，此时再次判断自己创建的节点是否是locker子节点中序号最小的，如果是则获取到了锁，如果不是则重复以上步骤继续获取到比自己小的一个节点并注册监听。当前这个过程中还需要许多的逻辑判断。 代码的实现主要是基于互斥锁，获取分布式锁的重点逻辑在于BaseDistributedLock，实现了基于Zookeeper实现分布式锁的细节。 Zookeeper队列管理（文件系统、通知机制）两种类型的队列： 同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 队列按照 FIFO 方式进行入队和出队操作。 和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下创建PERSISTENT_SEQUENTIAL节点，创建成功时Watcher通知等待的队列，队列删除序列号最小的节点用以消费。此场景下Zookeeper的znode用于消息存储，znode存储的数据就是消息队列中的消息内容，SEQUENTIAL序列号就是消息的编号，按序取出即可。由于创建的节点是持久化的，所以不必担心队列消息的丢失问题。 Zookeeper数据复制Zookeeper作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处： 容错：一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作； 提高系统的扩展能力 ：把负载分布到多个节点上，或者增加节点来提高系统的负载能力； 提高性能：让客户端本地访问就近的节点，提高用户访问速度。 从客户端读写访问的透明度来看，数据复制集群系统分下面两种： 写主(WriteMaster) ：对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离； 写任意(Write Any)：对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。 对zookeeper来说，它采用的方式是写任意。通过增加机器，它的==读吞吐能力和响应能力扩展性非常好==，而==写，随着机器的增多吞吐能力肯定下降==（这也是它建立observer的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。 7. Zookeeper工作原理Zookeeper 的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议(Zookeeper原子广播)。 Zab协议有两种模式: 它们分别是恢复模式（选主）和广播模式（同步） 当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 8. Zookeeper 下 Server工作状态每个Server在工作过程中有三种状态：LOOKING：当前Server不知道leader是谁，正在搜寻LEADING：当前Server即为选举出来的leaderFOLLOWING: leader已经选举出来，当前Server与之同步 9. zookeeper是如何选取主leader的？当leader崩溃或者leader失去大多数的follower，这时zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。 Zookeeper选主流程(basic paxos)(每个节点都发送一zxid, 选举线程推荐zxid最大的那个server作为leader)（1）选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server；（2）选举线程首先向所有Server发起一次询问(包括自己)；（3）选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中；（4）选举线程收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server；（5）线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n/2 + 1(大于一般的投票数)的Server票数，设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。 通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1. 每个Server启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的server还会从磁盘快照中恢复数据和会话信息，zk会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。 Zookeeper选主流程(fast paxos)(有一个节点主动提出要成为leader,去问大家是否同意)fast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和 zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。 10. Zookeeper同步流程选完Leader以后，zk就进入状态同步过程。 Leader等待server连接； Follower连接leader，将最大的zxid发送给leader； Leader根据follower的zxid确定同步点； 完成同步后通知follower 已经成为uptodate状态； Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。 11. 分布式通知和协调对于系统调度来说：操作人员发送通知实际是通过控制台改变某个节点的状态，然后zk将这些变化发送给注册了这个节点的watcher的所有客户端。对于执行情况汇报：每个工作进程都在某个目录下创建一个临时节点。并携带工作的进度数据，这样汇总的进程可以监控目录子节点的变化获得工作进度的实时的全局情况。 12. 机器中为什么会有leader？在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行leader选举。 13. zk节点宕机如何处理？Zookeeper本身也是集群，推荐配置不少于3个服务器。Zookeeper自身也要保证当一个节点宕机时，其他节点会继续提供服务。如果是一个Follower宕机，还有2台服务器提供访问，因为Zookeeper上的数据是有多个副本的，数据并不会丢失；如果是一个Leader宕机，Zookeeper会选举出新的Leader。ZK集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在ZK节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。所以3个节点的cluster可以挂掉1个节点(leader可以得到2票&gt;1.5)2个节点的cluster就不能挂掉任何1个节点了(leader可以得到1票&lt;=1) 14. zookeeper负载均衡和nginx负载均衡区别zk的负载均衡是可以调控，nginx只是能调权重，其他需要可控的都需要自己写插件；但是nginx的吞吐量比zk大很多，应该说按业务选择用哪种方式。 15. zookeeper watch机制Watch机制官方声明：一个Watch事件是一个一次性的触发器，当被设置了Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了Watch的客户端，以便通知它们。 为什么不是永久的，举个例子，如果服务端变动频繁，而监听的客户端很多情况下，每次变动都要通知到所有的客户端，这太消耗性能了。在实际应用中，很多情况下，我们的客户端不需要知道服务端的每一次变动，我只要最新的数据即可。 Zookeeper机制的特点： 一次性触发数据发生改变时，一个watcher event会被发送到client，但是client只会收到一次这样的信息。 watcher event异步发送: watcher的通知事件从server发送到client是异步的，这就存在一个问题，不同的客户端和服务器之间通过socket进行通信，由于网络延迟或其他因素导致客户端在不同的时刻监听到事件，由于Zookeeper本身提供了ordering guarantee，即客户端监听事件后，才会感知它所监视znode发生了变化。所以我们使用Zookeeper不能期望能够监控到节点每次的变化。Zookeeper只能保证最终的一致性，而无法保证强一致性。 数据监视Zookeeper有数据监视和子数据监视getdata() and exists()设置数据监视，getchildren()设置了子节点监视。 注册watcher getData、exists、getChildren 触发watcher create、delete、setData( 修改操作通过删除,创建 实现) setData()会触发znode上设置的data watch（如果set成功的话）。一个成功的create() 操作会触发被创建的znode上的数据watch，以及其父节点上的child watch。而一个成功的delete()操作将会同时触发一个znode的data watch和child watch（因为这样就没有子节点了），同时也会触发其父节点的child watch。 当一个客户端连接到一个新的服务器上时，watch将会被以任意会话事件触发。当与一个服务器失去连接的时候，是无法接收到watch的。而当client重新连接时，如果需要的话，所有先前注册过的watch，都会被重新注册。通常这是完全透明的。只有在一个特殊情况下，watch可能会丢失：对于一个未创建的znode的exist watch，如果在客户端断开连接期间被创建了，并且随后在客户端连接上之前又删除了，这种情况下，这个watch事件可能会被丢失。 Watch是轻量级的，其实就是本地JVM的Callback，服务器端只是存了是否有设置了Watcher的布尔类型 集群支持动态添加机器吗？其实就是水平扩容了，Zookeeper在这方面不太好。两种方式：全部重启：关闭所有Zookeeper服务，修改配置之后启动。不影响之前客户端的会话。逐个重启：顾名思义。这是比较常用的方式。","categories":[],"tags":[{"name":"IT - 技术笔记 - Job - zookeeper","slug":"IT-技术笔记-Job-zookeeper","permalink":"/tags/IT-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0-Job-zookeeper/"}]},{"title":"Redis面试","slug":"Redis面试","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T06:04:02.571Z","comments":true,"path":"2019/10/20/Redis面试/","link":"","permalink":"/2019/10/20/Redis%E9%9D%A2%E8%AF%95/","excerpt":"","text":"什么是Redis？简述它的优缺点？ Redis本质上是一个Key-Value类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。 因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。 Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 memcached只能保存1MB的数据，因此Redis可以用来实现很多有用的功能。 比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。 另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。 Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 1、 使用Redis有哪些好处？ 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) 支持丰富数据类型，支持==string，list，set，sorted set，hash== 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 2、 redis相比memcached有哪些优势 memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 redis的速度比memcached快很多 redis可以持久化其数据 – 附加的数据持久化， 溢写到磁盘当中 3、redis常见性能问题和解决方案： Master最好不要做任何持久化工作，如RDB内存快照和AOF日志文件 如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次 为了主从复制的速度和连接的稳定性，Master和Slave最好在同一个局域网内 尽量避免在压力很大的主库上增加从库 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3… 这样的结构方便解决单点故障问题，实现Slave对Master的替换。如果Master挂了，可以立刻启用Slave1做Master，其他不变。 4、redis的淘汰策略, 保证数据的热点性==redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。== enviction 驱逐，流浪 voltile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 5、Memcache与Redis的区别都有哪些 存储方式 Memecache把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。 Redis有部份存在硬盘上，这样能保证数据的持久性。 数据支持类型 Memcache对数据类型支持相对简单。 Redis有复杂的数据类型 使用底层模型不同 它们之间底层实现方式 以及与客户端之间通信的应用协议不一样。 Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 value大小redis最大可以达到1GB，而memcache只有1MB 6、个人总结了以下多种Web应用场景，在这些场景下可以充分的利用Redis的特性，大大提高效率。 在主页中显示最新的项目列表：Redis使用的是常驻内存的缓存，速度非常快。LPUSH用来插入一个内容ID，作为关键字存储在列表头部。LTRIM用来限制列表中的项目数最多为5000。如果用户需要的检索的数据量超越这个缓存容量，这时才需要把请求发送到数据库。 删除和过滤：如果一篇文章被删除，可以使用LREM从缓存中彻底清除掉。 排行榜及相关问题：排行榜（leader board）按照得分进行排序。ZADD命令可以直接实现这个功能，而ZREVRANGE命令可以用来按照得分来获取前100名的用户，ZRANK可以用来获取用户排名，非常直接而且操作容易。 按照用户投票和时间排序：排行榜，得分会随着时间变化。LPUSH和LTRIM命令结合运用，把文章添加到一个列表中。一项后台任务用来获取列表，并重新计算列表的排序，ZADD命令用来按照新的顺序填充生成列表。列表可以实现非常快速的检索，即使是负载很重的站点。 过期项目处理：使用Unix时间作为关键字，用来保持列表能够按时间排序。对current_time和time_to_live进行检索，完成查找过期项目的艰巨任务。另一项后台任务使用ZRANGE…WITHSCORES进行查询，删除过期的条目。 计数：进行各种数据统计的用途是非常广泛的，比如想知道什么时候封锁一个IP地址。INCRBY命令让这些变得很容易，通过原子递增保持计数；GETSET用来重置计数器；过期属性用来确认一个关键字什么时候应该删除。 特定时间内的特定项目：这是特定访问者的问题，可以通过给每次页面浏览使用SADD命令来解决。SADD不会将已经存在的成员添加到一个集合。 Pub/Sub：在更新中保持用户对数据的映射是系统中的一个普遍任务。Redis的pub/sub功能使用了SUBSCRIBE、UNSUBSCRIBE和PUBLISH命令，让这个变得更加容易。 队列：在当前的编程中队列随处可见。除了push和pop类型的命令之外，Redis还有阻塞队列的命令，能够让一个程序在执行时被另一个程序添加到队列 7、Redis的全称是什么？Remote Dictionary Server。 8、一个字符串类型的值能存储最大容量是多少？512M 9、为什么Redis需要把所有数据放到内存中？Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响redis的性能。在内存越来越便宜的今天，redis将会越来越受欢迎。 如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。 10、Redis集群方案应该怎么做？都有哪些方案？ 1.codis。 目前用的最多的集群方案，基本和twemproxy一致的效果，但它支持在 节点数量改变情况下，旧节点数据可恢复到新hash节点。 2.redis cluster3.0自带的集群，特点在于他的分布式算法不是一致性hash，而是hash槽的概念，以及自身支持节点设置从节点。具体看官方文档介绍。 3.在业务代码层实现，起几个毫无关联的redis实例，在代码层，对key 进行hash计算，然后去对应的redis实例操作数据。 这种方式对hash层代码要求比较高，考虑部分包括，节点失效后的替代算法方案，数据震荡后的自动脚本恢复，实例的监控，等等。 11、Redis集群方案什么情况下会导致整个集群不可用？有A，B，C三个节点的集群,在没有复制模型的情况下,如果节点B失败了，那么整个集群就会以为缺少5501-11000这个范围的槽而不可用。 12、Redis有哪些适合的场景？ （1）会话缓存（Session Cache） 最常用的一种使用Redis的情景是会话缓存（session cache）。用Redis缓存会话比其他存储（如Memcached）的优势在于：Redis提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用Redis来缓存会话的文档。甚至广为人知的商业平台Magento也提供Redis的插件。 （2）全页缓存（FPC） 除基本的会话token之外，Redis还提供很简便的FPC平台。回到一致性问题，即使重启了Redis实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似PHP本地FPC。 再次以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。 此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 （3）队列 Reids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得Redis能作为一个很好的消息队列平台来使用。Redis作为队列使用的操作，就类似于本地程序语言（如Python）对 list 的 push/pop 操作。 如果你快速的在Google中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用Redis创建非常好的后端工具，以满足各种队列需求。例如，Celery有一个后台就是使用Redis作为broker，你可以从这里去查看。 （4）排行榜/计数器 Redis在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis只是正好提供了这两种数据结构。 所以，我们要从排序集合中获取到排名最靠前的10个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可： 当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行： ZRANGE user_scores 0 10 WITHSCORES Agora Games就是一个很好的例子，用Ruby实现的，它的排行榜就是使用Redis来存储数据的，你可以在这里看到。 （5）发布/订阅 最后（但肯定不是最不重要的）是Redis的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用Redis的发布/订阅功能来建立聊天系统！ 13、Redis支持的Java客户端都有哪些？官方推荐用哪个？Redisson、Jedis、lettuce等等，官方推荐使用Redisson。 14、Jedis与Redisson对比有什么优缺点？Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持； Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。 15、Redis和Redisson有什么关系？==Redisson是一个高级的分布式协调Redis客服端==，能帮助用户在分布式环境中轻松实现一些Java的对象 (Bloom filter, BitSet, Set, SetMultimap, ScoredSortedSet, SortedSet, Map, ConcurrentMap, List, ListMultimap, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, ReadWriteLock, AtomicLong, CountDownLatch, Publish / Subscribe, HyperLogLog)。 17、Redis如何设置密码及验证密码？设置密码：config set requirepass 123456 授权密码：auth 123456 18、说说Redis哈希槽的概念？Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。 19、Redis集群的主从复制模型是怎样的？为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型,每个节点都会有N-1个复制品. 20、Redis集群会有写操作丢失吗？为什么？Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。 21、Redis集群之间是如何复制的？异步复制 22、Redis集群最大节点个数是多少？16384个。 23、Redis集群如何选择数据库？Redis集群目前无法做数据库选择，默认在0数据库。 24、怎么测试Redis的连通性？ping 25、Redis中的管道有什么用？一次请求/响应服务器能实现处理新的请求即使旧的请求还未被响应。这样就可以将多个命令发送到服务器，而不用等待回复，最后在一个步骤中读取该答复。 这就是管道（pipelining），是一种几十年来广泛使用的技术。例如许多POP3协议已经实现支持这个功能，大大加快了从服务器下载新邮件的过程。 26、怎么理解Redis事务？事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 27、Redis事务相关的命令有哪几个？****MULTI、EXEC、DISCARD、WATCH 28、Redis key的过期时间和永久有效分别怎么设置？EXPIRE和PERSIST命令。 29、Redis如何做内存优化？尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。 比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key,而是应该把这个用户的所有信息存储到一张散列表里面。 30、Redis回收进程如何工作的？一个客户端运行了新的命令，添加了新的数据。Redi检查内存使用情况，如果大于maxmemory的限制, 则根据设定好的策略进行回收。","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"数据库","slug":"数据库","permalink":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"},{"name":"Redis","slug":"Redis","permalink":"/tags/Redis/"}]},{"title":"算法笔记","slug":"算法笔记","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T11:37:22.747Z","comments":true,"path":"2019/10/20/算法笔记/","link":"","permalink":"/2019/10/20/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/","excerpt":"","text":"十大算法递归算法能够用递归解决的问题需要满足三个条件： 原问题可以转换为一个或多个子问题来求解，而这些子问题的求解方法和原问题完全相同，只是规模不同； 递归调用次数必须是有限的； 必须有结束递归的条件（递归出口）来终止递归。 设计递归算法模式 先求解问题的递归模型。 在设计递归算法的时候，如果纠结递归树的每一个阶段的话，就会极为复杂。因此，只考虑递归树中的第一层和第二层之间的关系即可，即“大问题” 和 “小问题” 的关系，其他关系类似。 求解问题的递归模型： 对原问题 f(n) 进行分析，假设出合理的小问题 f(n-1)； 假设小问题 f(n-1) 是可解的，在此基础上确定大问题 f(n) 的解，即给出 f(n) 与 f(n-1) 之间的关系，也就是确定了递归体。（与数学归纳法中确定 i=n-1 时成立，再求证 i=n 时等式成立的过程相似）。还要明确需要返回什么信息给上一层递归调用。 确定一个特定情况（如 f(0) 或 f(1)）的解，由此作为递归出口。（与数学归纳法中求证 i=0 或 i=1时等式成立相似） 例子:求数组中最小值：123456789101112131415161718/** * 递归求数组最大值 * * @param arr 目标数组 * @param i 起始下标 * @return */private static int getMax(int[] arr, int i) &#123; if (i == arr.length - 1) &#123; return arr[i];//只有一个元素的时候,它就是最大值（特定问题的解） &#125; else &#123; //假设解出了子问题i+1的最大值, //将i+1的最大值和当前i进行比较, 返回最大的 //将最后一个元素与“假设存在的”最大值比较 int max = Math.max(arr[i], getMax(arr, i + 1)); return max; &#125;&#125; 分治算法 分治法是一种很重要的算法。字面上的解释是“分而治之”，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题……直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并。 分治法适用的情况分治算法所能解决的问题一般具有以下几个特征: 1) 该问题的规模缩小到一定的程度就可以容易地解决 2) 该问题可以分解为若干个规模较小的相同问题，即该问题具有最优子结构性质。 3) 利用该问题分解出的子问题的解可以合并为该问题的解； 如果不具备这一条特征, 那么可以考虑使用贪心算法或者动态规划算法 4) 该问题所分解出的各个子问题是相互独立的，即子问题之间不包含公共的子子问题。 如果各子问题是不独立的则分治法要做许多不必要的工作，重复地解公共的子问题，此时虽然可用分治法，但一般用动态规划法较好。 分治法的基本步骤分治法在每一层递归上都有三个步骤： （1）分解，将要解决的问题划分成若干规模较小的同类问题； （2）求解，当子问题划分得足够小时，用较简单的方法解决； （3）合并，按原问题的要求，将子问题的解逐层合并构成原问题的解。 分治算法的设计模式:1234567891011Divide_and_Conquer(P)&#123; if(xxx) //递归出口：如果规模足够小，克制直接求解，则开始“治” return ADHOC(P); //ADHOC是治理可直接求解子问题的子过程 &lt;divide P into smaller subinstances P1,P2,...Pk&gt;; //将P“分”解为k个子问题 for(int i = 0; i &lt; k; ++i) yi = Divide_and_Conquer(Pi); //递归求解各个子问题 return merge(y1, y2, ..., yk); //将各个子问题的解“合”并为原问题的解&#125; 案例: 求数组的和123456private static int getSum(int[] arr, int low, int high) &#123; if (low &gt;= high) return arr[high];// “治” 直接求解 int mid = (low + high) / 2;// “分” 分解成两个子问题 // “合” 自底向上求子数组l和r的和 return getSum(arr, low, mid) + getSum(arr, mid + 1, high); &#125; 分治 - 求逆序对数目如，{3, 1, 2, 4} 中逆序对有 &lt;3,1&gt; &lt;3,2&gt; 12345678910111213141516171819202122232425262728293031323334353637383940static int getReverseCount(int[] arr, int low, int high) &#123; //治 if (low &gt;= high) return 0; //分 成子问题,子问题解有三个 //1. 左边部分的解 //2. 右边部分的解 //3. 左右两部分合起来的解 int mid = (low + high) / 2; int lC = getReverseCount(arr, low, mid); int rC = getReverseCount(arr, mid + 1, high); int mergeC = merge(arr, low, mid, high); //合并子问题的解(3个) return lC + rC + mergeC; &#125; private static int merge(int[] arr, int low, int mid, int high) &#123; //将左右两部分的限定区间内的值拿出来(这样不会影响整体)求左右两部分合起来的解-&gt; 把中间部分拿出来排好序在放回去 int[] tmp = Arrays.copyOfRange(arr, low, high + 1); int nMid = mid - low; int nHigh = tmp.length - 1; int num = 0;//逆序数 //开始求两部分合起来的解(仅仅!! 把中间那部分拿出来进行排序后,放回到原数组arr中) int index = low; int p1 = 0; int p2 = nMid + 1; while (p1 &lt;= nMid &amp;&amp; p2 &lt;= nHigh) &#123; // 比较左右两部分的元素，哪个小，把那个元素填入原数组index位置中 if (tmp[p1] &gt; tmp[p2]) &#123; num++; System.out.println(tmp[p1] + \" \" + tmp[p2]); arr[index++] = tmp[p2++]; &#125; else &#123; arr[index++] = tmp[p1++]; &#125; &#125; //把剩余的元素填入arr相应位置 while (p1 &lt;= nMid) arr[index++] = tmp[p1++]; while (p2 &lt;= nHigh) arr[index++] = tmp[p2++]; return num; &#125; 大整数乘法 动态规划 动态规划过程是：每次决策依赖于当前状态，又随即引起状态的转移。是一种分阶段求解决策问题的数学思想。总结起来就是一句话，大事化小，小事化了。 基本思想与分治法类似，也是将待求解的问题分解为若干个子问题（阶段），按顺序求解子阶段，前一子问题的解，为后一子问题的求解提供了有用的信息。在求解任一子问题时，列出各种可能的局部解，通过决策保留那些有可能达到最优的局部解，丢弃其他局部解。依次解决各子问题，最后一个子问题就是初始问题的解。 通用递推式:$$ f(n,m)=max{f(n-1,m), f(n-1,m-w[n])+P(n,m)}$$ 优化: 由于动态规划解决的问题多数有重叠子问题这个特点，为减少重复计算，对每一个子问题只解一次，将其不同阶段的不同状态保存在一个二维数组中。 分治法最大的差别:适合于用动态规划法求解的问题，经分解后得到的子问题往往不是互相独立的（即下一个子阶段的求解是建立在上一个子阶段的解的基础上，进行进一步的求解）。 适用的情况能采用动态规划求解的问题的一般要具有3个性质： 最优化原理：如果问题的最优解所包含的子问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。 无后效性：即某阶段状态一旦确定，就不受这个状态以后决策的影响。也就是说，某状态以后的过程不会影响以前的状态，只与当前状态有关。 有重叠子问题：即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。（该性质并不是动态规划适用的必要条件，但是如果没有这条性质，动态规划算法同其他算法相比就不具备优势） 求解的基本步骤​ 初始状态→│决策１│→│决策２│→…→│决策ｎ│→结束状态 划分阶段：按照问题的时间或空间特征，把问题分为若干个阶段。在划分阶段时，注意划分后的阶段一定要是有序的或者是可排序的，否则问题就无法求解。 确定状态和状态变量-&gt;(递归的定义最优解。)：将问题发展到各个阶段时所处于的各种客观情况用不同的状态表示出来。当然，状态的选择要满足无后效性。 确定决策并写出状态转移方程：因为决策和状态转移有着天然的联系，状态转移就是根据上一阶段的状态和决策来导出本阶段的状态。所以如果确定了决策，状态转移方程也就可写出。但事实上常常是反过来做，根据相邻两个阶段的状态之间的关系来确定决策方法和状态转移方程。 寻找边界条件：给出的状态转移方程是一个递推式，需要一个递推的终止条件或边界条件。 动态规划案例0, 1背包问题 两种情况来分析, 要拿的东西能不能装进背包,能装进去的话,要不要那它(看价值) 如果第k件物品的重量w[k]比此时的背包的剩余重量c大了，那我肯定是拿不动了，即w[k]&gt;c。所以此时包中物品的价值就是我拿的前一个物品之后包中的价值，即 b(k,c)=b(k-1,c).包中剩余空间不变，还是c。 那么第二种情况，如果我拿得动第k件物品，即第k件物品的重量w[k]&lt;c，面对k号物品，无外乎两种选择，拿或者不拿，这时我就要根据拿走之后产生的效益进行决策了： 不拿k号物品，那么此时包中物品的总价值b(k,c)=b(k-1,c)，和第一种拿不动k号物品的一样。 拿走k号物品，那么此时包中物品的总价值b(k,c)=b(k-1,c-w[k])+v[k]拿了第k件物品后，那我的包中的价值肯定就是原先的价值再加上第k件物品的价值，而且拿了之后包中的剩余容量就为c-w[k]了。 总结一下，就是如下的公式了：b(k,c)=max{b(k-1,c),b(k-1,c-w[k])+v[k]} 1234567891011121314151617181920class Main &#123; public static void main(String[] args) &#123; int[] w = &#123; 0, 2, 3, 4, 5, 9 &#125;; int[] v = &#123; 0, 3, 4, 5, 8, 10 &#125;; int N = 6, W = 21; int[][] b = new int[N][W]; for (int k = 1; k &lt; N; k++) &#123; for (int c = 1; c &lt; W; c++) &#123; if (w[k] &gt; c) &#123; b[k][c] = b[k - 1][c]; &#125; else &#123; int value1 = b[k - 1][c - w[k]] + v[k]; // 拿第k件物品 int value2 = b[k - 1][c]; // 不拿第k件物品 b[k][c] = Math.max(value1, value2); &#125; &#125; &#125; System.out.println(b[5][20]); &#125;&#125; 1234567891011121314151617181920212223242526//递归方式 /** * @param w 重量数组 * @param v 物品价值数组 * @param capacity 背包容量 * @param i 物品i开始 * @return */ private static int getValue(int[] w, int[] v, int capacity, int i) &#123; if (i == w.length - 1) &#123; //最后一件物品 return w[i] &lt;= capacity ? v[i] : 0; &#125; else &#123; if (capacity &lt; w[i]) &#123; //背包装不了不了的情况 return getValue(w, v, capacity, i + 1); &#125; //能装下, 但是考虑拿不拿 //拿 int v1 = getValue(w, v, capacity - w[i], i + 1) + v[i]; //不拿 int v2 = getValue(w, v, capacity, i + 1); return Math.max(v1, v2); &#125; &#125; 贪心算法 所谓贪心算法是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的仅是在某种意义上的局部最优解。 必须注意的是，贪心算法不是对所有问题都能得到整体最优解，采用的贪心策略一定要仔细分析其是否满足无后效性。即某个状态以后的过程不会影响以前的状态 贪心算法的基本思路：​ 1.建立数学模型来描述问题。 ​ 2.把求解的问题分成若干个子问题。 ​ 3.对每一子问题求解，得到子问题的局部最优解。 ​ 4.把子问题的解局部最优解合成原来解问题的一个解。 贪心算法适用的问题贪心策略适用的前提是：局部最优策略能导致产生全局最优解。 实际上，贪心算法适用的情况很少。一般，对一个问题分析是否适用于贪心算法，可以先选择该问题下的几个实际数据进行分析，就可做出判断。 贪心算法的实现框架 从问题的某一初始解出发； ​ while （能朝给定总目标前进一步） ​ { ​ 利用可行的决策，求出可行解的一个解元素； ​ } ​ 由所有解元素组合成问题的一个可行解； 案例物品装载问题:最优装载问题，给出n个物体，第i个物体重量为wi。选择尽量多的物体，使得总重量不超过C。经过前面的学习很容易想到贪心策略，那就是每次选重量最轻的物体，那么物体数就最多。 1234567891011121314151617181920/** * @param n 物品数 * @param w 物品对象的重量数组 * @param c 限重值 * @return */ private static int f(int n, int[] w, int c) &#123; int sum = 0; int cnt = 0; for (int i = 0; i &lt; n; i++) &#123; sum += w[i]; //优先选择重量最轻的物体, 则最终的物体数就会最多,且不超过限重 if (sum &lt;= c) &#123; cnt++; &#125; else &#123; break; &#125; &#125; return cnt; &#125; 回溯算法 回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。 许多复杂的，规模较大的问题都可以使用回溯法，有“通用解题方法”的美称。 回溯算法设计模式: （1）针对所给问题，确定问题的解空间： 首先应明确定义问题的解空间，问题的解空间应至少包含问题的一个（最优）解。 ​ （2）确定结点的扩展搜索规则 ​ （3）以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。 案例 八皇后问题 1234567891011121314151617181920void back_tracking(int row=0) //算法函数，从第0行开始遍历&#123; if (row==n) t ++; //判断若遍历完成，就进行计数 for (int col=0;col&lt;n;col++) //遍历棋盘每一列 &#123; queen[row] = col; //将皇后的位置记录在数组 if (is_ok(row)) //判断皇后的位置是否有冲突 back_tracking(row+1); //递归，计算下一个皇后的位置 &#125;&#125;bool is_ok(int row)&#123; //判断设置的皇后是否在同一行，同一列，或者同一斜线上 for (int j=0;j&lt;row;j++) &#123; if (queen[row]==queen[j]||row-queen[row]==j-queen[j]||row+queen[row]==j+queen[j]) return false; &#125; return true;&#125; 01背包问题 用回溯法解问题时，应明确定义问题的解空间。问题的解空间至少包含问题的一个(最优)解。对于 n=3 时的 0/1 背包问题，可用一棵完全二叉树表示解空间 求解步骤1)针对所给问题，定义问题的解空间； 2)确定易于搜索的解空间结构； 3)以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。 常用的剪枝函数：用约束函数在扩展结点处剪去不满足约束的子树；用限界函数剪去得不到最优解的子树。 回溯法对解空间做深度优先搜索时，有递归回溯和迭代回溯（非递归）两种方法，但一般情况下用递归方法实现回溯法。 算法描述解 0/1 背包问题的回溯法在搜索解空间树时，只要其左儿子结点是一个可行结点，搜索就进入其左子树。当右子树中有可能包含最优解时才进入右子树搜索。否则将右子树剪去。 12345678910111213141516171819202122232425void dfs(int i,int cv,int cw)&#123; //cw当前包内物品重量，cv当前包内物品价值 if(i&gt;n) &#123; if(cv&gt;bestval) //是否超过了最大价值 &#123; bestval=cv; //得到最大价值 for(i=1;i&lt;=n;i++) bestx[i]=x[i]; //得到选中的物品 &#125; &#125; else for(int j=0;j&lt;=1;j++) //枚举物体i所有可能的路径， &#123; x[i]=j; if(cw+x[i]*w[i]&lt;=TotCap) //满足约束,继续向子节点探索 &#123; cw+=w[i]*x[i]; cv+=val[i]*x[i]; dfs(i+1,cv,cw); cw-=w[i]*x[i]; //回溯上一层物体的选择情况 cv-=val[i]*x[i]; &#125; &#125;&#125; 主函数部分 123456789101112131415161718192021222324252627int main()&#123; int i; bestval=0; cout&lt;&lt;\"请输入背包最大容量:\"&lt;&lt;endl;; cin&gt;&gt;TotCap; cout&lt;&lt;\"请输入物品个数:\"&lt;&lt;endl; cin&gt;&gt;n; cout&lt;&lt;\"请依次输入物品的重量:\"&lt;&lt;endl; for(i=1;i&lt;=n;i++) cin&gt;&gt;w[i]; cout&lt;&lt;\"请依次输入物品的价值:\"&lt;&lt;endl; for(i=1;i&lt;=n;i++) cin&gt;&gt;val[i]; dfs(1,0,0); cout&lt;&lt;\"最大价值为:\"&lt;&lt;endl; cout&lt;&lt;bestval&lt;&lt;endl; cout&lt;&lt;\"被选中的物品的标号依次是:\"&lt;&lt;endl; for(i=1;i&lt;=n;i++) if(bestx[i]==1) cout&lt;&lt;i&lt;&lt;\" \"; cout&lt;&lt;endl; return 0;&#125; 分支限界法 类似于回溯法，也是一种在问题的解空间树T上搜索问题解的算法。但在一般情况下，分支限界法与回溯法的求解目标不同。回溯法的求解目标是找出T中满足约束条件的所有解，而分支限界法的求解目标则是找出满足约束条件的一个解，或是在满足约束条件的解中找出使某一目标函数值达到极大或极小的解，即在某种意义下的最优解。 所谓“分支”就是采用广度优先的策略，依次搜索E-结点的所有分支，也就是所有相邻结点，抛弃不满足约束条件的结点，其余结点加入活结点表。然后从表中选择一个结点作为下一个E-结点，继续搜索。 ​ 选择下一个E-结点的方式不同，则会有几种不同的分支搜索方式。 1）FIFO搜索 2）LIFO搜索 3）优先队列式搜索 回溯法和分支限界法的一些区别 方法对解空间树的搜索方式 存储结点的常用数据结构 结点存储特性常用应用 回溯法深度优先搜索堆栈活结点的所有可行子结点被遍历后才被从栈中弹出找出满足约束条件的所有解 分支限界法广度优先或最小消耗优先搜索队列、优先队列每个结点只有一次成为活结点的机会找出满足约束条件的一个解或特定意义下的最优解","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"},{"name":"数据结构","slug":"数据结构","permalink":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"算法","slug":"算法","permalink":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"排序","permalink":"/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"Java高级核心","slug":"Java高级","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T13:47:17.964Z","comments":true,"path":"2019/10/20/Java高级/","link":"","permalink":"/2019/10/20/Java%E9%AB%98%E7%BA%A7/","excerpt":"","text":"队列等待队列（Condition Queue） 我们都熟悉wait/notify，它主要是实现线程间协作的，其常用的使用模式如下： 123456789101112131415public synchronized void produce(T t) throws InterruptedException &#123; while (isFull())&#123; wait(); &#125; produce(t); notifyAll();&#125;public synchronized T consume() throws InterruptedException &#123;while (isEmpty())&#123; wait();&#125;T t = consume();notifyAll();return t;&#125; 当条件满足，原来等待的线程就会立即被唤醒，这就要涉及到等待队列，等待队列中的是等待某类条件发生的线程。每一个对象都可以作为锁对象，也同时被当作一个等待队列，并具有wait，notify，notifyall方法，另见图： 判断条件总是涉及到一些状态，如集合是否已满，是否为空等等，这些状态变量必须被锁监控，因为线程在等待或者唤醒另一个线程前，需要访问、操作这些与条件相关的状态变量，而加锁可以保证状态的一致性。另外，正如上例所示，wait方法必须包含在while循环中，原因有二： 1、从线程被唤醒到重新获得锁的间隙，其他线程获取了锁并且改变了状态，使得条件重新变为false。 2、如果多种条件与一个等待队列关联，必须使用notifyAll,一个线程可能在条件不满足的情况下被唤醒，这时候需要重新检查条件。 对象的内置锁只有一个内置等待队列与其关联，这样多个唤醒条件不同的线程就必须在同一个等待队列上，唤醒线程时必须使用notifyAll，导致大部分不符合条件的线程将被唤醒并且参与锁竞争，上下文切换频繁，性能下降，当然，notifyAll是一种比较安全保险的做法。上次我们提过还有另一种实现锁的形式，即Lock，与其对应的是Condition，它可以根据不同的条件提供对应的condition，可将上述使用模式改装一下： 12345678910111213141516171819202122232425262728protected final Lock lock = new ReentrantLock(); private final Condition notFull = lock.newCondition(); private final Condition notEmpty = lock.newCondition(); public void produce(T t) throws InterruptedException &#123; lock.lock(); try &#123; while (isFull()) &#123; notFull.await(); &#125; produce(t); notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public T consume() throws InterruptedException &#123; lock.lock(); try &#123; while (isEmpty()) &#123; notEmpty.await(); &#125; T t = consume(); notFull.signal(); return t; &#125; finally &#123; lock.unlock(); &#125; &#125; 通过wait/notify实现线程间协作，是需要一定的技巧的，初级的开发人员不一定能正确使用，我们可以使用一些并发工具类，像LinkedBlockingQueue，ConcurrentHashMap，CountDownLatch实现相应的功能 BlockingQueue 在新增的Concurrent包中，BlockingQueue很好的解决了多线程中，如何高效安全“传输”数据的问题 ​ 常用的队列主要有以下两种：（当然通过不同的实现方式，还可以延伸出很多不同类型的队列，DelayQueue就是其中的一种） 先进先出（FIFO）：先插入的队列的元素也最先出队列，类似于排队的功能。从某种程度上来说这种队列也体现了一种公平性。 后进先出（LIFO）：后插入队列的元素最先出队列，这种队列优先处理最近发生的事件。 ​ 当队列中没有数据的情况下，消费者端的所有线程都会被自动阻塞（挂起），直到有数据放入队列。 当队列中填满数据的情况下，生产者端的所有线程都会被自动阻塞（挂起），直到队列中有空的位置，线程被自动唤醒。BlockingQueue的核心方法: offer(anObject):表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false.（本方法不阻塞当前执行方法的线程） put(anObject):把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断,直到BlockingQueue里面有空间再继续. 获取数据 poll(long timeout, TimeUnit unit)：从BlockingQueue取出一个队首的对象，如果在指定时间内，队列一旦有数据可取，则立即返回队列中的数据。否则知道时间超时还没有数据可取，返回失败。 take():取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到BlockingQueue有新的数据被加入; 常见BlockingQueue 1. ArrayBlockingQueue 基于数组的阻塞队列实现，在ArrayBlockingQueue内部，维护了一个定长数组，以便缓存队列中的数据对象，这是一个常用的阻塞队列，除了一个定长数组外，ArrayBlockingQueue内部还保存着两个整形变量，分别标识着队列的头部和尾部在数组中的位置。 ArrayBlockingQueue在生产者放入数据和消费者获取数据，都是共用同一个锁对象，由此也意味着两者无法真正并行运行，这点尤其不同于LinkedBlockingQueue；按照实现原理来分析，ArrayBlockingQueue完全可以采用分离锁，从而实现生产者和消费者操作的完全并行运行。Doug Lea之所以没这样去做，也许是因为ArrayBlockingQueue的数据写入和获取操作已经足够轻巧，以至于引入独立的锁机制，除了给代码带来额外的复杂性外，其在性能上完全占不到任何便宜。 ArrayBlockingQueue和LinkedBlockingQueue间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的Node对象。这在长时间内需要高效并发地处理大批量数据的系统中，其对于GC的影响还是存在一定的区别。而在创建ArrayBlockingQueue时，我们还可以控制对象的内部锁是否采用公平锁，默认采用非公平锁。 2. LinkedBlockingQueue 基于链表的阻塞队列，同ArrayListBlockingQueue类似，其内部也维持着一个数据缓冲队列（该队列由一个链表构成），当生产者往队列中放入一个数据时，队列会从生产者手中获取数据，并缓存在队列内部，而生产者立即返回；只有当队列缓冲区达到最大值缓存容量时（LinkedBlockingQueue可以通过构造函数指定该值），才会阻塞生产者队列，直到消费者从队列中消费掉一份数据，生产者线程会被唤醒，反之对于消费者这端的处理也基于同样的原理。而LinkedBlockingQueue之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 作为开发者，我们需要注意的是，如果构造一个LinkedBlockingQueue对象，而没有指定其容量大小，LinkedBlockingQueue会默认一个类似无限大小的容量（Integer.MAX_VALUE），这样的话，如果生产者的速度一旦大于消费者的速度，也许还没有等到队列满阻塞产生，系统内存就有可能已被消耗殆尽了。 ArrayBlockingQueue和LinkedBlockingQueue是两个最普通也是最常用的阻塞队列，一般情况下，在处理多线程间的生产者消费者问题，使用这两个类足以。 3. DelayQueue DelayQueue中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。DelayQueue是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。 使用场景： DelayQueue使用场景较少，但都相当巧妙，常见的例子比如使用一个DelayQueue来管理一个超时未响应的连接队列。 4. PriorityBlockingQueue 基于优先级的阻塞队列（优先级的判断通过构造函数传入的Compator对象来决定），但需要注意的是PriorityBlockingQueue并不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者。因此使用的时候要特别注意，生产者生产数据的速度绝对不能快于消费者消费数据的速度，否则时间一长，会最终耗尽所有的可用堆内存空间。在实现PriorityBlockingQueue时，内部控制线程同步的锁采用的是公平锁。 5. SynchronousQueue 一种无缓冲的等待队列，类似于无中介的直接交易，有点像原始社会中的生产者和消费者，生产者拿着产品去集市销售给产品的最终消费者，而消费者必须亲自去集市找到所要商品的直接生产者，如果一方没有找到合适的目标，那么对不起，大家都在集市等待。相对于有缓冲的BlockingQueue来说，少了一个中间经销商的环节（缓冲区），如果有经销商，生产者直接把产品批发给经销商，而无需在意经销商最终会将这些产品卖给那些消费者，由于经销商可以库存一部分商品，因此相对于直接交易模式，总体来说采用中间经销商的模式会吞吐量高一些（可以批量买卖）；但另一方面，又因为经销商的引入，使得产品从生产者到消费者中间增加了额外的交易环节，单个产品的及时响应性能可能会降低。 声明一个SynchronousQueue有两种不同的方式，它们之间有着不太一样的行为。公平模式和非公平模式的区别: 如果采用公平模式：SynchronousQueue会采用公平锁，并配合一个FIFO队列来阻塞多余的生产者和消费者，从而体系整体的公平策略； 但如果是非公平模式（SynchronousQueue默认）：SynchronousQueue采用非公平锁，同时配合一个LIFO队列来管理多余的生产者和消费者，而后一种模式，如果生产者和消费者的处理速度有差距，则很容易出现饥渴的情况，即可能有某些生产者或者是消费者的数据永远都得不到处理。 设计模式 常见: 单例模式、工厂模式、建造模式、观察者模式、适配器模式、代理模式、装饰模式. 设计模式的六大原则及其含义: 单一职责原则：一个类只负责一个功能领域中的相应职责，或者可以定义为：就一个类而言，应该只有一个引起它变化的原因。主要作用实现代码高内聚，低耦合。 开闭原则：一个软件实体应当对扩展开放，对修改关闭。即软件实体应尽量在不修改原有代码的情况下进行扩展。 里氏替换原则：所有引用基类（父类）的地方必须能透明地使用其子类的对象。里氏替换原则是实现开闭原则的方式之一 依赖倒置原则：抽象不应该依赖于细节，细节应当依赖于抽象。换言之，要针对接口编程，而不是针对实现编程。 接口隔离原则：使用多个专门的接口，而不使用单一的总接口，即客户端不应该依赖那些它不需要的接口。 迪米特法则：一个软件实体应当尽可能少地与其他实体发生相互作用。 常见模式及优缺点： 饿汉式： 优点：不用加锁可以确保对象的唯一性，线程安全。 缺点：初始化对象会浪费不必要的资源，未实现延迟加载。 懒汉式： 优点：实现了延时加载。 缺点：线程不安全，想实现线程安全，得加锁（synchronized），这样会浪费一些不必要的资源。 双重检测锁式（ Double Check Lock –DCL）： 优点：资源利用率高，效率高。 缺点：第一次加载稍慢，由于java处理器允许乱序执行，偶尔会失败。 静态内部式： 优点：第一次调用方法时才加载类，不仅保证线程安全还能保证对象的唯一，还延迟了单例的实例化 缺点：无确定 枚举实现单例模式 优点: 线程安全, 任何模式下都是单例的, 包括序列化 推荐使用静态内部式 1234567891011121314155) public class Singleton4 &#123; /* *当第一次加载Singleton类时并不会初始化SINGLRTON,只有第一次调用getInstance方法的时候才会初始化SINGLETON *第一次调用getInstance 方法的时候虚拟机才会加载SingletonHoder类,这种方式不仅能够保证线程安全,也能够保证对象的唯一, *还延迟了单例的实例化,所有推荐使用这种方式 * */ private Singleton4() &#123; &#125; public Singleton4 getInstance() &#123; return SingletonHolder.SINGLETON; &#125; private static class SingletonHolder &#123; private static final Singleton4 SINGLETON = new Singleton4(); &#125;&#125; 设计模式在实际场景的应用单例：连接数据库，记录日志 ==Spring中用到了哪些设计模式== 工厂模式：spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。 代理模式：Spring的AOP就是代理模式的体现。 观察者模式：常用的地方是Listener的实现，spring中ApplicationListener就是观察者的体现。 策略模式：spring在实例化对象的时候使用到了。 工厂方法：Spring中的FactoryBean就是典型的工厂方法模式。 单例模式: 参考：https://www.cnblogs.com/hwaggLee/p/4510687.html ==MyBatis中用到了哪些设计模式== Builder模式，例如SqlSessionFactoryBuilder、XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuilder、CacheBuilder； 工厂模式，例如SqlSessionFactory、ObjectFactory、MapperProxyFactory； 单例模式，例如ErrorContext和LogFactory； 代理模式，Mybatis实现的核心，比如MapperProxy、ConnectionLogger，用的jdk的动态代理；还有executor.loader包使用了cglib或者javassist达到延迟加载的效果； 组合模式，例如SqlNode和各个子类ChooseSqlNode等； 模板方法模式，例如BaseExecutor和SimpleExecutor，还有BaseTypeHandler和所有的子类例如IntegerTypeHandler； 适配器模式，例如Log的Mybatis接口和它对jdbc、log4j等各种日志框架的适配实现； 装饰者模式，例如Cache包中的cache.decorators子包中等各个装饰者的实现； 迭代器模式，例如迭代器模式PropertyTokenizer； 参考：https://www.cnblogs.com/shuchen007/p/9193179.html 代理模式(Proxy)静态代理: AspectJ 动态代理: cglib(静态) jdk(动态) 类加载器 顾名思义，类加载器（class loader）用来加载 Java 类到 Java 虚拟机中。类加载器负责读取 Java 字节代码，并转换成 java.lang.Class类的一个实例。 有三类类加载器: 启动类加载器 BootStrap ClassLoader 扩展类加载器 Extension ClassLoader 应用类加载器 Application ClassLoader ClassLoaderClassLoader类是一个抽象类，它定义了类加载器的基本方法。 方法 说明 getParent() 返回该类加载器的父类加载器。 loadClass(String name) 加载名称为 name的类，返回的结果是 java.lang.Class类的实例。 findClass(String name) 查找名称为 name的类，返回的结果是 java.lang.Class类的实例。 findLoadedClass(String name) 查找名称为 name的已经被加载过的类，返回的结果是 java.lang.Class类的实例。 defineClass(String name, byte[] b, int off, int len) 把字节数组 b中的内容转换成 Java 类，返回的结果是 java.lang.Class类的实例。这个方法被声明为 final的。 来看看 loadClass 方法的代码：双亲委托机制自定义类加载器一般只重写findClass方法即可 1234567891011121314protected Class&lt;?&gt; loadClass(String name, boolean resolve)&#123; Class c = findLoadedClass(name);//是否已经加载 if (c == null) &#123;//没有加载 if (parent != null) &#123; //使用父加载器加载此类 c = parent.loadClass(name, false); &#125; if (c == null) &#123; // 如果父加载器没有成功加载，则自己尝试加载 c = findClass(name); &#125; &#125; return c;&#125; IOJava的io，nio，bio区别 Java IO（Java数据流）主要就是Java用来读取和输出数据流 读取纯文本数据优选用字符流，其他使用字节流 Java中IO主要有两类 |——&gt;字节流（读写以字节（8bit）为单位，InputStream和OutputStream为主要代表 |——&gt;字符流（读写以字符为单位，Reader和Writer为主要代表） BIO, NIO, AIO的区别 BIO: 同步并阻塞, 服务器模式为一个连接一个线程, 客户端有连接请求时,就必须开启一个线程进行处理, 会造成很大的线程开销–数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机1000）的情况下 NIO: 同步非阻塞,服务器模式为: 一个请求一个线程, 即客户端的请求都会注册到多路复用机,轮询到有io请求时才启动一个线程进行处理.把一些无效的连接挡在了启动线程之前，减少了这部分资源的浪费 –对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发 AIO: 一部非阻塞, 一个有效的请求一个线程, 客户端的io请求都是os先完成了在通知服务器应用取启动线程进行处理将一些暂时可能无效的请求挡在了启动线程之前 I/O 模型一个输入操作通常包括两个阶段： 等待数据准备好(数据复制到内核缓冲区) 从内核向进程复制数据 对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待数据到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。 Unix 有五种 I/O 模型： 阻塞式 I/O (BIO) 非阻塞式 I/O (NIO) I/O 复用（select 和 poll） 信号驱动式 I/O（SIGIO） 异步 I/O（AIO） 阻塞式 I/O 应用进程被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区中才返回。 应该注意到，在阻塞的过程中，其它应用进程还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其它应用进程还可以执行，所以不消耗 CPU 时间，这种模型的 CPU 利用率会比较高。 下图中，recvfrom() 用于接收 Socket 传来的数据，并复制到应用进程的缓冲区 buf 中。这里把 recvfrom() 当成系统调用。 1ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen);Copy to clipboardErrorCopied 非阻塞式 I/O 应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式称为轮询（polling）。 由于 CPU 要处理更多的系统调用，因此这种模型的 CPU 利用率比较低-因为要去不断的轮询是否数据准备好。 ==此时数据准备阶段不会阻塞, 而数据复制阶段一样会阻塞== I/O 复用 使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读。这一过程会被阻塞，当某一个套接字可读时返回，之后再使用 recvfrom 把数据从内核复制到进程中。 它可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。 I/O复用可以避免频繁创建和销毁线程, 以及切换线程的开销 如果一个 Web 服务器没有 I/O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。 ==在数据准备阶段select是阻塞的== 信号驱动 I/O 应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。 相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高-数据准备阶段不阻塞。 异步 I/O (AIO) 应用进程执行 aio_read 系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。 异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。 五大 I/O 模型比较 同步 I/O：将数据从内核缓冲区复制到应用进程缓冲区的阶段（第二阶段），应用进程会阻塞。 异步 I/O：第二阶段应用进程不会阻塞。 同步 I/O 包括阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O ，它们的主要区别在第一个阶段。 非阻塞式 I/O 、信号驱动 I/O 和异步 I/O 在第一阶段不会阻塞。 I/O 复用 详细讲解select/poll/epoll 都是 I/O 多路复用的具体实现，select 出现的最早，之后是 poll，再是 epoll。 select1int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);Copy to clipboardErrorCopied select 允许应用程序监视一组文件描述符，等待一个或者多个描述符成为就绪状态，从而完成 I/O 操作。 fd_set 使用数组实现，数组大小使用 FD_SETSIZE 定义，所以只能监听少于 FD_SETSIZE 数量的描述符。有三种类型的描述符类型：readset、writeset、exceptset，分别对应读、写、异常条件的描述符集合。 timeout 为超时参数，调用 select 会一直阻塞直到有描述符的事件到达或者等待的时间超过 timeout。 成功调用返回结果大于 0，出错返回结果为 -1，超时返回结果为 0。 poll1int poll(struct pollfd *fds, unsigned int nfds, int timeout);Copy to clipboardErrorCopied poll 的功能与 select 类似，也是等待一组描述符中的一个成为就绪状态。 poll 中的描述符是 pollfd 类型的数组，pollfd 的定义如下： 比较1. 功能select 和 poll 的功能基本相同，不过在一些实现细节上有所不同。 select 会修改描述符，而 poll 不会； select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译；而 poll 没有描述符数量的限制； poll 提供了更多的事件类型，并且对描述符的重复利用上比 select 高。 如果一个线程对某个描述符调用了 select 或者 poll，另一个线程关闭了该描述符，会导致调用结果不确定。 2. 速度select 和 poll 速度都比较慢，每次调用都需要将全部描述符从应用进程缓冲区复制到内核缓冲区。 3. 可移植性几乎所有的系统都支持 select，但是只有比较新的系统支持 poll。 epoll123int epoll_create(int size);int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);Copy to clipboardErrorCopied epoll_ctl() 用于向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。 从上面的描述可以看出，epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。 epoll 仅适用于 Linux OS。 epoll 比 select 和 poll 更加灵活而且没有描述符数量限制。 epoll 对多线程编程更有友好，一个线程调用了 epoll_wait() 另一个线程关闭了同一个描述符也不会产生像 select 和 poll 的不确定情况。 工作模式epoll 的描述符事件有两种触发模式：LT（level trigger）和 ET（edge trigger）。 1. LT 模式 当 epoll_wait() 检测到描述符事件到达时，将此事件通知进程，进程可以不立即处理该事件，下次调用 epoll_wait() 会再次通知进程。是默认的一种模式，并且同时支持 Blocking 和 No-Blocking。 2. ET 模式 和 LT 模式不同的是，通知之后进程必须立即处理事件，下次再调用 epoll_wait() 时不会再得到事件到达的通知。 很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。只支持 No-Blocking，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 应用场景很容易产生一种错觉认为只要用 epoll 就可以了，select 和 poll 都已经过时了，其实它们都有各自的使用场景。 1. select 应用场景select 的 timeout 参数精度为 1ns，而 poll 和 epoll 为 1ms，因此 select 更加适用于实时性要求比较高的场景，比如核反应堆的控制。 select 可移植性更好，几乎被所有主流平台所支持。 2. poll 应用场景poll 没有最大描述符数量的限制，如果平台支持并且对实时性要求不高，应该使用 poll 而不是 select。 3. epoll 应用场景只需要运行在 Linux 平台上，有大量的描述符需要同时轮询，并且这些连接最好是长连接。 需要同时监控小于 1000 个描述符，就没有必要使用 epoll，因为这个应用场景下并不能体现 epoll 的优势。 需要监控的描述符状态变化多，而且都是非常短暂的，也没有必要使用 epoll。因为 epoll 中的所有描述符都存储在内核中，造成每次需要对描述符的状态改变都需要通过 epoll_ctl() 进行系统调用，频繁系统调用降低效率。并且 epoll 的描述符存储在内核，不容易调试。 I/O 复用（select 和 poll） 信号驱动式 I/O（SIGIO） 异步 I/O（AIO） 多线程进程和线程区别 进程是资源（CPU、内存等）分配的基本单位，它是程序执行时的一个实例 线程是程序执行时的最小单位，它是进程的一个执行流，是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量,是一个独立的执行单元主要用于解决程序执行的效率 守护线程&amp;非守护线程 GC线程属于守护线程, 和主线程同生共死 用户线程也叫非守护线程, 用户自己创建的线程, 如果主线程停止掉,不会影响用户线程. 用户线程非守护线程 线程的状态转换 线程安全coout++ count++ 这行代码实际上需要执行三个指令： getfield：从内存中获取变量 count 的值iadd：将 count 加 1putfield：将加 1 后的结果赋值给 count 变量这也就是线程不安全的原因所在，因为 count++ 操作不具备原子性。 原子性操作指的是不可被中断的一个或一系列操作。下图描述了为什么非原子操作造成了这里的线程不安全问题 假设有两个线程去执行 add 操作，此时 count 是 0，那么存在上图中的这种可能，在线程 A 执行这三步的过程中 cpu 时间片耗尽线程 B 被调度，此时由于内存中 count 的值仍为 0（因为线程 A 的操作结果还未刷新到内存中），所以线程 B 仍是在 0 的基础上执行自增，所以导致最终内存中的 count 是 1，而不是 2. 为什么要用线程池? 线程池作用就是限制系统中执行线程的数量。根据系统的环境情况, 可以手动或者自动的设置线程数量, 避免浪费系统资源, 造成系统拥挤, 当一个新任务需要运行时，如果线程池 中有等待的工作线程，就可以开始运行了；否则进入等待队列。 减少了创建和销毁线程的次数, 每个工作线程都可以被重复利用, 可执行多个任务 可以根据系统的承受能力, 调整线程池中工作线程的数目, 防止因为消耗过多的内存, (每个线程需要大约1MB内存，线程开的越多，消耗的内存也就越大，最后死机)。 Java里边线程池的顶级接口时Executor, 但是严格意义上将Executor并不是一个线程池, 而是一个执行工具,真正的线程池接口时ExecutorService 线程中重要的几个类 ExecutorService 真正的线程池接口。 ScheduledExecutorService能和Timer/TimerTask类似，解决那些需要任务重复执行的问题。 ThreadPoolExecutorExecutorService的默认实现。 ScheduledThreadPoolExecutor继承ThreadPoolExecutor的ScheduledExecutorService接口实现，周期性任务调度的类实现。 new Thread的弊端 每次new Thread新建对象性能差。 线程缺乏统一管理，可能无限制新建线程，相互之间竞争，及可能占用过多系统资源导致死机或oom。 缺乏更多功能，如定时执行、定期执行、线程中断。 Java提供的四种线程池的好处在于： 重用存在的线程，减少对象创建、消亡的开销，性能佳。 可有效控制最大并发线程数，提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。 提供定时执行、定期执行、单线程、并发数控制等功能。 四种线程池Java通过Executors提供四种线程池,分别为: newCachedThreadPool 创建一个可缓存的线程池, 如果线程池长度超过处理需要, 可以灵活回收空闲线程, 若无可回收,则新建线程 ==适应场景: 创建一个可以无限扩大的线程池，适用于服务器负载较轻，执行很多短期异步任务。== newFixedThreadPool 创建一个定长线程池,可控制线程最大并发数, 采用无界的阻塞队列，所以实际线程数量永远不会变化,超出的线程会在队列中等待 适应场景:适用于可以预测线程数量的业务中，或者服务器负载较重，对当前线程数量进行限制。 newScheduledThreadPool 创建一个定长线程池, 至此定时及周期性任务执行 ==适应场景: 适用于需要多个后台线程执行周期任务的场景。== newSingleThreadExecutor 创建一个单线程化的线程池, 它只会用唯一的工作线程来执行任务, 保证所有任务按照指定顺序(FIFO, LIFO,优先级)执行 ==适应场景: 适用于需要保证顺序执行各个任务，并且在任意时间点，不会有多个线程是活动的场景。== submit()和execute()的以及shutdown()和shutdownNow()的区别 submit()，提交一个线程任务，可以接受回调函数的返回值吗，适用于需要处理返回着或者异常的业务场景 execute()，执行一个任务，没有返回值 shutdown()，表示不再接受新任务，但不会强行终止已经提交或者正在执行中的任务 shutdownNow()，对于尚未执行的任务全部取消，正在执行的任务全部发出interrupt()，停止执行 RejectedExecutionHandler 线程池四种拒绝任务策略 线程池有一个任务队列，用于缓存所有待处理的任务，正在处理的任务将从任务队列中移除。因此在任务队列长度有限的情况下就会出现新任务的拒绝处理问题，需要有一种策略来处理应该加入任务队列却因为队列已满无法加入的情况。另外在线程池关闭的时候也需要对任务加入队列操作进行额外的协调处理。 RejectedExecutionHandler提供了四种方式来处理任务拒绝策略: -&gt;==这四种策略是独立无关的== 1、直接丢弃（DiscardPolicy） 2、丢弃队列中最老的任务(DiscardOldestPolicy)。 3、抛异常(AbortPolicy) 4、将任务分给调用线程来执行(CallerRunsPolicy)。 Runnable, Callable, Future和 FutureTask 直接继承Thread, 和Runnable都不能返回执行结果, 就必须通过共享变量或者使用线程通信的方式来达到效果，这样使用起来就比较麻烦。 Callable与RunnableCallable位于java.util.concurrent包下，它也是一个接口，在它里面也只声明了一个方法，只不过这个方法叫做call()：一般情况下是配合ExecutorService来使用的，在ExecutorService接口中声明了若干个submit方法的重载版本 123&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);Future&lt;?&gt; submit(Runnable task); Future Future就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过get方法获取执行结果，该方法会阻塞直到任务返回结果。Future类位于java.util.concurrent包下，它是一个接口 Future提供了三种功能： 1）判断任务是否完成； 2）能够中断任务； 3）能够获取任务执行结果。 因为Future只是一个接口，所以是无法直接用来创建对象使用的，因此就有了下面的FutureTask。 FutureTask 可以看出RunnableFuture继承了Runnable接口和Future接口，而FutureTask实现了RunnableFuture接口。所以它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值。 1234public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt;public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run();&#125; 锁 常见的锁有synchronized、volatile、偏向锁、轻量级锁、重量级锁 死锁的原因 互斥性: 某一段时间内, 某一资源只能一个线程使用 请求保持: A申请了一部分资源不足以运行,需要额外的其他资源,但又申请不到, 而其他线程又拿不到A保持的那份资源 不可剥夺: 进程未使用完之前,资源不可剥夺 循环等待: 进程资源形成环型链, 导致任何一个线程都拿不到运行所需的全部资源 CAS CAS(Compare and Swap): 它的作用是将指定内存地址的内容与所给的某个值相比，如果相等，则将其内容替换为指令中提供的新值，如果不相等，则更新失败。这一比较并交换的操作是原子的，不可以被中断** ==CAS是通过硬件命令保证了原子性== （1）在多线程竞争下，加锁、释放锁会导致比较多的上下文切换和调度延时，引起性能问题。 （2）一个线程持有锁会导致其它所有需要此锁的线程挂起。 （3）如果一个优先级高的线程等待一个优先级低的线程释放锁会导致优先级倒置，引起性能风险。 volatile是不错的机制，但是volatile不能保证原子性。因此对于同步最终还是要回到锁机制上来。独占锁是一种悲观锁，synchronized就是一种独占锁，会导致其它所有需要锁的线程挂起，等待持有锁的线程释放锁。而另一个更加有效的锁就是乐观锁。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。乐观锁用到的机制就是CAS，Compare and Swap。 CAS导致的问题 ABA问题 大多数情况下乐观锁的实现都会通过引入一个版本号标记这个对象，每次修改版本号都会变话，比如使用时间戳作为版本号，这样就可以很好的解决ABA问题 在JDK中提供了AtomicStampedReference类来解决这个问题，思路是一样的。这个类也维护了一个int类型的标记stamp，每次更新数据的时候顺带更新一下stamp。 1234 // 带有时间戳的原子类，不存在ABA问题，第二个参数就是默认时间戳，这里指定为0AtomicStampedReference&lt;Integer&gt; a2 = new AtomicStampedReference&lt;Integer&gt;(10, 0);//可以看到使用AtomicStampedReference进行compareAndSet的时候，除了要验证数据，还要验证时间戳。 a2.compareAndSet(10, 11, stamp, stamp + 1); AtomicInteger的CAS原理 ​ 通过查看AtomicInteger的源码可知， 通过申明一个volatile （内存锁定，同一时刻只有一个线程可以修改内存值）类型的变量，再加上unsafe.compareAndSwapInt的方法，来保证实现线程同步的。 1234private volatile int value;public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; 循环时间长开销大 自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i＝2,j=a，合并一下ij=2a，然后用CAS来操作ij。从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。 synchronized、volatile的区别 相同点：都保证了可见性 不同点 ： ==volatile不能保证原子性==，但是==synchronized能保证原子性且会发生阻塞==（在线程状态转换中详说），开销更大。 Volatile volatile可以看做是一种synchronized的轻量级锁，他能够保证并发时，被它修饰的共享变量的可见性，当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 volatile可以解决指令重排，它使用的是内存屏障进行解决的，所谓的内存屏障是一个cpu命令，它有两个作用保证特定的执行顺序，保证可见性，通过在volatile 指令前后增加内存屏障从而解决指令重排问题。 实现原理: 被volatile修饰的共享变量在进行写操作的时候 1、将当前处理器缓存行的数据写回到系统内存。 2、这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 使用场景: 1.访问变量不需要加锁（加锁的话使用volatile就没必要了） 2、对变量的写操作不依赖于当前值(因为他不能保证原子性) 3.该变量没有包含在具有其他变量的不变式中。 一般我们会用来修饰状态标志；读写锁（读&gt;&gt;写，对写加锁，读不加锁）；DCL的单例模式中；volatile bean（例如放入HTTPSession中的对象） synchronized synchronized是并发编程中接触的最基本的同步工具，是一种重量级锁,也是 悲观锁，也是java内置的同步机制，首先我们知道synchronized提供了互斥性的语义和可见性，那么我们可以通过使用它来保证并发的安全。 synchronized三种用法 用于类上: 当使用synchronized修饰类静态方法时，那么当前加锁的级别就是类，当多个线程并发访问该类（所有实例对象）的同步方法以及同步代码块时，会进行同步 用于代码块: 当使用synchronized修饰代码块时，那么当前加锁的级别就是synchronized（X）中配置的x对象实例，当多个线程并发访问该对象的同步方法、同步代码块以及当前的代码块时，会进行同步。 ==使用同步代码块时要注意的是不要使用String类型对象，因为String常量池的存在，所以很容易导致出问题。== Synchronized和CAS区别 CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则返回V。这是一种乐观锁的思路，它相信在它修改之前，没有其它线程去修改它； 而Synchronized是一种悲观锁，它认为在它修改之前，一定会有其它线程去修改它，悲观锁效率很低。下面来看一下AtomicInteger是如何利用CAS实现原子性操作的。 ReentrantLock ReentrantLock实现了Lock接口, 支持两种获取锁的方式，一种是公平模型，一种是非公平模型。 公平锁模型： 初始化时， state=0，表示无人抢占了打水权。这时候，村民A来打水(A线程请求锁)，占了打水权，把state+1，如下所示： 线程A取得了锁，把 state原子性+1,这时候state被改为1，A线程继续执行其他任务，然后来了村民B也想打水（线程B请求锁），线程B无法获取锁，生成节点进行排队，如下图所示： 初始化的时候，会生成一个空的头节点，然后才是B线程节点，这时候，如果线程A又请求锁，是否需要排队？答案当然是否定的，否则就直接死锁了。当A再次请求锁，就相当于是打水期间，同一家人也来打水了，是有特权的，这时候的状态如下图所示： 到了这里，相信大家应该明白了什么是可重入锁了吧。就是一个线程在获取了锁之后，再次去获取了同一个锁，这时候仅仅是把状态值进行累加。如果线程A释放了一次锁，就成这样了： 仅仅是把状态值减了，只有线程A把此锁全部释放了，状态值减到0了，其他线程才有机会获取锁。当A把锁完全释放后，state恢复为0，然后会通知队列唤醒B线程节点，使B可以再次竞争锁。当然，如果B线程后面还有C线程，C线程继续休眠，除非B执行完了，通知了C线程。注意，当一个线程节点被唤醒然后取得了锁，对应节点会从队列中删除。 非公平锁模型 如果你已经明白了前面讲的公平锁模型，那么非公平锁模型也就非常容易理解了。当线程A执行完之后，要唤醒线程B是需要时间的，而且线程B醒来后还要再次竞争锁，所以如果在切换过程当中，来了一个线程C，那么线程C是有可能获取到锁的，如果C获取到了锁，B就只能继续乖乖休眠了。 偏向锁(jvm内部) 偏向锁（顾名思义，它会偏向于第一个访问锁的线程，如果在运行过程中，同步锁只有一个线程访问，不存在多线程争用的情况，则线程是不需要触发同步的，这种情况下，就会给线程加一个偏向锁） 大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。 轻量级锁(jvm) 轻量级锁是由偏向所升级来的，偏向锁运行在一个线程进入同步块的情况下，当第二个线程加入锁争用的时候，偏向锁就会升级为轻量级锁； 如果说轻量级锁是在无竞争的情况下使用CAS操作去消除同步使用的互斥量， 那偏向锁就是在无竞争的情况下把整个同步都消除掉， 连CAS操作都不做了。 自旋锁(jvm)自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗 jvm内部对锁的操作如果已经存在偏向锁了，则会尝试获取轻量级锁，如果以上两种都失败，则启用自旋锁，如果自旋也没有获取到锁，则使用重量级锁，没有获取到锁的线程阻塞挂起，直到持有锁的线程执行完同步块唤醒他们； 偏向锁是在无锁争用的情况下使用的，也就是同步开在当前线程没有执行完之前，没有其它线程会执行该同步快，一旦有了第二个线程的争用，偏向锁就会升级为轻量级锁，一点有两个以上线程争用，就会升级为重量级锁；如果线程争用激烈，那么应该禁用偏向锁。 synchronized实现，lock实现，有何区别 JUC Unsafe类是在sun.misc包下，不属于Java标准。但是很多Java的基础类库，包括一些被广泛使用的高性能开发库都是基于Unsafe类开发的，比如Netty、Cassandra、Hadoop、Kafka等。Unsafe类在提升Java运行效率，增强Java语言底层操作能力方面起了很大的作用。 Unsafe类使Java拥有了像C语言的指针一样操作内存空间的能力，同时也带来了指针的问题。过度的使用Unsafe类会使得出错的几率变大，因此Java官方并不建议使用的，官方文档也几乎没有。通常我们最好也不要使用Unsafe类，除非有明确的目的，并且也要对它有深入的了解才行。 J.U.C - AQSjava.util.concurrent（J.U.C）大大提高了并发性能，AQS 被认为是 J.U.C 的核心。 CountDownLatch 用来控制一个或者多个线程等待多个线程。 维护了一个计数器 cnt，每次调用 countDown() 方法会让计数器的值减 1，减到 0 的时候，那些因为调用 await() 方法而在等待的线程就会被唤醒。 1234567891011121314151617public class CountdownLatchExample &#123; public static void main(String[] args) throws InterruptedException &#123; final int totalThread = 10; CountDownLatch countDownLatch = new CountDownLatch(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print(\"run..\"); countDownLatch.countDown(); &#125;); &#125; countDownLatch.await();//当前线程需要等待Countdownlatch值为0才会苏醒继续执行 System.out.println(\"end\"); executorService.shutdown(); &#125;&#125; CyclicBarrier 用来控制多个线程互相等待，只有当多个线程都到达时，这些线程才会继续执行。 和 CountdownLatch 相似，都是通过维护计数器来实现的。线程执行 await() 方法之后计数器会减 1，并进行等待，直到计数器为 0，所有调用 await() 方法而在等待的线程才能继续执行。 CyclicBarrier 和 CountdownLatch 的一个区别是，CyclicBarrier 的计数器通过调用 reset() 方法可以循环使用，所以它才叫做循环屏障。 CyclicBarrier 有两个构造函数，其中 parties 指示计数器的初始值，barrierAction 在所有线程都到达屏障的时候会执行一次。 12345678910public CyclicBarrier(int parties, Runnable barrierAction) &#123; if (parties &lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction;&#125;public CyclicBarrier(int parties) &#123; this(parties, null);&#125; 1234567891011121314151617181920public class CyclicBarrierExample &#123; public static void main(String[] args) &#123; final int totalThread = 10; CyclicBarrier cyclicBarrier = new CyclicBarrier(totalThread); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalThread; i++) &#123; executorService.execute(() -&gt; &#123; System.out.print(\"before..\"); try &#123; cyclicBarrier.await(); &#125; catch (InterruptedException | BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.print(\"after..\"); &#125;); &#125; executorService.shutdown(); &#125;&#125; Semaphore Semaphore 类似于操作系统中的信号量，可以控制对互斥资源的访问线程数。 以下代码模拟了对某个服务的并发请求，每次只能有 3 个客户端同时访问，请求总数为 10。 1234567891011121314151617181920212223public class SemaphoreExample &#123; public static void main(String[] args) &#123; final int clientCount = 3; final int totalRequestCount = 10; Semaphore semaphore = new Semaphore(clientCount); ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; totalRequestCount; i++) &#123; executorService.execute(()-&gt;&#123; try &#123; semaphore.acquire(); System.out.print(semaphore.availablePermits() + \" \"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; semaphore.release(); &#125; &#125;); &#125; executorService.shutdown(); &#125;&#125;//2 1 2 2 2 2 2 1 2 2 基本类型原子类concurrent 包中提供了Java基本类型的原子操作封装类—–只要类名包含了Atomic关键字都是用于迸发的类型的封装 如: AtomicInteger, AtomicLong, AtomicIntegerArray, AtomicBoolean, AtomicReference, … 等等 以AtomicInteger为例 1234567891011121314/* * 其中getIntVolatile和compareAndSwapInt都是native方法 * getIntVolatile是获取当前的期望值 * compareAndSwapInt就是我们平时说的CAS(compare and swap)，通过比较如果内存区的值没有改变，那么就用新值直接给该内存区赋值 */public final int getAndAddInt(Object paramObject, long paramLong, int paramInt)&#123; int i; do &#123; i = getIntVolatile(paramObject, paramLong); &#125; while (!compareAndSwapInt(paramObject, paramLong, i, i + paramInt)); return i;&#125; incrementAndGet是将自增后的值返回，还有一个方法getAndIncrement是将自增前的值返回，分别对应++i和i++操作。同样的decrementAndGet和getAndDecrement则对--i和i--操作。 并发容器 哈希表非常高效，复杂度为O(1)的数据结构，在Java开发中，我们最常见到最频繁使用的就是HashMap和HashTable，但是在线程竞争激烈的并发场景中使用都不够合理,会导致线程安全问题 HashMap 关于位运算 关于位运算,左移一位就是×2倍的值, 位运算之高效，如下文在本来可以求模运算的时候，也换用位运算提高运算速度 为何要用2的幂次作为其容量 为了追求速度与效率，计算key的bucket进行hash计算的时候把取模运算转换为位运算，而当容量一定是2^n时： h &amp; (length - 1) 等价与 h % length，但他们是等价（效果）不等效（效率）的，其效果是计算h与length的模 负载因子默认常量： static final floatDEFAULT_LOAD_FACTOR=0.75f; hash运算 (为什么长度要是2的原因) 左边两组是数组长度为16（2的4次方），右边两组是数组长度为15。两组的hashcode均为8和9，但是很明显，当它们和1110“与”的时候，产生了相同的结果，也就是说它们会定位到数组中的同一个位置上去，这就产生了碰撞，8和9会被放到同一个链表上，那么查询的时候就需要遍历这个链表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hashcode的值会与14（1110）进行“与”，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！ hashmap什么时候进行扩容呢？ 当hashmap中的元素个数超过数组大小loadFactor时，就会进行数组扩容，loadFactor的默认值为0.75，也就是说，默认情况下，数组大小为16，那么当hashmap中元素个数超过160.75=12的时候，就把数组的大小扩展为2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知hashmap中元素的个数，那么预设元素的个数能够有效的提高hashmap的性能. HashMap是线程不安全 扩容时可能造成(因为采用了倒插法导致了死循环链表,所以线程不安全-&gt; 1.8采用了顺插法解决了这个问题) 结果覆盖问题(多线程环境下,在同一个桶内插入数据,会拿到头节点,若A对头节点修改值后,b不知道又修改则覆盖了原值) HashTableHashTable和HashMap的实现原理几乎一样，差别无非是1.HashTable不允许key和value为null；2.HashTable是线程安全的。但是HashTable线程安全的策略实现代价却太大了，简单粗暴，get/put所有相关操作都是synchronized的，这相当于给整个哈希表加了一把大锁，多线程访问时候，只要有一个线程访问或操作该对象，那其他线程只能阻塞，相当于将所有的操作串行化，在==竞争激烈的并发场景中性能就会非常差==。 HashTable性能差主要是由于所有操作需要竞争同一把锁，而如果容器中有多把锁，每一把锁锁一段数据，这样在多线程访问时不同段的数据时，就不会存在锁竞争了，这样便可以有效地提高并发效率。这就是ConcurrentHashMap所采用的”分段锁“思想。 put的主要逻辑也就两步： put方法是要加锁的, 若c超出阈值threshold，需要扩容并rehash。扩容后的容量是当前容量的2倍。这样可以最大程度避免之前散列好的entry重新散列，具体在另一篇文章中有详细分析，不赘述。扩容并rehash的这个过程是比较消耗资源的。 *1.定位segment并确保定位的Segment已初始化 * 2.调用Segment的put方法。 get方法 先定位Segment，再定位HashEntry 无需加锁，由于其中涉及到的共享变量都使用volatile修饰，volatile可以保证内存可见性，所以不会读取到过期数据。 Vector[1. 同步](https://cyc2018.github.io/CS-Notes/#/notes/Java 容器?id=_1-同步)它的实现与 ArrayList 类似，但是使用了 synchronized 进行同步。 12345678910111213public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125;Copy to clipboardErrorCopied [2. 扩容](https://cyc2018.github.io/CS-Notes/#/notes/Java 容器?id=_2-扩容-1)Vector 的构造函数可以传入 capacityIncrement 参数，它的作用是在扩容时使容量 capacity 增长 capacityIncrement。如果这个参数的值小于等于 0，扩容时每次都令 capacity 为原来的两倍。 12345678910111213141516171819public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125;Copy to clipboardErrorCopiedprivate void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125;Copy to clipboardErrorCopied 调用没有 capacityIncrement 的构造函数时，capacityIncrement 值被设置为 0，也就是说默认情况下 Vector 每次扩容时容量都会翻倍。 1234567public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125;Copy to clipboardErrorCopied [3. 与 ArrayList 的比较](https://cyc2018.github.io/CS-Notes/#/notes/Java 容器?id=_3-与-arraylist-的比较) Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍（也可以通过构造函数设置增长的容量），而 ArrayList 是 1.5 倍。 [4. 替代方案](https://cyc2018.github.io/CS-Notes/#/notes/Java 容器?id=_4-替代方案)可以使用 Collections.synchronizedList(); 得到一个线程安全的 ArrayList。 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list);Copy to clipboardErrorCopied 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); CopyOnWriteList读写分离写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响。 写操作需要加锁，防止并发写入时导致写入数据丢失。 写操作结束之后需要把原始数组指向新的复制数组。 123456789101112131415161718192021public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125;final void setArray(Object[] a) &#123; array = a;&#125;@SuppressWarnings(\"unchecked\")private E get(Object[] a, int index) &#123; return (E) a[index];&#125; [适用场景](https://cyc2018.github.io/CS-Notes/#/notes/Java 容器?id=适用场景)CopyOnWriteArrayList 在写操作的同时允许读操作，大大提高了读操作的性能，因此很适合读多写少的应用场景。 但是 CopyOnWriteArrayList 有其缺陷： 内存占用：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右； 数据不一致：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中。 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景。 JVMJava内存模型 Java 内存模型试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果。 主内存与工作内存 处理器上的寄存器的读写的速度比内存快几个数量级，为了解决这种速度矛盾，在它们之间加入了高速缓存。 加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。 所有的变量都存储在主内存中，每个线程还有自己的工作内存，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。 线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成 (共享内存) 内存间交互操作Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。 read：把一个变量的值从主内存传输到工作内存中 load：在 read 之后执行，把 read 得到的值放入工作内存的变量副本中 use：把工作内存中一个变量的值传递给执行引擎 assign：把一个从执行引擎接收到的值赋给工作内存的变量 store：把工作内存的一个变量的值传送到主内存中 write：在 store 之后执行，把 store 得到的值放入主内存的变量中 lock：作用于主内存的变量 unlock: 作用于主内存的变量 内存模型三大特性1. 原子性Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile(可见性,立即同步) 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。 有一个错误认识就是，int 等原子性的类型在多线程环境中不会出现线程安全问题。前面的线程不安全示例代码中，cnt 属于 int 类型变量，1000 个线程对它进行自增操作之后，得到的值为 997 而不是 1000。 为了方便讨论，将内存间的交互操作简化为 3 个：load、assign、store。 下图演示了两个线程同时对 cnt 进行操作，load、assign、store 这一系列操作整体上看不具备原子性，那么在 T1 修改 cnt 并且还没有将修改后的值写入主内存，T2 依然可以读入旧值。可以看出，这两个线程虽然执行了两次自增运算，但是主内存中 cnt 的值最后为 1 而不是 2。因此对 int 类型读写操作满足原子性只是说明 load、assign、store 这些单个操作具备原子性。 AtomicInteger 能保证多个线程修改的原子性。 使用 AtomicInteger 重写之前线程不安全的代码之后得到以下线程安全实现： 1234567891011public class AtomicExample &#123; private AtomicInteger cnt = new AtomicInteger(); public void add() &#123; cnt.incrementAndGet(); &#125; public int get() &#123; return cnt.get(); &#125;&#125; 除了使用原子类之外，也可以使用 synchronized 互斥锁来保证操作的原子性。它对应的内存间交互操作为：lock 和 unlock，在虚拟机实现上对应的字节码指令为 monitorenter 和 monitorexit。 12345678910public class AtomicSynchronizedExample &#123; private int cnt = 0; public synchronized void add() &#123; cnt++; &#125; public synchronized int get() &#123; return cnt; &#125;&#125; 2. 可见性可见性指当一个线程修改了共享变量的值，其它线程能够立即得知这个修改。Java 内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值来实现可见性的。 主要有三种实现可见性的方式： volatile synchronized，对一个变量执行 unlock 操作之前，必须把变量值同步回主内存。 final，被 final 关键字修饰的字段在构造器中一旦初始化完成，并且没有发生 this 逃逸（其它线程通过 this 引用访问到初始化了一半的对象），那么其它线程就能看见 final 字段的值。 对前面的线程不安全示例中的 cnt 变量使用 volatile 修饰，不能解决线程不安全问题，因为 volatile 并不能保证操作的原子性。 3. 有序性有序性是指：在本线程内观察，所有操作都是有序的。在一个线程观察另一个线程，所有操作都是无序的，无序是因为发生了指令重排序。在 Java 内存模型中，允许编译器和处理器对指令进行重排序，重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。 volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。 也可以通过 synchronized 来保证有序性，它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。 先行发生原则上面提到了可以用 volatile 和 synchronized 来保证有序性。除此之外，JVM 还规定了先行发生原则，让一个操作无需控制就能先于另一个操作完成。 1. 单一线程原则 Single Thread rule 在一个线程内，在程序前面的操作先行发生于后面的操作。 管程锁定规则 Monitor Lock Rule 一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。 3. volatile 变量规则 Volatile Variable Rule 对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 4. 线程启动规则 Thread Start Rule Thread 对象的 start() 方法调用先行发生于此线程的每一个动作。 5. 线程加入规则 Thread Join Rule Thread 对象的结束先行发生于 join() 方法返回。 6. 线程中断规则 Thread Interruption Rule 对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。 [7. 对象终结规则 Finalizer Rule 一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。 8. 传递性 Transitivity 如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。 内存分区内存分区: (总共分为: 程序计数器, 堆, 栈, 方法区, 本地方法区) 线程私有区域:程序计数器、Java虚拟机栈、本地方法栈 如果当前线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行 的是一个Native方法，这个计数器值为空。 线程共享区域:Java堆(存储静态文件, 变量) , 方法区、运行时常量池 当线程终止时，三者（虚拟机栈，本地方法栈和程序计数器）所占用的内存空间也会被释放掉, 因为它们是非共享区 内存溢出原因:系统不能再分配你所需的内存空间 一次性加载了过多的数据 对象不能被回收 代码bug导致死循环 解决方案: 使用内存分析工具jstat分析原因 调式是否是三方软件的漏洞 设置启动内存的大小 内存泄漏原因:内存中有一块空间无法在被使用,又不能被清理掉,累积量大了之后就会导致内存泄漏 代码漏洞导致常发性或者偶发性泄漏 对象不能被回收是根本原因 内存溢出和内存泄露的联系内存泄露会最终会导致内存溢出。相同点：都会导致应用程序运行出现问题，性能下降或挂起。不同点： 1) 内存泄露是导致内存溢出的原因之一，内存泄露积累起来将导致内存溢出。 2) 内存泄露可以通过完善代码来避免，内 GCGC算法和机制 机制: 标记清除, 引用计数, 停止-复制(程序暂停执行) 算法: 引用计数法, 根搜索算法, 标记-清除算法, 复制算法 Copying, 标记整理算法 GC算法java语言规范没有明确的说明JVM 使用哪种垃圾回收算法，但是任何一种垃圾回收算法一般要做两件基本事情： （1）发现无用的信息对象； （2）回收将无用对象占用的内存空间。使该空间可被程序再次使用。 1. 引用计数法堆中每个对象实例都有一个引用计数。任何引用计数器为0的对象实例可以被当作垃圾收集。当一个对象实例被垃圾收集时，它引用的任何对象实例的引用计数器减1。 优点:引用计数器可以很快的额执行, 交织在程序运行中,对程序需要不被长时间打断的实时环境比较有利 缺点:无法检测出循环引用。如父对象有一个对子对象的引用，子对象反过来引用父对象。这样，他们的引用计数永远不可能为0. 123456789101112public class Main &#123; public static void main(String[] args) &#123; MyObject object1 = new MyObject(); MyObject object2 = new MyObject(); object1.object = object2; object2.object = object1; object1 = null; object2 = null; &#125;&#125; 2. tracing算法(Tracing Collector) 或 标记-清除算法(mark and sweep) 从一个节点GC ROOT开始，寻找对应的引用节点，找到这个节点以后，继续寻找这个节点的引用节点，当所有的引用节点寻找完毕之后，剩余的节点则被认为是没有被引用到的节点，即无用的节点。 java中可作为GC Root的对象有 1.虚拟机栈中引用的对象（本地变量表） 2.方法区中静态属性引用的对象 3.方法区中常量引用的对象 4.本地方法栈中引用的对象（Native对象) 2.2.1 标记-清除算法标记-清除算法采用从根集合进行扫描，对存活的对象对象标记，标记完毕后，再扫描整个空间中未被标记的对象，进行回收，如图所示 2.2.2 compacting算法 或 标记-整理算法标记-整理算法采用标记-清除算法一样的方式进行对象的标记，但在清除时不同，在回收不存活的对象占用的空间后，会将所有的存活对象往左端空闲空间移动，并更新对应的指针,解决了内存碎片的问题. 2.2.3copying算法(Compacting Collector)基于copying算法的垃圾 收集就从根集中扫描活动对象，并将每个 活动对象复制到空闲面(使得活动对象所占的内存之间没有空闲洞)，这样空闲面变成了对象面，原来的对象面变成了空闲面，程序会在新的对象面中分配内存。一种典型的基于coping算法的垃圾回收是stop-and-copy算法，它将堆分成对象面和空闲区域面，在对象面与空闲区域面的切换过程中，==程序暂停执行==。 2.3.4 generation算法(Generational Collector)分代回收分代的垃圾回收策略，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的回收算法，以便提高回收效率。 Java中的对是jvm所管理的最大的一块内存空间, 主要用于粗放各种类的实例对象. 在Java中堆被分成两个不同的区域: 1. 新生代(young) 所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。 存在区域:Eden, From Survivor, To Survivor 回收时先将eden区存活对象复制到一个survivor0区，然后清空eden区，当这个survivor0区也存放满了时，则将eden区和survivor0区存活对象复制到另一个survivor1区，然后清空eden和这个survivor0区，此时survivor0区是空的，然后将survivor0区和survivor1区交换，即保持survivor1区为空， 如此往复。 当survivor1区不足以存放 eden和survivor0的存活对象时，就将存活对象直接存放到老年代。若是老年代也满了就会触发一次Full GC，也就是==新生代、老年代都进行回收== 2. 老年代(old) 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。 堆大小=新生代+老年代, 堆的大小可以通过参数 –Xms、-Xmx 来指定。 JVM 每次只会使用 Eden 和其中的一块 Survivor 区域来为对象服务，所以无论什么时候，总是有一块 Survivor 区域是空闲着的。因此，新生代实际可用的内存空间为 9/10 ( 即90% )的新生代空间。 持久代（Permanent Generation） 用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。 GC（垃圾收集器） Serial收集器（复制算法) 新生代单线程收集器，标记和清理都是单线程，优点是简单高效。 Serial Old收集器(标记-整理算法) 老年代单线程收集器，Serial收集器的老年代版本。 ParNew收集器(停止-复制算法) 新生代收集器，可以认为是Serial收集器的多线程版本,在多核CPU环境下有着比Serial更好的表现。 Parallel Scavenge收集器(停止-复制算法) 并行收集器，追求高吞吐量，高效利用CPU。吞吐量一般为99%， 吞吐量= 用户线程时间/(用户线程时间+GC线程时间)。适合后台应用等对交互相应要求不高的场景。 Parallel Old收集器(停止-复制算法) Parallel Scavenge收集器的老年代版本，并行收集器，吞吐量优先 CMS(Concurrent Mark Sweep)收集器（标记-清理算法） 高并发、低停顿，追求最短GC回收停顿时间，cpu占用比较高，响应时间快，停顿时间短，多核cpu 追求高响应时间的选择 GC的执行机制由于对象进行了分代处理，因此垃圾回收区域、时间也不一样。GC有两种类型：Scavenge GC和Full GC。 Scavenge GC一般情况下，当新对象生成，并且在Eden申请空间失败时，就会触发Scavenge GC，对Eden区域进行GC，清除非存活对象，并且把尚且存活的对象移动到Survivor区。然后整理Survivor的两个区。这种方式的GC是对年轻代的Eden区进行，不会影响到年老代。因为大部分对象都是从Eden区开始的，同时Eden区不会分配的很大，所以Eden区的GC会频繁进行。因而，一般在这里需要使用速度快、效率高的算法，使Eden去能尽快空闲出来。 Full GC对整个堆进行整理，包括Young、Tenured和Perm。Full GC因为需要对整个堆进行回收，所以比Scavenge GC要慢，因此应该尽可能减少Full GC的次数。在对JVM调优的过程中，很大一部分工作就是对于FullGC的调节。有如下原因可能导致Full GC： 1.年老代（Tenured）被写满 2.持久代（Perm）被写满 3.System.gc()被显示调用 4.上一次GC之后Heap的各域分配策略动态变化 Java有了GC同样会出现内存泄露问题 1.静态集合类像HashMap、Vector等的使用最容易出现内存泄露，这些静态变量的生命周期和应用程序一致，所有的对象Object也不能被释放，因为他们也将一直被Vector等应用着。 2.各种连接，数据库连接，网络连接，IO连接等没有显示调用close关闭，不被GC回收导致内存泄露。 3.监听器的使用，在释放对象的同时没有相应删除监听器的时候也可能导致内存泄露。 JVM中的安全点 在 JVM 中如何判断对象可以被回收 一文中，我们知道 HotSpot 虚拟机采取的是可达性分析算法。即通过 GC Roots 枚举判定待回收的对象。 那么，首先要找到哪些是 GC Roots。 有两种查找 GC Roots 的方法： 一种是遍历方法区和栈区查找（保守式 GC）。 一种是通过 OopMap 数据结构来记录 GC Roots 的位置（准确式 GC）。 很明显，保守式 GC 的成本太高。准确式 GC 的优点就是能够让虚拟机快速定位到 GC Roots。 对应 OopMap 的位置即可作为一个安全点（Safe Point）。 在执行 GC 操作时，所有的工作线程必须停顿，这就是所谓的”Stop-The-World”。 为什么呢？ 因为可达性分析算法必须是在一个确保一致性的内存快照中进行。如果在分析的过程中对象引用关系还在不断变化，分析结果的准确性就不能保证。 安全点意味着在这个点时，所有工作线程的状态是确定的，JVM 就可以安全地执行 GC 。 如何选定安全点安全点太多，GC 过于频繁，增大运行时负荷；安全点太少，GC 等待时间太长。 一般会在如下几个位置选择安全点： 循环的末尾 方法临返回前 调用方法之后 抛异常的位置 为什么选定这些位置作为安全点： 主要的目的就是避免程序长时间无法进入 Safe Point。比如 JVM 在做 GC 之前要等所有的应用线程进入安全点，如果有一个线程一直没有进入安全点，就会导致 GC 时 JVM 停顿时间延长。比如这里，超大的循环导致执行 GC 等待时间过长。 如何在 GC 发生时，所有线程都跑到最近的 Safe Point 上再停下来？主要有两种方式： 抢断式中断：在 GC 发生时，首先中断所有线程，如果发现线程未执行到 Safe Point，就恢复线程让其运行到 Safe Point 上。 主动式中断：在 GC 发生时，不直接操作线程中断，而是简单地设置一个标志，让各个线程执行时主动轮询这个标志，发现中断标志为真时就自己中断挂起。 JVM 采取的就是主动式中断。轮询标志的地方和安全点是重合的。 安全区域又是什么？Safe Point 是对正在执行的线程设定的, 如果一个线程处于 Sleep 或中断状态，它就不能响应 JVM 的中断请求，再运行到 Safe Point 上。因此 JVM 引入了 Safe Region。 Safe Region 是指在一段代码片段中，引用关系不会发生变化。在这个区域内的任意地方开始 GC 都是安全的。线程在进入 Safe Region 的时候先标记自己已进入了 Safe Region，等到被唤醒时准备离开 Safe Region 时，先检查能否离开，如果 GC 完成了，那么线程可以离开，否则它必须等待直到收到安全离开的信号为止。","categories":[{"name":"Java","slug":"Java","permalink":"/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"/tags/Java/"},{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"}]},{"title":"数据库","slug":"数据库","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T05:57:37.548Z","comments":true,"path":"2019/10/20/数据库/","link":"","permalink":"/2019/10/20/%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"","text":"数据完整性约束 数据完整性约束指的是为了防止不符合规范的数据进入数据库，在用户对数据进行插入、修改、删除等操作时，DBMS自动按照一定的约束条件对数据进行监测，使不符合规范的数据不能进入数据库，以确保数据库中存储的数据正确、有效、相容。 1) 实体完整性：规定表的每一行在表中是惟一的实体(主键唯一)。 2) 域完整性：是指表中的列必须满足某种特定的数据类型约束，其中约束又包括取值范围、精度等规定。 3) 参照完整性：是指两个表的主关键字和外关键字的数据应一致，保证了表之间的数据的一致性，防止了数据丢失或无意义的数据在数据库中扩散。 4) 用户定义的完整性：不同的关系数据库系统根据其应用环境的不同，往往还需要一些特殊的约束条件。用户定义的完整性即是针对某个特定关系数据库的约束条件，它反映某一具体应用必须满足的语义要求。 什么是sql? SQL：Structured Query Language 结构化查询语言 DQL：Data Query Language 数据查询语 ，如select * from 表名 DDL：Data Defintion Language 数据定义语言 create、alter、 drop DML：Data Manipulation Language 数据操纵语言 updata、instert、delete mysql数据库的隔离级别数据读取状态 脏读：事务A读取了事务B更新的数据，然后B执行回滚操作，那么A读到的就是脏数据。 不可重复读：事务A多次读取同一个数据，事务B在事务A多次读取的过程中，对数据做了更新并提交，导致事务A多次读取同一个数据时，结果不一致。 幻读：比如事务A将所有成绩从具体分数改为了ABCDE等级，但是事务B在这个时候插入了一条具体的分数，最后事务A结束后发现还有一条记录没改过来，好像发生了幻觉。抽象来说是事务A在操作的过程中插入了事务B的操作。 ==不可重复读侧重于修改，幻读侧重于新增或者删除。解决不可重复读只需要锁住满足条件的行，解决幻读需要锁表。== MySql的事务隔离级别就是来解决这个问题的 隔离级别 脏读 不可重复读 幻读 读未提交 是 是 是 读已提交 否 是 是 可重复读 否 否 是 串行化 否 否 否 读未提交，连脏读也避免不了，事务A可以读到事务B更新但还未提交的数据。 读已提交，可以解决脏读的问题，但是如果此时另一个事务修改了数据，就会造成不可重复读的问题。 读提交时写操作会锁住行 可重复读，解决了脏读和不可重复读的问题，用的比较多。 可重复读时，当检索条件有索引时，包括主键索引，默认加锁方式是next-key，如果没有索引，更新数据时会锁住整张表。、 串行化，解决了所有问题，但是是表锁，高并发下性能非常差，在实际开发中很少用到。一个间隙被事务加了锁，其他事务是不能在这个间隙插入记录的，这样可以防止幻读。串行化时读写数据都会锁住整张表 Mysql引擎InnoDB,MyinsM,MEMORY 在 MySQL 中有两个存储引擎 MyISAM 和 InnoDB，每个引擎都有利有弊。酷壳以前文章《MySQL: InnoDB 还是 MyISAM?》讨论和这个事情。 MyISAM 适合于一些需要大量查询的应用，但其对于有大量写操作并不是很好。甚至你只是需要update一个字段，整个表都会被锁起来，而别的进程，就算是读进程都无法操作直到读操作完成。另外，MyISAM 对于 SELECT COUNT(*) 这类的计算是超快无比的。 InnoDB 的趋势会是一个非常复杂的存储引擎，对于一些小的应用，它会比 MyISAM 还慢。他是它支持“行锁” ，于是在写操作比较多的时候，会更优秀。并且，他还支持更多的高级应用，比如：事务。 索引锁事务事务的特性 ACID 原子性（Atomicity) 事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部失败回滚。 回滚可以用回滚日志来实现，回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistency） 数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对一个数据的读取结果都是相同的。 隔离性（Isolation) 一个事务所做的修改在最终提交以前，对其它事务是不可见的。 持久性（Durability） 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 使用重做日志来保证持久性。 性能优化 数据库的优化方法有很多种，在应用层来说，主要是基于索引的优化。 1. 优化分析 必须熟悉数据库应用程序中的所有SQL语句，从中统计出常用的可能对性能有影响的部分SQL，分析、归纳出作为Where条件子句的字段及其组合方式；在这一基础上可以初步判断出哪些表的哪些字段应该建立索引。 必须了解哪些表是数据操作频繁的表；哪些表经常与其他表进行连接；哪些表中的数据量可能很大；对于数据量大的表，其中各个字段的数据分布情况如何； 2. 索引建立的原则 表的主键, 外键必须要有索引 数据量超过300的表应该有索引； 经常与其他表进行连接的表，在连接字段上应该建立索引； 经常出现在Where子句中的字段，特别是大表的字段，应该建立索引； 索引应该建在选择性高(区分度高)的字段上； 索引应该建在小字段上，对于大的文本字段甚至超长字段，不要建索引； 复合索引的建立需要进行仔细分析；尽量考虑用单字段索引代替： A、正确选择复合索引中的主列字段，一般是选择性较好的字段； B、复合索引的几个字段是否经常同时以AND方式出现在Where子句中？单字段查询是否极少甚至没有？如果是，则可以建复合索引；否则考虑单字段索引； C、如果复合索引中包含的字段经常单独出现在Where子句中，则分解为多个单字段索引； D、如果复合索引所包含的字段超过3个，那么仔细考虑其必要性，考虑减少复合的字段； ==维护成本较高== E、如果既有单字段索引，又有这几个字段上的复合索引，一般可以删除复合索引； 频繁进行数据操作的表，不要建立太多的索引； 删除无用的索引，避免对执行计划造成负面影响； 错误案例 任何对列的操作都可能导致全表扫描，这里所谓的操作包括数据库函数、计算表达式等等，查询时要尽可能将操作移至等式的右边，甚至去掉函数。 select * from record where substrb(CardNo,1,4)=’5378’(13秒) 优化: substrb函数对列操作, 不会使用索引 select * from record where substrb(CardNo,1,4)=’5378’(13秒) select * from record where amount/30&lt; 1000（11秒） *优化: * select * from record where amount &lt; 1000*30（&lt; 1秒） select * from record where to_char(ActionTime,’yyyymmdd’)=’19991201’（10秒） 优化: select * from record where ActionTime= to_date (‘19991201’ ,’yyyymmdd’)（&lt; 1秒） 3.避免索引失效 去掉 IN, OR 含有”IN”、”OR”的Where子句常会使用工作表，使索引失效；如果不产生大量重复值，可以考虑把子句拆开；拆开的子句中应该包含索引。 尽量去掉 “&lt;&gt;” 尽量去掉 “&lt;&gt;”，避免全表扫描，如果数据是枚举值，且取值范围固定，则修改为”OR”方式。 UPDATE SERVICEINFO SET STATE=0 WHERE STATE&lt;&gt;0; 以 上语句由于其中包含了”&lt;&gt;”，执行计划中用了全表扫描（TABLE ACCESSFULL），没有用到state字段上的索引 去掉 IS NULL和IS NOT NULL Where字句中的IS NULL和IS NOT NULL将不会使用索引而是进行全表搜索，因此需要通过改变查询方式，分情况讨论等方法，去掉Where子句中的IS NULL和IS NOT NULL。 like字句尽量前端匹配 因为like参数使用的非常频繁，因此如果能够对like子句使用索引，将很高的提高查询的效率。 select * from city where name like ‘%S%’ 以上查询的执行计划用了全表扫描（TABLE ACCESS FULL），如果能够修改为： select * from city where name like ‘S%’ 那 么查询的执行计划将会变成（INDEX RANGE SCAN），成功的利用了name字段的索引。 4.避免全表扫描 需要注意的是，尽量避免潜在的数据类型转换。如将字符型数据与数值型数据比较，ORACLE会自动将字符型用to_number()函数进行转换，从而导致全表扫描。 select col1,col2 from tab1 where col1&gt;10， 应该写为： select col1,col2 from tab1 where col1&gt;’10’。 5.为查询缓存优化sql 大多数的MySQL服务器都开启了查询缓存。这是提高性最有效的方法之一，而且这是被MySQL的数据库引擎处理的。 你所需要的就是用一个变量来代替MySQL的函数，从而开启缓存。 123456// 查询缓存不开启$r = mysql_query(\"SELECT username FROM user WHERE signup_date &gt;= CURDATE()\");// 开启查询缓存$today = date(\"Y-m-d\");$r = mysql_query(\"SELECT username FROM user WHERE signup_date &gt;= '$today'\"); 6. EXPLAIN 你的 SELECT 查询 EXPLAIN 关键字可以让你知道MySQL是如何处理你的SQL语句的。这可以帮你分析你的查询语句或是表结构的性能瓶颈。 如: EXPLAIN 的查询结果还会告诉你你的索引主键被如何利用的，你的数据表是如何被搜索和排序","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"mysql","slug":"mysql","permalink":"/tags/mysql/"},{"name":"数据库","slug":"数据库","permalink":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"}]},{"title":"Hibernate面试题","slug":"Hibernate","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T06:00:16.682Z","comments":true,"path":"2019/10/20/Hibernate/","link":"","permalink":"/2019/10/20/Hibernate/","excerpt":"","text":"1、Hibernate工作原理及为什么要用？ Hibernate工作原理及为什么要用？ 读取并解析配置文件 读取并解析映射信息，创建SessionFactory 打开Sesssion 创建事务Transation 持久化操作 提交事务 关闭Session 关闭SesstionFactory 使用Hibernate框架就不用我们写很多繁琐的SQL语句。Hibernate实现了ORM，能够将对象映射成数据库表，从而简化我们的开发！ 2、Hibernate是如何延迟加载(懒加载)?通过设置属性lazy进行设置是否需要懒加载 当Hibernate在查询数据的时候，数据并没有存在于内存中，当程序真正对数据的操作时，对象才存在与内存中，就实现了延迟加载，他节省了服务器的内存开销，从而提高了服务器的性能。 3、Hibernate中怎样实现类之间的关系?(如：一对多、多对多的关系)它们通过配置文件中的many-to-one、one-to-many、many-to-many来实现类之间的关联关系的。 4、hibernate的三种状态之间如何转换Hibernate中对象的状态： 临时/瞬时状态: 对象刚new出来，还没设id，设了其他值。 该对象还没有被持久化【没有保存在数据库中】 不受Session的管理 1User user=new User();//处于瞬时态 持久化状态 调用了save()、saveOrUpdate()，就变成Persistent，有id. 当保存在数据库中的对象就是持久化状态了 当调用session的save/saveOrUpdate/get/load/list等方法的时候，对象就是持久化状态 在数据库有对应的数据 受Session的管理 当对对象属性进行更改的时候，会反映到数据库中! 12session.save(idCard);idCard.setIdCardName(\"我是测试持久化对象\"); 游离状态. 当session close()完之后，变成Detached。数据库中有对应的记录 new出来的对象是瞬时状态-&gt;保存到数据库中(受Session管理)就是持久化状态-&gt;将session close掉就是游离状态 5、比较hibernate的三种检索策略优缺点 立即检索： 优点： 对应用程序完全透明，不管对象处于持久化状态，还是游离状态，应用程序都可以方便的从一个对象导航到与它关联的对象； 缺点： 1.select语句太多；2.可能会加载应用程序不需要访问的对象白白浪费许多内存空间； 立即检索:lazy=false； 延迟检索： 优点： 由应用程序决定需要加载哪些对象，可以避免可执行多余的select语句，以及避免加载应用程序不需要访问的对象。因此能提高检索性能，并且能节省内存空间； 缺点： 应用程序如果希望访问游离状态代理类实例，必须保证他在持久化状态时已经被初始化； 延迟加载：lazy=true； 迫切左外连接检索： 优点： 1对应用程序完全透明，不管对象处于持久化状态，还是游离状态，应用程序都可以方便地从一个对象导航到与它关联的对象。2使用了外连接，select语句数目少； 缺点： 1 可能会加载应用程序不需要访问的对象，白白浪费许多内存空间；2复杂的数据库表连接也会影响检索性能； 预先抓取： fetch=“join”； 6、hibernate都支持哪些缓存策略 hibernate都支持哪些缓存策略 usage的属性有4种： 放入二级缓存的对象，只读(Read-only); 非严格的读写(Nonstrict read/write) 读写； 放入二级缓存的对象可以读、写(Read/write)； 基于事务的策略(Transactional) 7、hibernate里面的sorted collection 和ordered collection有什么区别sorted collection 是在内存中通过Java比较器进行排序的 ordered collection 是在数据库中通过order by进行排序的 对于比较大的数据集，为了避免在内存中对它们进行排序而出现 Java中的OutOfMemoryError，最好使用ordered collection。 8、说下Hibernate的缓存机制 一级缓存： Hibenate中一级缓存，也叫做session的缓存，它可以在session范围内减少数据库的访问次数！ 只在session范围有效！ Session关闭，一级缓存失效！ 只要是持久化对象状态的，都受Session管理，也就是说，都会在Session缓存中！ Session的缓存由hibernate维护，用户不能操作缓存内容； 如果想操作缓存内容，必须通过hibernate提供的evit/clear方法操作。 二级缓存： 二级缓存是基于应用程序的缓存，所有的Session都可以使用 Hibernate提供的二级缓存有默认的实现，且是一种可插配的缓存框架！如果用户想用二级缓存，只需要在hibernate.cfg.xml中配置即可； 不想用，直接移除，不影响代码。 如果用户觉得hibernate提供的框架框架不好用，自己可以换其他的缓存框架或自己实现缓存框架都可以。 Hibernate二级缓存：存储的是常用的类 9、Hibernate的查询方式有几种 对象导航查询(objectcomposition) HQL查询 1、 属性查询 2、 参数查询、命名参数查询 3、 关联查询 4、 分页查询 5、 统计函数 Criteria 查询 SQLQuery本地SQL查询 10、如何优化Hibernate？ 数据库设计调整 HQL优化 API的正确使用(如根据不同的业务类型选用不同的集合及查询API) 主配置参数(日志，查询缓存，fetch_size, batch_size等) 映射文件优化(ID生成策略，二级缓存，延迟加载，关联优化) 一级缓存的管理 针对二级缓存，还有许多特有的策略 详情可参考资料： https://www.cnblogs.com/xhj123/p/6106088.html 11、谈谈Hibernate中inverse的作用inverse属性默认是false,就是说关系的两端都来维护关系。 比如Student和Teacher是多对多关系，用一个中间表TeacherStudent维护。Gp) 如果Student这边inverse=”true”, 那么关系由另一端Teacher维护，就是说当插入Student时，不会操作TeacherStudent表（中间表）。只有Teacher插入或删除时才会触发对中间表的操作。所以两边都inverse=”true”是不对的，会导致任何操作都不触发对中间表的影响；当两边都inverse=”false”或默认时，会导致在中间表中插入两次关系。 如果表之间的关联关系是“一对多”的话，那么inverse只能在“一”的一方来配置！ 详情可参考： https://zhongfucheng.bitcron.com/post/hibernate/hibernate-inversehe-cascadeshu-xing-zhi-shi-yao-dian 12、JDBC，hibernate 和 ibatis 的区别jdbc:手动 手动写sql delete、insert、update要将对象的值一个一个取出传到sql中,不能直接传入一个对象。 select:返回的是一个resultset，要从ResultSet中一行一行、一个字段一个字段的取出，然后封装到一个对象中，不直接返回一个对象。 ibatis的特点:半自动化 sql要手动写 delete、insert、update:直接传入一个对象 select:直接返回一个对象 hibernate:全自动 不写sql,自动封装 delete、insert、update:直接传入一个对象 select:直接返回一个对象 13、在数据库中条件查询速度很慢的时候,如何优化? 建索引 减少表之间的关联 优化sql，尽量让sql很快定位数据，不要让sql做全表查询，应该走索引,把数据量大的表排在前面 简化查询字段，没用的字段不要，已经对返回结果的控制，尽量返回少量数据 详情可参考： https://mp.weixin.qq.com/s?timestamp=1520300404&amp;src=3&amp;ver=1&amp;signature=W6Fo7aDHiJtK4ecUcnSJ4h9bN0vRAcTPKBTgLWSJDsMcdQReJC487RYzUIU9UFYQdmgLFyss9cKifM*GFp*CEVLtaLlwjj2HaDOjsCRkTnwfVlUY5cDhSyRi-c8leheofZJVnu6wYQ3IvT*hYyVB1pQCqqnuXIWERaksjXuyNP8= 14、什么是SessionFactory,是线程安全么?SessionFactory 是Hibrenate单例数据存储和线程安全的，以至于可以多线程同时访问。一个SessionFactory 在启动的时候只能建立一次。SessionFactory应该包装各种单例以至于它能很简单的在一个应用代码中储存. 15、get和load区别get()立即查询 load()懒加载 get如果没有找到会返回null， load如果没有找到会抛出异常。 get会先查一级缓存， 再查二级缓存，然后查数据库；load会先查一级缓存，如果没有找到，就创建代理对象， ==等需要的时候去查询二级缓存和数据库==。 16、merge的含义： merge的含义：(返回最新的实例数据) 如果session中存在相同持久化标识(identifier)的实例，用用户给出的对象的状态覆盖旧有的持久实例 如果session没有相应的持久实例，则尝试从数据库中加载，或创建新的持久化实例,最后返回该持久实例 用户给出的这个对象没有被关联到session上，它依旧是脱管的 详情可参考： http://cp3.iteye.com/blog/786019 17、persist和save的区别 persist不保证立即执行，可能要等到flush； persist不更新缓存； save, 把一个瞬态的实例持久化标识符，及时的产生,它要返回标识符，所以它会立即执行Sql insert 使用 save() 方法保存持久化对象时，该方法返回该持久化对象的标识属性值(即对应记录的主键值)； 使用 persist() 方法来保存持久化对象时，该方法没有任何返回值。 参考资料： http://blog.csdn.net/u010739551/article/details/47253881 18、主键生成 策略有哪些主键的自动生成策略 identity 自增长(mysql,db2) sequence 自增长(序列)， oracle中自增长是以序列方法实现** native 自增长【==会根据底层数据库自增长的方式==选择identity或sequence】 如果是mysql数据库, 采用的自增长方式是identity 如果是oracle数据库， 使用sequence序列的方式实现自增长 increment 自增长(会有并发访问的问题，一般在服务器集群环境使用会存在问题。) 指定主键生成策略为手动指定主键的值 assigned 指定主键生成策略为UUID生成的值 uuid foreign(外键的方式) 19、简述hibernate中getCurrentSession和openSession区别 getCurrentSession会绑定当前线程，而openSession不会，因为我们把hibernate交给我们的spring来管理之后，我们是有事务配置，这个有事务的线程就会绑定当前的工厂里面的每一个session，而openSession是创建一个新session。 getCurrentSession事务是有spring来控制的，而openSession需要我们手动开启和手动提交事务， getCurrentSession是不需要我们手动关闭的，因为工厂会自己管理，而openSession需要我们手动关闭。 而getCurrentSession需要我们手动设置绑定事务的机制，有三种设置方式， jdbc本地的Thread JTA 第三种是spring提供的事务管理机制， org.springframework.orm.hibernate4.SpringSessionContext，而且srping默认使用该种事务管理机制 20、Hibernate中的命名SQL查询指的是什么? 命名查询指的是用&lt;sql-query&gt;标签在映射文档中定义的SQL查询，可以通过使用Session.getNamedQuery()方法对它进行调用。命名查询使你可以使用你所指定的一个名字拿到某个特定的查询。 Hibernate中的命名查询可以使用注解来定义，也可以使用我前面提到的xml映射文件来定义。在Hibernate中，@NameQuery用来定义单个的命名查询，@NameQueries用来定义多个命名查询。 21、为什么在Hibernate的实体类中要提供一个无参数的构造器这一点非常重要？每个==Hibernate实体类必须包含一个 无参数的构造器==, 这是因为Hibernate框架要使用Reflection API，通过调用Class.newInstance()来创建这些实体类的实例。如果在实体类中找不到无参数的构造器，这个方法就会抛出一个InstantiationException异常。 22、可不可以将Hibernate的实体类定义为final类?你可以将Hibernate的实体类定义为final类，但这种做法并不好。因为Hibernate会使用代理模式在延迟关联的情况下提高性能，如果你把实体类定义成final类之后，因为 Java不允许对final类进行扩展，所以Hibernate就无法再使用代理了， 如此一来就限制了使用可以提升性能的手段。==不能代理就不能使用延迟加载了== 最后参考资料： http://blog.csdn.net/qq1137623160/article/details/71194677 http://blog.csdn.net/u013842976/article/details/52518218 http://blog.csdn.net/yubotianxiao/article/details/52238200","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"数据库","slug":"数据库","permalink":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Hibernate","slug":"Hibernate","permalink":"/tags/Hibernate/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"}]},{"title":"Kafka面试","slug":"Kafka面试","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T08:52:31.449Z","comments":true,"path":"2019/10/20/Kafka面试/","link":"","permalink":"/2019/10/20/Kafka%E9%9D%A2%E8%AF%95/","excerpt":"","text":"Kafka基本理论知识请说明什么是Apache Kafka?Apache Kafka是由Apache开发的一种发布订阅消息系统，它是一个分布式的、分区的和重复的日志服务。 请说明什么是传统的消息传递方法?传统的消息传递方法包括两种： 排队：在队列中，一组用户可以从服务器中读取消息，每条消息都发送给其中一个人。 发布-订阅：在这个模型中，消息被广播给所有的用户。 Kafka 与传统消息系统之间有三个关键区别 Kafka 持久化日志，这些日志可以被重复读取和无限期保留 Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性 Kafka 支持实时的流式处理 列举kafka的应用场景 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如Hadoop、Hbase、Solr等 消息系统：解耦和生产者和消费者、缓存消息等 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到Hadoop、数据仓库中做离线分析和挖掘 运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告 流式处理：比如spark streaming和storm 事件源 ISR问题 Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么 ISR: In-Sync Replicas 副本同步队列 AR: Assigned Replicas 所有副本 ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。 如果副本在ISR中停留了很长时间表明什么?如果一个副本在ISR中保留了很长一段时间，那么它就表明，跟踪器无法像在leader收集数据那样快速地获取数据。 请说明如果首选的副本不在ISR中会发生什么?如果首选的副本不在ISR中，控制器将无法将leadership转移到首选的副本。 解释如何减少ISR中的扰动? broker什么时候离开ISR?ISR是一组与leaders完全同步的消息副本，也就是说ISR中包含了所有提交的消息。ISR应该总是包含所有的副本，直到出现真正的故障。如果一个副本从leader中脱离出来，将会从ISR中删除。 ISR集合是什么？谁维护着？如何维护？ISR（In-Sync Replica）集合表示的是目前可用并且消息量与leader相差不多的副本集合，这是整个副本集合的一个子集, ISR集合的副本必须满足： 副本所在节点必须维持着与zookeeper的连接； 副本最后一条消息的offset与leader副本最后一条消息的offset之间的差值不能超出指定的阈值 每个分区的leader副本都会维护此分区的ISR集合，写请求首先由leader副本处理，之后follower副本会从leader副本上拉取写入的消息，这个过程会有一定的延迟，导致follower副本中保存的消息略少于leader副本，只要未超出阈值都是可以容忍的 请说明Kafka相对传统技术有什么优势? 快速：单一的Kafka代理可以处理成千上万的客户端，每秒处理数兆字节的读写操作。 可伸缩：在一组机器上对数据进行分区和简化，以支持更大的数据 持久(容错性高)：消息是持久性的，并在集群中进行复制，以防止数据丢失。 设计：它提供了容错保证和持久性 在Kafka中broker的意义是什么?接收Producer发过来的数据，并且将它持久化，同时提供给Consumer去订阅组成Kafka集群节点，之间没有主从关系，依赖ZooKeeper来协调，broker负责消息的读取和存储，一个broker可以管理多个partition 什么是broker？作用是什么? 一个单独的kafka server就是一个broker，broker主要工作就是接收生产者发过来的消息，分配offset，之后保存到磁盘中。同时，接收消费者、其他broker的请求，根据请求类型进行相应的处理并返回响应，在一般的生产环境中，一个broker独占一台物理服务器 Kafka服务器能接收到的最大信息是多少?Kafka服务器可以接收到的消息的最大大小是1000000字节。1MB=1000kb 解释Kafka的Zookeeper是什么?我们可以在没有Zookeeper的情况下使用Kafka吗?Zookeeper是一个开放源码的、高性能的协调服务，它用于Kafka的分布式应用。作用：协调Kafka Broker，存储原数据：==consumer的offset+broker信息+topic信息+partition个数信息==。不，不可能越过Zookeeper，直接联系Kafka broker。一旦Zookeeper停止工作，它就不能服务客户端请求。 Zookeeper主要用于在集群中不同节点之间进行通信 在Kafka中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都可以从之前提交的偏移量中获取 除此之外，它还执行其他活动，如: ==leader检测、分布式同步、配置管理、识别新节点何时离开或连接、集群、节点实时状态等等。== 解释Kafka的用户如何消费信息?在Kafka中传递消息是通过使用sendfile【零拷贝】 API完成的。它支持将字节从套接口转移到磁盘，通过内核空间保存副本，并在内核用户之间调用内核。零拷贝：用户向内核去发送一个命令，我要操作那些数据，然后直接从磁盘转成Socket Buffer，再从Socket Buffer到网卡Buffer，再传出去【少了两次的copy】 解释如何提高远程用户的吞吐量?如果用户位于与broker不同的数据中心，则可能需要调优套接口缓冲区大小，以对长网络延迟进行摊销。 解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息?在数据中，为了精确地获得Kafka的消息，你必须遵循两件事: 在数据消耗期间避免重复 在数据生产过程中避免重复 这里有两种方法，可以在数据生成时准确地获得一个语义 每个分区使用一个单独的写入器，每当你发现一个网络错误，检查该分区中的最后一条消息，以查看您的最后一次写入是否成功 在消息中包含一个主键(UUID或其他)，并在用户中进行反复制 Kafka为什么需要复制?==Kafka的数据复制确保了任何已发布的消息不会丢失，并且可以在机器错误、程序错误或更常见些的软件升级中使用。== 有可能在生产后发生消息偏移吗?在大多数队列系统中，作为生产者的类无法做到这一点，它的作用是触发并忘记消息。 broker将完成剩下的工作，比如使用id进行适当的元数据处理、偏移量等。作为消息的用户，你可以从Kafka broker中获得补偿。如果你注视SimpleConsumer类，你会注意到它会获取包括偏移量作为列表的MultiFetchResponse对象。此外，当你对Kafka消息进行迭代时，你会拥有包括偏移量和消息发送的MessageAndOffset对象。 kafka主要特征 kafka具有近乎实时性的消息处理能力，面对海量数据，高效的存储消息和查询消息。kafka将消息保存在磁盘中，以顺序读写的方式访问磁盘，从而避免了随机读写磁盘导致的性能瓶颈 kafka支持批量读写消息，并且对消息批量压缩，提高了网络利用率和压缩效率 kafka支持消息分区，每个分区中的消息保证顺序传输，而分区之间可以并发操作，提高了kafka的并发能力 kafka支持在线增加分区，支持在线水平扩展 kafka支持为每个分区创建多个副本，其中只会有一个leader副本负责读写，其他副本只负责与leader副本同步，这种方式提高了数据的容灾能力，kafka会将leader副本均匀的分布在集群中的服务器上，实现性能最大化 kafka主题分区的作用?kafka的每个topic都可以分为多个partition，每个partition都有多个replica（副本），每个分区中的消息是不同的，提高了并发读写的能力，而同一分区的不同副本中保存的是相同的消息，副本之间是一主多从关系，其中leader副本处理读写请求，follower副本只与leader副本进行消息同步，当leader副本出现故障时，则从follower副本中重新选举leader副本对外提供服务。这样，通过提高分区的数量，就可以实现水平扩展，通过提高副本数量，就可以提高容灾能力 consumer水平扩展如何实现?kafka支持consumer水平扩展，可以让多个consumer加入一个consumer group，在一个consumer group中，每个分区只能分配给一个consumer，当kafka服务端增加分区数量进行水平扩展后，可以向consumer group中增加新的consumer来提高整个consumer group的消费能力，当consumer group 中的一个consumer出现故障下线时，会通过rebalance操作下线的consumer，它负责处理的分区将分配给其他consumer 消息的顺序kafka保证一个partition内消息是有序的，但是并不保证多个partition之间的数据有顺序，每个topic可以划分成多个分区，同一个topic下的不同分区包含的消息是不同的，每个消息在被添加到分区时，都会被分配一个offset，它是此消息在分区中的唯一编号，kafka通过offset保证消息在分区内的顺序，offset顺序不跨分区，即kafka只保证在同一个分区内的消息是有序的 为了避免磁盘被占满，kafka会周期性的删除陈旧的消息，删除策略是什么? 一种是根据消息保留的时间 一种是根据topic存储的数据大小 什么是日志压缩?在很多场景中，消息的key与value之间的对应关系是不断变化的，消费者只关心key对应的最新value，此时，可以开启kafka的日志压缩功能，kafka会在后台启动一个线程，定期将相同key的消息进行合并，只保留最新的value值 同一分区的多个副本包括的消息是否一致？每个副本中包含的消息是一样的，但是再同一时刻，副本之间并不是完全一样的 Kafka的设计时什么样的呢？ Kafka将消息以topic为单位进行归纳 将向Kafka topic发布消息的程序成为producer 将预订topics并消费消息的程序成为consumer Kafka以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个broker producers通过网络将消息发送到Kafka集群，集群向消费者提供消息 数据传输的事物定义有哪三种？数据传输的事务定义通常有以下三种级别：（1）最多一次：消息不会被重复发送，最多被传输一次，但也有可能一次不传输（2）最少一次：消息不会被漏发送，最少被传输一次，但也有可能被重复传输.（3）精确的一次（Exactly once）：不会漏传输也不会重复传输,每个消息都传输被一次而且仅仅被传输一次，这是大家所期望的 Kafka判断一个节点是否还活着有那两个条件？心跳机制,延时限制 节点必须可以维护和ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接 如果节点是个follower,他必须能及时的同步leader的写操作，延时不能太久 producer是否直接将数据发送到broker的leader(主节点)？可以producer直接将数据发送到broker的leader(主节点)，不需要在多个节点进行分发，为了帮助producer做到这点，所有的Kafka节点都可以及时的告知：哪些节点是活动的，目标topic目标分区的leader在哪。这样producer就可以直接将消息发送到目的地了 Kafka consumer是否可以消费指定分区消息？Kafka consumer消费消息时，向broker发出”fetch”请求去消费特定分区的消息，consumer指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer拥有了offset的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的 Kafka消息是采用Pull模式，还是Push模式？kafka使用了pull模式, 根据订阅者的消费能力进行拉取数据,Kafka最初考虑的问题是，customer应该从brokers拉取消息还是brokers将消息推送到consumer，也就是pull还push。在这方面，Kafka遵循了一种大部分消息系统共同的传统的设计：producer将消息推送到broker，consumer从broker拉取消息一些消息系统比如Scribe和Apache Flume采用了push模式，将消息推送到下游的consumer。这样做有好处也有坏处：由broker决定消息推送的速率，对于不同消费速率的consumer就不太好处理了。消息系统都致力于让consumer以最大的速率最快速的消费消息，但不幸的是，push模式下，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了。最终Kafka还是选取了传统的pull模式Pull模式的另外一个好处是consumer可以自主决定是否批量的从broker拉取数据。Push模式必须在不知道下游consumer消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免consumer崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。Pull模式下，consumer就可以根据自己的消费能力去决定这些策略Pull有个缺点是，如果broker没有可供消费的消息，将导致consumer不断在循环中轮询，直到新消息到t达。为了避免这点，Kafka有个参数可以让consumer阻塞知道新消息到达(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发 Kafka存储在硬盘上的消息格式是什么？==版本号(1个字节)+校验码(4个字节)+消息(n个字节)== 消息由一个固定长度的头部和可变长度的字节数组组成。头部包含了一个版本号和CRC32校验码。 1234消息长度: 4 bytes (value: 1+4+n)版本号: 1 byteCRC32校验码: 4 bytes具体的消息: n bytes Kafka高效文件存储设计特点： Kafka把topic中一个partition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。 通过索引信息可以快速定位message和确定response的最大大小。 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 Kafka创建Topic时如何将分区放置到不同的Broker中 副本因子不能大于 Broker 的个数； 第一个分区（编号为0）的第一个副本放置位置是随机从 brokerList 选择的；其他分区的第一个副本放置位置相对于第0个分区依次往后移。也就是如果我们有5个 Broker，5个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个 Broker 上，依次类推； 剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的Kafka新建的分区会在哪个目录下创建在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘上用于提高读写性能。当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个目录下创建文件夹用于存放数据。但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic名+分区ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。 partition的数据如何保存到硬盘? topic中的多个partition以文件夹的形式保存到broker，每个分区序号从0递增，且消息有序 Partition文件下有多个segment（xxx.index，xxx.log）,segment 文件里的 大小和配置文件大小一致可以根据要求修改 默认为1g, 如果大小大于1g时，会滚动一个新的segment并且以上一个segment最后一条消息的偏移量命名 kafka的ack机制生产阶段可能会丢数据, 但是发不出去之后就不会丢==生产阶段可能会丢数据, 但是发不出去之后就不会丢==(会备份) request.required.acks有三个值 0、1、-1： 0 (生产者只负责生产不管其他的)：生产者不会等待broker的ack，这个延迟最低但是存储的保证最弱当server挂掉的时候就会丢数据 1 (生产者等待leader的副本之, 还是可能丢数据)：服务端会等待ack值 leader副本确认接收到消息后发送ack但是如果leader挂掉后他不确保是否复制完成新leader也会导致数据丢失 -1(所有人都响应了,才继续生产)：同样在1的基础上 服务端会等所有的follower的副本受到数据后才会受到leader发出的ack，这样数据不会丢失 Kafka的消费者如何消费数据==记录偏移量, 下次接着消费== 消费者每次消费数据的时候，消费者都会记录消费的物理偏移量（offset）的位置, 等到下次消费时，他会接着上次位置继续消费 消费者负载均衡策略一个消费者组中的一个分片对应一个消费者成员，他能保证每个消费者成员都能访问，如果组中成员太多会有空闲的成员 数据有序 一个消费者组里它的内部是有序的 消费者组与消费者组之间是无序的 kafka生产数据时数据的分组策略 生产者决定数据产生到集群的哪个partition中 每一条消息都是以（key，value）格式, Key是由生产者发送数据传入, 所以生产者（key）决定了数据产生到集群的哪个partition 面试题1. Kafka中的HW、LEO、LSO、LW等分别代表什么？ HW:High Watermark 高水位，取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置上一条信息。 LEO:LogEndOffset 当前日志文件中下一条待写信息的offset HW/LEO这两个都是指最后一条的下一条的位置而不是指最后一条的位置。 LSO:Last Stable Offset 对未完成的事务而言，LSO 的值等于事务中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同 HW 相同 LW:Low Watermark 低水位, 代表 AR 集合中最小的 logStartOffset 值 2. Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？拦截器-&gt;序列化器-&gt;分区器 3. Kafka生产者客户端中使用了几个线程来处理？分别是什么？2个，主线程和Sender线程。主线程负责创建消息，然后通过分区器、序列化器、拦截器作用之后缓存到累加器RecordAccumulator中。Sender线程负责将RecordAccumulator中消息发送到kafka中. 4. “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？不正确，通过自定义分区分配策略，可以将一个consumer指定消费所有partition。 5. 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?offset+1 6. 有哪些情形会造成重复消费？消费者消费后没有commit offset(程序崩溃/强行kill/消费耗时/自动提交偏移情况下unscrible) 7. 那些情景下会造成消息漏消费？==消费者没有处理完消息 提交offset(自动提交偏移 未处理情况下程序异常结束)== 8. Kafka Consumer是非线程安全的，那么怎么样实现多线程消费？1.在每个线程中新建一个Kafka Consumer 2.单线程创建Kafka Consumer，多个处理线程处理消息（难点在于是否要考虑消息顺序性，offset的提交方式） 9. 当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？创建:在zk上/brokers/topics/下节点 kafkabroker会监听节点变化创建主题删除:调用脚本删除topic会在zk上将topic设置待删除标志，kafka后台有定时的线程会扫描所有需要删除的topic进行删除 10. 创建topic时如何选择合适的分区数？根据集群的机器数量和需要的吞吐量来决定适合的分区数 11. Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？__consumer_offsets 以下划线开头，保存消费组的偏移 12. 优先副本是什么？它有什么特殊的作用？优先副本 会是默认的leader副本 发生leader变化时重选举会优先选择优先副本作为leader 13. Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理创建主题时如果不手动指定分配方式 有两种分配方式 消费组内分配 14. 简述Kafka的日志目录结构每个partition一个文件夹，包含四类文件.index .log .timeindex leader-epoch-checkpoint.index .log .timeindex 三个文件成对出现 前缀为上一个segment的最后一个消息的偏移 log文件中保存了所有的消息 index文件中保存了稀疏的相对偏移的索引 timeindex保存的则是时间索引leader-epoch-checkpoint中保存了每一任leader开始写入消息时的offset 会定时更新follower被选为leader时会根据这个确定哪些消息可用 15. Kafka中有那些索引文件？如上 16. 如果我指定了一个offset，Kafka怎么查找到对应的消息？1.通过文件名前缀数字x找到该绝对offset 对应消息所在文件 2.offset-x为在文件中的相对偏移 3.通过index文件中记录的索引找到最近的消息的位置 4.从最近位置开始逐条寻找 17. 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？原理同上 但是时间的因为消息体中不带有时间戳 所以不精确 18. 聊一聊你对Kafka的Log Retention的理解kafka留存策略包括 删除和压缩两种 删除: 根据时间和大小两个方式进行删除 大小是整个partition日志文件的大小, 超过的会从老到新依次删除 时间指日志文件中的最大时间戳而非文件的最后修改时间 压缩: 相同key的value只保存一个 压缩过的是clean 未压缩的dirty 压缩之后的偏移量不连续 未压缩时连续 19. 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）20. 消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）21. Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ….”）22. Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？23. 失效副本是指什么？有那些应对措施？24. 多副本下，各个副本中的HW和LEO的演变过程25. 为什么Kafka不支持读写分离？26. Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）27. Kafka中怎么实现死信队列和重试队列？28. Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）29. Kafka有哪些指标需要着重关注？生产者关注MessagesInPerSec、BytesOutPerSec、BytesInPerSec 消费者关注消费延迟Lag 30. 怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)参考 如何监控kafka消费Lag情况 31. Kafka的那些设计让它有如此高的性能？零拷贝，页缓存，顺序写 32. Kafka有什么优缺点？33. 为什么选择Kafka?吞吐量高，大数据消息系统唯一选择。 34. 怎么样才能确保Kafka极大程度上的可靠性？","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"},{"name":"Kafka","slug":"Kafka","permalink":"/tags/Kafka/"}],"author":"蜗牛君"},{"title":"Java基础核心","slug":"Java基础","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T14:00:53.540Z","comments":true,"path":"2019/10/20/Java基础/","link":"","permalink":"/2019/10/20/Java%E5%9F%BA%E7%A1%80/","excerpt":"","text":"数据类型基本类型 byte/8 char/16 short/16 int/32 float/32 long/64 double/64 boolean/~ 包装类型 基本类型都有对应的包装类型，基本类型与其对应的包装类型之间的赋值使用自动装箱与拆箱完成。 12Integer x = 2; // 装箱 调用了 Integer.valueOf(2)int y = x; // 拆箱 调用了 X.intValue() 缓存池(!!!重要)new Integer(123) 与 Integer.valueOf(123) 的区别在于： new Integer(123) 每次都会新建一个对象； Integer.valueOf(123) 会使用缓存池中的对象，多次调用会取得同一个对象的引用。 123456Integer x = new Integer(123);Integer y = new Integer(123);System.out.println(x == y); // falseInteger z = Integer.valueOf(123);Integer k = Integer.valueOf(123);System.out.println(z == k); // trueCopy to clipboardErrorCopied valueOf() 方法的实现比较简单，就是先判断值是否在缓存池中，如果在的话就直接返回缓存池的内容。 在 Java 8 中，Integer 缓存池的大小默认为 -128~127。 编译器会在自动装箱过程调用 valueOf() 方法，因此多个值相同且值在缓存池范围内的 Integer 实例使用自动装箱来创建，那么就会引用相同的对象。 基本类型对应的缓冲池如下: boolean values true and false all byte values (byte取值范围内的全部值 -128~127) short values between -128 and 127 int values between -128 and 127 char in the range \\u0000 to \\u007F 在使用这些基本类型对应的包装类型时，如果该数值范围在缓冲池范围内，就可以直接使用缓冲池中的对象。 在 jdk 1.8 所有的数值类缓冲池中，Integer 的缓冲池 IntegerCache 很特殊，这个缓冲池的下界是 - 128，上界默认是 127，但是这个上界是可调的，在启动 jvm 的时候，通过 -XX:AutoBoxCacheMax= 来指定这个缓冲池的大小，该选项在 JVM 初始化的时候会设定一个名为 java.lang.IntegerCache.high 系统属性，然后 IntegerCache 初始化的时候就会读取该系统属性来决定上界。 [String](https://cyc2018.github.io/CS-Notes/#/notes/Java 基础?id=二、string) String 被声明为 final，因此它不可被继承。(Integer 等包装类也不能被继承） 在 Java 8 中，String 内部使用 ** final byte[] 数组存储数据**。 内部实现: 12345678public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; /** The value is used for character storage. */ private final byte[] value;// /** The identifier of the encoding used to encode the bytes in &#123;@code value&#125;. */ private final byte coder;&#125; value数组被声明为final,这意味着value数组初始化之后就不能在引用其他数组,并且String内部没有改变value数组的方法,因此可以保证String不可变 String Pool 字符串常量池（String Pool）保存着所有字符串字面量（literal strings），这些字面量在编译时期就确定。不仅如此，还可以使用 String 的 intern() 方法在运行过程中将字符串添加到 String Pool 中。 123456String s1 = new String(\"aaa\");String s2 = new String(\"aaa\");System.out.println(s1 == s2); // falseString s3 = s1.intern();String s4 = s1.intern();System.out.println(s3 == s4); // true 不可变的好处 1. 可以缓存 hash 值 因为 String 的 hash 值经常被使用，例如 String 用做 HashMap 的 key。不可变的特性可以使得 hash 值也不可变，因此只需要进行一次计算。 2. String Pool 的需要 如果一个 String 对象已经被创建过了，那么就会从 String Pool 中取得引用。只有 String 是不可变的，才可能使用 String Pool(否则可变的话,某个修改操作将导致所有引用该对象的值都发生改变)。 String, StringBuffer and StringBuilder1. 可变性 String 不可变 StringBuffer 和 StringBuilder 可变 2. 线程安全 String 不可变，因此是线程安全的 StringBuilder 不是线程安全的 StringBuffer 是线程安全的，内部使用 synchronized 进行同步 (几乎所有方法都用了synchronized关键字进行加锁) 运算参数传递!!!! Java 的参数是以值传递的形式传入方法中，而不是引用传递。 以下代码中 Dog dog 的 dog 是一个指针，存储的是对象的地址。在将一个参数传入一个方法时，本质上是将对象的地址以值的方式传递到形参中。因此在方法中使指针引用其它对象，那么这两个指针此时指向的是完全不同的对象，在一方改变其所指向对象的内容时对另一方没有影响。 123456789101112131415public class PassByValueExample &#123; public static void main(String[] args) &#123; Dog dog = new Dog(\"A\"); System.out.println(dog.getObjectAddress()); // Dog@4554617c func(dog); System.out.println(dog.getObjectAddress()); // Dog@4554617c System.out.println(dog.getName()); // A &#125; private static void func(Dog dog) &#123; System.out.println(dog.getObjectAddress()); // Dog@4554617c dog = new Dog(\"B\"); System.out.println(dog.getObjectAddress()); // Dog@74a14482 System.out.println(dog.getName()); // B &#125;&#125; 如果在方法中改变对象的字段值会改变原对象该字段值，因为改变的是同一个地址指向的内容 !!! 12345678910class PassByValueExample &#123; public static void main(String[] args) &#123; Dog dog = new Dog(\"A\"); func(dog); System.out.println(dog.getName()); // B &#125; private static void func(Dog dog) &#123; dog.setName(\"B\");//改变原地址指向的对象的值,所以原对象会受到影响 &#125;&#125; float与doubleJava 不能隐式执行向下转型，因为这会使得精度降低。 1.1 字面量属于 double 类型，不能直接将 1.1 直接赋值给 float 变量，因为这是向下转型。 1234// float f = 1.1; double a=1.1; float b=2.3f; double v = 32.32d; 1.1f 字面量才是 float 类型。 1float f = 1.1f; 隐式类型转换因为字面量 1 是 int 类型，它比 short 类型精度要高，因此不能隐式地将 int 类型下转型为 short 类型。 12short s1 = 1;// s1 = s1 + 1; 但是使用 += 或者 ++ 运算符可以执行隐式类型转换。 12s1 += 1;// s1++; 上面的语句相当于将 s1 + 1 的计算结果进行了向下转型： 1s1 = (short) (s1 + 1); switch从 Java 7 开始，可以在 switch 条件判断语句中使用 String 对象。但是不支持Long !!!! 继承权限修饰符 Java 中有三个访问权限修饰符：private、protected 以及 public，如果不加访问修饰符，表示default包级可见 可以对类或类中的成员（字段以及方法）加上访问修饰符。 重写与重载1. 重写（Override） 存在于继承体系中，指子类实现了一个与父类在方法声明上完全相同的一个方法。 为了满足里式替换原则，重写有以下三个限制： 子类方法的访问权限必须大于等于父类方法； ==( 权限&gt;= )== 子类方法的返回类型必须是父类方法返回类型或为其子类型。 ==( 返回类型相容 )== 子类方法抛出的异常类型必须是父类抛出异常类型或为其子类型。 (抛出异常相容) 使用 @Override 注解，可以让编译器帮忙检查是否满足上面的三个限制条件。 下面的示例中，SubClass 为 SuperClass 的子类，SubClass 重写了 SuperClass 的 func() 方法。其中： 子类方法访问权限为 public，大于父类的 protected。 子类的返回类型为 ArrayList，是父类返回类型 List 的子类。 子类抛出的异常类型为 Exception，是父类抛出异常 Throwable 的子类。 子类重写方法使用 @Override 注解，从而让编译器自动检查是否满足限制条件。 123456789//父类protected List&lt;Integer&gt; func() throws Throwable &#123; return new ArrayList&lt;&gt;(); &#125;//子类重写的 @Override public ArrayList&lt;Integer&gt; func() throws Exception &#123; return new ArrayList&lt;&gt;(); &#125; 2. 重载（Overload） 重载只关心的是参数是否相同,而不关心返回值类型 !!!! 存在于同一个类中，指一个方法与已经存在的方法名称上相同，但是参数类型、个数、顺序至少有一个不同。 应该注意的是，返回值不同，其它都相同不算是重载。 equals() Object类中默认实现是比较两个对象的地址引用是否相等 对于基本类型，== 判断两个值是否相等，基本类型没有 equals() 方法。 对于引用类型，== 判断两个变量是否引用同一个对象，而 equals() 判断引用的对象是否等价。 hashCode() hashCode() 返回散列值，而 equals() 是用来判断两个对象是否等价。等价的两个对象散列值一定相同，但是散列值相同的两个对象不一定等价。 !!! 所以hashCode是equals() 方法使用的前提 所以重写equals方法一定要重写hashCode方法, 一般多用于hash环境下 toString()Object类原生实现: 类名+对象的hash值 clone() 使用 clone() 方法来拷贝一个对象即复杂又有风险，它会抛出异常，并且还需要类型转换。Effective Java 书上讲到，最好不要去使用 clone()，可以使用拷贝构造函数或者拷贝工厂来拷贝一个对象。 Object类原生实现: 这个是一个protected方法, 必须要重写才能调用, 并且其原生实现是直接clone的对象的地址属于浅拷贝 浅拷贝 拷贝对象和原始对象的引用类型引用同一个对象。 深拷贝 拷贝对象和原始对象的引用类型引用不同对象。 super 访问父类的构造函数：可以使用 super() 函数访问父类的构造函数，从而委托父类完成一些初始化的工作。应该注意到，子类一定会调用父类的构造函数来完成初始化工作，(子类不用super()显式调用的情况下) 一般是自动调用父类的默认构造函数，如果子类需要调用父类其它构造函数，那么就可以使用 super 函数。 访问父类的成员：如果子类重写了父类的某个方法，可以通过使用 super 关键字来引用父类的方法实现。 关键字static1. 静态变量 静态变量：又称为类变量，也就是说这个变量属于类的，类所有的实例都共享静态变量，可以直接通过类名来访问它。静态变量在内存中只存在一份。 实例变量：每创建一个实例就会产生一个实例变量，它与该实例同生共死。 123456789101112public class A &#123; private int x; // 实例变量 private static int y; // 静态变量 public static void main(String[] args) &#123; // int x = A.x; // Non-static field 'x' cannot be referenced from a static context A a = new A(); int x = a.x; int y = A.y; &#125;&#125; 2. 静态方法 只能访问所属类的静态字段和静态方法( 这些属于类所有的, 而不是对象所有)，方法中不能有 this 和 super 关键字。 静态方法在类加载的时候就存在了，它不依赖于任何实例。所以静态方法必须有实现，也就是说它不能是抽象方法。 12345public abstract class A &#123; public static void func1()&#123; &#125; // public abstract static void func2(); // Illegal combination of modifiers: 'abstract' and 'static'&#125; 3. 静态语句块 静态语句块在类初始化时仅运行一次。 123456789public class A &#123; static &#123; System.out.println(\"123\"); &#125; public static void main(String[] args) &#123; A a1 = new A(); A a2 = new A(); &#125;&#125; 4. 静态内部类 非静态内部类依赖于外部类的实例，而静态内部类不需要。 静态内部类不能访问外部类的非静态的变量和方法。 123456789101112131415public class OuterClass &#123; class InnerClass &#123; &#125; static class StaticInnerClass &#123; &#125; public static void main(String[] args) &#123; // InnerClass innerClass = new InnerClass(); // 'OuterClass.this' cannot be referenced from a static context OuterClass outerClass = new OuterClass(); InnerClass innerClass = outerClass.new InnerClass(); StaticInnerClass staticInnerClass = new StaticInnerClass(); &#125;&#125; 5. 静态导包 在使用静态变量和方法时不用再指明 ClassName，从而简化代码，但可读性大大降低。 1import static com.xxx.ClassName.* 6. 初始化顺序 静态变量和静态语句块优先于实例变量和普通语句块，静态变量和静态语句块的初始化顺序取决于它们在代码中的顺序。 12345678public static String staticField = \"静态变量\";Copy to clipboardErrorCopiedstatic &#123; System.out.println(\"静态语句块\");&#125;Copy to clipboardErrorCopiedpublic String field = \"实例变量\";Copy to clipboardErrorCopied&#123; System.out.println(\"普通语句块\");&#125;Copy to clipboardErrorCopied 最后才是构造函数的初始化。 123public InitialOrderTest() &#123; System.out.println(\"构造函数\");&#125;Copy to clipboardErrorCopied 存在继承的情况下，初始化顺序为： 父类（静态变量、静态语句块） 子类（静态变量、静态语句块） 父类（实例变量、普通语句块） 父类（构造函数） 子类（实例变量、普通语句块） 子类（构造函数） 反射 ==每个类都有一个 Class 对象，包含了与类有关的信息。当编译一个新类时，会产生一个同名的 .class 文件，该文件内容保存着 Class 对象。== 类加载相当于 Class 对象的加载，类在第一次使用时才动态加载到 JVM 中。也可以使用 Class.forName(&quot;com.mysql.jdbc.Driver&quot;) 这种方式来控制类的加载，该方法会返回一个 Class 对象。 反射可以提供运行时的类信息，并且这个类可以在运行时才加载进来，甚至在编译时期该类的 .class 不存在也可以加载进来。 Class 和 java.lang.reflect 一起对反射提供了支持，java.lang.reflect 类库主要包含了以下三个类： Field ：可以使用 get() 和 set() 方法读取和修改 Field 对象关联的字段； Method ：可以使用 invoke() 方法调用与 Method 对象关联的方法； Constructor ：可以用 Constructor 的 newInstance() 创建新的对象。 反射的优点： 可扩展性 ：应用程序可以利用全限定名创建可扩展对象的实例，来使用来自外部的用户自定义类。 类浏览器和可视化开发环境 ：一个类浏览器需要可以枚举类的成员。可视化开发环境（如 IDE）可以从利用反射中可用的类型信息中受益，以帮助程序员编写正确的代码。 调试器和测试工具 ： 调试器需要能够检查一个类里的私有成员。测试工具可以利用反射来自动地调用类里定义的可被发现的 API 定义，以确保一组测试中有较高的代码覆盖率。 反射的缺点： 尽管反射非常强大，但也不能滥用。如果一个功能可以不用反射完成，那么最好就不用。在我们使用反射技术时，下面几条内容应该牢记于心。 性能开销 ：反射涉及了动态类型的解析，所以 JVM 无法对这些代码进行优化。因此，反射操作的效率要比那些非反射操作低得多。我们应该避免在经常被执行的代码或对性能要求很高的程序中使用反射。 安全限制 ：使用反射技术要求程序必须在一个没有安全限制的环境中运行。如果一个程序必须在有安全限制的环境中运行，如 Applet，那么这就是个问题了。 内部暴露 ：由于反射允许代码执行一些在正常情况下不被允许的操作（暴露了源码文件: 比如访问私有的属性和方法），所以使用反射可能会导致意料之外的副作用，这可能导致代码功能失调并破坏可移植性。反射代码破坏了抽象性，因此当平台发生改变的时候，代码的行为就有可能也随着变化。 Trail: The Reflection API 深入解析 Java 反射（1）- 基础 抽象类与接口1. 抽象类 抽象类和抽象方法都使用 abstract 关键字进行声明。如果一个类中包含抽象方法，那么这个类必须声明为抽象类。 抽象类和普通类最大的区别是，抽象类不能被实例化，需要继承抽象类才能实例化其子类。 2. 接口 接口是抽象类的延伸，在 Java 8 之前，它可以看成是一个完全抽象的类，也就是说它不能有任何的方法实现。从 Java 8 开始，接口也可以拥有默认的方法实现，这是因为不支持默认方法的接口的维护成本太高了。在 Java 8 之前，如果一个接口想要添加新的方法，那么要修改所有实现了该接口的类。 接口的成员（字段 + 方法）默认都是 public 的，并且不允许定义为 private 或者 protected (因为接口必须被实现)。 接口的字段默认都是 static 和 final 的。 3. 比较 从设计层面上看，抽象类提供了一种 IS-A 关系，那么就必须满足里式替换原则，即子类对象必须能够替换掉所有父类对象。而接口更像是一种 LIKE-A 关系，它只是提供一种方法实现契约，并不要求接口和实现接口的类具有 IS-A 关系。 从使用上来看，一个类可以实现多个接口，但是不能继承多个抽象类。 接口的字段只能是 static 和 final 类型的，而抽象类的字段没有这种限制。 接口的成员只能是 public 的，而抽象类的成员可以有多种访问权限。 4. 使用选择 使用接口： 需要让不相关的类都实现一个方法，例如不相关的类都可以实现 Compareable 接口中的 compareTo() 方法； 需要使用多重继承。 使用抽象类： 需要在几个相关的类中共享代码。 需要能控制继承来的成员的访问权限，而不是都为 public。 需要继承非静态和非常量字段。 在很多情况下，接口优先于抽象类。因为接口没有抽象类严格的类层次结构要求，可以灵活地为一个类添加行为。并且从 Java 8 开始，接口也可以有默认的方法实现，使得修改接口的成本也变的很低。 异常Throwable 可以用来表示任何可以作为异常抛出的类,是所有异常的基类，分为两种： Error 和 Exception。其中 Error 用来表示 JVM 无法处理的错误，Exception 分为两种： 受检异常 ：需要用 try…catch… 语句捕获并进行处理，并且可以从异常中恢复； 非受检异常 ：是程序运行时错误，例如除 0 会引发 Arithmetic Exception，此时程序崩溃并且无法恢复。 泛型 泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 泛型类:123456789/***在使用泛型的时候如果传入泛型实参，则会根据传入的泛型实参做相应的限制，此时泛型才会起到本应起到的限制作用。如果不传入泛型类型实参的话，在泛型类中使用泛型的方法或成员变量定义的类型可以为任何的类型。 */public class Box&lt;T&gt; &#123; // T stands for \"Type\" private T t; public void set(T t) &#123; this.t = t; &#125; public T get() &#123; return t; &#125;&#125; 泛型接口:1234567891011/** * 未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中 * 即：class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;&#123; * 如果不声明泛型，如：class FruitGenerator implements Generator&lt;T&gt;，编译器会报错：\"Unknown class\" */class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;&#123; @Override public T next() &#123; return null; &#125;&#125; 泛型方法:123456789101112131415/** * 泛型方法的基本介绍 * @param tClass 传入的泛型实参 * @return T 返回值为T类型 * 说明： * 1）public 与 返回值中间&lt;T&gt;非常重要，可以理解为声明此方法为泛型方法。 * 2）只有声明了&lt;T&gt;的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 * 3）&lt;T&gt;表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T。 * 4）与泛型类的定义一样，此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型。 */public &lt;T&gt; T genericMethod(Class&lt;T&gt; tClass)throws InstantiationException , IllegalAccessException&#123; T instance = tClass.newInstance(); return instance;&#125; 泛型面试题: Java中的泛型是什么 ? 使用泛型的好处是什么? 那些拥有Java1.4或更早版本的开发背景的人都知道，在集合中存储对象并在使用前进行类型转换是多么的不方便。泛型防止了那种情况的发生。它提供了编译期的类型安全，确保你只能把正确类型的对象放入集合中，避免了在运行时出现ClassCastException。 Java的泛型是如何工作的 ? 什么是类型擦除 ? 泛型是通过类型擦除来实现的，编译器在编译时擦除了所有类型相关的信息，所以在运行时不存在任何类型相关的信息。例如List在运行时仅用一个List来表示。这样做的目的，是确保能和Java 5之前的版本开发二进制类库进行兼容。你无法在运行时访问到类型参数，因为编译器已经把泛型类型转换成了原始类型。根据你对这个泛型问题的回答情况，你会得到一些后续提问，比如为什么泛型是由类型擦除来实现的或者给你展示一些会导致编译器出错的错误泛型代码。请阅读我的Java中泛型是如何工作的来了解更多信息。 Array中可以用泛型吗? 不可以, Array事实上并不支持泛型, 建议用List来代替Array,list可以提供编译期间的类型安全保证 可以把List传递给一个接受List参数的方法吗？ 不可以, 会导致编译错误, 因为List 可以存储任何类型的对象,而List 只能是String 编写一个泛型方法，让它能接受泛型参数并返回泛型类型? 123public V put(K key, V value) &#123; return cache.put(key, value); &#125; 注解 注解本质是一个继承了Annotation 的特殊接口，其具体实现类是Java 运行时生成的动态代理类。而我们通过反射获取注解时，返回的是Java 运行时生成的动态代理对象$Proxy1。通过代理对象调用自定义注解（接口）的方法，会最终调用AnnotationInvocationHandler 的invoke 方法。该方法会从memberValues 这个Map 中索引出对应的值。而memberValues 的来源是Java 常量池。 (存了类结构文件的值) 自定义注解类编写的一些规则: \\1. Annotation 型定义为@interface, 所有的Annotation 会自动继承java.lang.Annotation这一接口,并且不能再去继承别的类或是接口. \\2. 参数成员只能用public 或默认(default) 这两个访问权修饰 \\3. 参数成员只能用基本类型byte、short、char、int、long、float、double、boolean八种基本数据类型和String、Enum、Class、annotations等数据类型，以及这一些类型的数组. \\4. 要获取类方法和字段的注解信息，必须通过Java的反射技术来获取 Annotation 对象，因为你除此之外没有别的获取注解对象的方法 \\5. 注解也可以没有定义成员,，不过这样注解就没啥用了PS:自定义注解需要使用到元注解 基本容器List, Set, Queue 都继承自Collection接口 List 优点 缺点 保存元素的顺序 应用 ArrayList 随机访问速度快，内部使用数组实现。 迭代，插入和删除元素慢，尤其是当List]尺寸比较大的时候。 插入顺序 可变长数组 LinkedList 迭代(顺序访问经过优化)，插入，删除都很快内部使用双向链表实现 随机访问速度慢 插入顺序 顺序访问, 批量插入删除元素的场合 Set 优点 保存元素的顺序 要求 HashSet 为快速查找设计 散列存储 必须定义hashCode()方法 LinkedHashSet 和HashSet一样的查询速度，但是插入要比HashSet慢一些，因为它通过维护链表形式维护元素。 使用链表维护元素顺序(插入顺序) 必须定义hashCode()方法 TreeSet 保存有序的Set，底层通过TreeMap来实现的 按照排序顺序维护元素 必须实现Comparable接口(包含compareTo方法) HashMap, HashTable都继承自Map接口 Queue 特点 保存元素的顺序 LinkedList LinkedList除了普通List之外，还添加了很多实现&lt;队列，栈，双向队列&gt;三种数据结构的方法。尤其是模拟Queue的时候在两端插入删除元素很快(经过了优化)。 插入的顺序 PriorityQueue 按照排序顺序取出元素，所以要求必须实现Comparable接口。 排序顺序 Map 特点 保存元素的顺序 要求 HashMap Map基于散列存储，插入和查询“键值对”的开销是固定的。 散列存储 存入的键需要具备hashCode()方法，当然，返回的标识不一定要唯一 LinkedHashMap 为了提高速度散列了所有元素，插入查询只比HashMap慢一点点，因为它在维护散列数据结构的同时还要维护链表(插入顺序)。 但是迭代访问的时候更快，因为内部使用链表维护次序。 插入顺序 同样需要键实现hashCode()方法 TreeMap Map基于红黑树的实现。所以所得的结果是经过排序的。 红黑树 为了排序，必须实现Comparable接口。 特性Java 各版本的新特性New highlights in Java SE 8 Lambda Expressions Pipelines and Streams Date and Time API Default Methods Type Annotations Nashhorn JavaScript Engine Concurrent Accumulators Parallel operations PermGen Error Removed New highlights in Java SE 7 Strings in Switch Statement Type Inference for Generic Instance Creation Multiple Exception Handling Support for Dynamic Languages Try with Resources Java nio Package Binary Literals, Underscore in literals Diamond Syntax Difference between Java 1.8 and Java 1.7? Java 8 特性 Java 与 C++ 的区别 Java 是纯粹的面向对象语言，所有的对象都继承自 java.lang.Object，C++ 为了兼容 C 即支持面向对象也支持面向过程。 Java 通过虚拟机从而实现跨平台特性，但是 C++ 依赖于特定的平台。 Java 没有指针，它的引用可以理解为安全指针，而 C++ 具有和 C 一样的指针。 Java 支持自动垃圾回收，而 C++ 需要手动回收。 Java 不支持多重继承，只能通过实现多个接口来达到相同目的，而 C++ 支持多重继承。 Java 不支持操作符重载，虽然可以对两个 String 对象执行加法运算，但是这是语言内置支持的操作，不属于操作符重载，而 C++ 可以。 Java 的 goto 是保留字，但是不可用，C++ 可以使用 goto。 JRE or JDK JRE is the JVM program, Java application need to run on JRE. JDK is a superset of JRE, JRE + tools for developing java programs. e.g, it provides the compiler “javac”","categories":[{"name":"Java","slug":"Java","permalink":"/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"/tags/Java/"},{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"}]},{"title":"Mybatis面试","slug":"Mybatis面试","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T06:03:25.382Z","comments":true,"path":"2019/10/20/Mybatis面试/","link":"","permalink":"/2019/10/20/Mybatis%E9%9D%A2%E8%AF%95/","excerpt":"","text":"1、什么是Mybatis？ （1）Mybatis是一个半ORM（对象关系映射）框架，它内部封装了JDBC，开发时只需要关注SQL语句本身，不需要花费精力去处理加载驱动、创建连接、创建statement等繁杂的过程。程序员直接编写原生态sql, 严格控制sql执行性能，灵活度高。 （2）MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。 （3）通过xml 文件或注解的方式将要执行的各种 statement 配置起来，并通过java对象和 statement中sql的动态参数进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射为java对象并返回。（从执行sql到返回result的过程). 2、Mybaits的优点： （1）基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用。 （2）与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接； （3）很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）。 （4）能够与Spring很好的集成； （5）提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护。 3、MyBatis框架的缺点： （1）SQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求。 （2）SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。 4、MyBatis框架适用场合： （1）MyBatis专注于SQL本身，是一个足够灵活的DAO层解决方案。 （2）对性能的要求很高，或者需求变化较多的项目，如互联网项目，MyBatis将是不错的选择。 5、MyBatis与Hibernate有哪些不同？ （1）Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句。 （2）Mybatis直接编写原生态sql，可以严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需求变化要求迅速输出成果。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件，则需要自定义多套sql映射文件，工作量大。 （3）Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件，如果用hibernate开发可以节省很多代码，提高效率。 6、#{}和${}的区别是什么？ #{}是预编译处理，${}是字符串替换。 Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值； #{} 支持更多的数据类型的操作, jdbctype, javatype,… 等等 Mybatis在处理${}时，就是把${}替换成变量的值。 使用#{}可以有效的防止SQL注入，提高系统安全性。 7、当实体类中的属性名和表中的字段名不一样 ，怎么办 ？ 第1种： 通过在查询的sql语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。 123&lt;select id=”selectorder” parametertype=”int” resultetype=”me.gacl.domain.order”&gt; select order_id id, order_no orderno ,order_price price form orders where order_id=#&#123;id&#125;;&lt;/select&gt; 第2种： 通过来映射字段名和实体类属性名的一一对应的关系。 123&lt;select id=\"getOrder\" parameterType=\"int\" resultMap=\"orderresultmap\"&gt; select * from orders where order_id=#&#123;id&#125; &lt;/select&gt; 1234567&lt;resultMap type=\"me.gacl.domain.order\" id=\"orderresultmap\"&gt; &lt;!–用id属性来映射主键字段–&gt; &lt;id property=\"id\" column=\"order_id\"&gt; &lt;!–用result属性来映射非主键字段，property为实体类属性名，column为数据表中的属性–&gt; &lt;result property =\"orderno\" column =\"order_no\"/&gt; &lt;result property=\"price\" column=\"order_price\" /&gt;&lt;/reslutMap&gt; 8、 模糊查询like语句该怎么写? 第1种：在Java代码中添加sql通配符。 12string wildcardname = “%smi%”;list&lt;name&gt; names = mapper.selectlike(wildcardname); 123&lt;select id=”selectlike”&gt; select * from foo where bar like #&#123;value&#125;&lt;/select&gt; 第2种：在sql语句中拼接通配符，会引起sql注入 12string wildcardname = “smi”;list&lt;name&gt; names = mapper.selectlike(wildcardname); 123&lt;select id=”selectlike”&gt; select * from foo where bar like \"%\"#&#123;value&#125;\"%\"&lt;/select&gt; 第3种 123456789&lt;select id=\"getEmpsTestInnerParameter\" resultType=\"Employee\"&gt; -- bind: 可以将OGNL表达式的值绑定到一个变量中给,方便后来引用这个变量的值 &lt;bind name=\"likeE\" value=\"'%'+lastName'+'%'\"&gt;&lt;/bind&gt; select * from tbl_employee &lt;if test=\"_parameter!=null\"&gt; -- where id=#&#123;_parameter.id&#125; where lastName like #&#123;likeE&#125; &lt;/if&gt;&lt;/select&gt; 9、通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗？ 原理: Mapper 接口的工作原理是JDK动态代理 不能重载, 但是可以根据标签上指定的数据库id, 动态得执行sql语句的版本 Dao接口即Mapper接口, 接口的全类名，就是映射文件中的namespace的值；接口的方法名，就是映射文件中Mapper的Statement的id值；接口方法内的参数，就是传递给sql的参数。 Mapper接口是没有实现类的，当调用接口方法时，接口全类名+方法名拼接字符串作为key值，可唯一定位一个MapperStatement。在Mybatis中，==每一个&lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt;标签，都会被解析为一个MapperStatement对象。== 举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到namespace为com.mybatis3.mappers.StudentDao下面 id 为 findStudentById 的 MapperStatement。 Mapper接口里的方法，是不能重载的，因为是使用 全类名+方法名 的保存和寻找策略。Mapper 接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Mapper接口生成代理对象proxy，代理对象会拦截接口方法，转而执行MapperStatement所代表的sql，然后将sql执行结果返回。 10、Mybatis是如何进行分页的？分页插件的原理是什么？Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页。可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件—-(com.github.pagehelper)来完成物理分页。 分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。 11、Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？ 第一种是使用标签，逐一定义数据库列名和对象属性名之间的映射关系。 第二种是使用sql列的别名功能，将列的别名书写为对象属性名。(自动封装) 有了列名与属性名的映射关系后，Mybatis通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 12、如何执行批量插入? 第一种方法 首先,创建一个简单的insert语句: 123&lt;insert id=”insertname”&gt; insert into names (name) values (#&#123;value&#125;)&lt;/insert&gt; 然后在java代码中像下面这样执行批处理插入: 12345list&lt;string&gt; names = new arraylist(); names.add(“fred”); names.add(“barney”); names.add(“betty”); names.add(“wilma”); 1234567891011121314151617// 注意这里 executortype.batchsqlsession sqlsession = sqlsessionfactory.opensession(executortype.batch);try &#123; namemapper mapper = sqlsession.getmapper(namemapper.class); //for循环执行插入 for (string name : names) &#123; mapper.insertname(name); &#125; sqlsession.commit();&#125;catch(Exception e)&#123; e.printStackTrace(); sqlSession.rollback(); throw e; &#125; finally &#123; sqlsession.close();&#125; 第二种方法: 123456789101112131415161718192021&lt;!-- 批量保存--&gt;&lt;!-- mysql下批量保存, 可以foreach宾利, mysql支持values(),(),()语法--&gt;&lt;insert id=\"addEmps\" databaseId=\"mysql\"&gt; insert into tbl_employee (lastName, gender, email, department) values &lt;foreach collection=\"emps\" item=\"emp\" separator=\",\"&gt; (#&#123;emp.lastName&#125;,#&#123;emp.gender&#125;,#&#123;emp.email&#125;,#&#123;emp.department.id&#125;) &lt;/foreach&gt;&lt;/insert&gt;&lt;!-- 批量保存一次执行多个sql语句需要在url连接处添加allowMultiQueries=true--&gt;&lt;insert id=\"addEmps2\" databaseId=\"oracle\"&gt; &lt;foreach collection=\"emps\" item=\"emp\" separator=\";\"&gt; insert into tbl_employee (lastName, gender, email, department) values (#&#123;emp.lastName&#125;,#&#123;emp.gender&#125;,#&#123;emp.email&#125;,#&#123;emp.department.id&#125;) &lt;/foreach&gt;&lt;/insert&gt; 13、如何获取自动生成的(主)键值?insert 方法总是返回一个int值 ，这个值代表的是插入的行数。 如果采用自增长策略，自动生成的键值在 insert 方法执行完后可以被设置到传入的参数对象中。 示例： 12345&lt;!-- useGeneratedKeys=\"true\": 使用自增的键值,会返回每次生成的值 keyProperty=\"id\" 将自增的主键值赋给对象的id(自定义)属性 --&gt;&lt;insert id=\"insertname\" usegeneratedkeys=\"true\" keyproperty=\"id\"&gt; insert into names (name) values (#&#123;name&#125;)&lt;/insert&gt; name name = new name(); name.setname(“fred”); int rows = mapper.insertname(name); // 完成后,id已经被设置到对象中 system.out.println(“rows inserted = ” + rows); system.out.println(“generated key value = ” + name.getid());14、在mapper中如何传递多个参数? （1）第一种： 123//DAO层的函数Public UserselectUser(String name,String area); //对应的xml,#&#123;0&#125;代表接收的是dao层中的第一个参数，#&#123;1&#125;代表dao层中第二参数，更多参数一致往后加即可。 123&lt;select id=\"selectUser\"resultMap=\"BaseResultMap\"&gt; select * fromuser_user_t whereuser_name = #&#123;0&#125; anduser_area=#&#123;1&#125; &lt;/select&gt; （2）第二种： 使用 @param 注解: 123public interface usermapper &#123; user selectuser(@param(“username”) string username,@param(“hashedpassword”) string hashedpassword);&#125; 然后,就可以在xml像下面这样使用(推荐封装为一个map,作为单个参数传递给mapper): 123456&lt;select id=”selectuser” resulttype=”user”&gt; select id, username, hashedpassword from some_table where username = #&#123;username&#125; and hashedpassword = #&#123;hashedpassword&#125;&lt;/select&gt; （3）第三种：多个参数封装成map 1234567891011121314try&#123;//映射文件的命名空间.SQL片段的ID，就可以调用对应的映射文件中的SQL//由于我们的参数超过了两个，而方法中只有一个Object参数收集，因此我们使用Map集合来装载我们的参数Map&lt;String, Object&gt; map = new HashMap(); map.put(\"start\", start); map.put(\"end\", end); return sqlSession.selectList(\"StudentID.pagination\", map); &#125;catch(Exception e)&#123; e.printStackTrace(); sqlSession.rollback(); throw e; &#125;finally&#123; MybatisUtil.closeSqlSession(); &#125; 15、Mybatis动态sql有什么用？执行原理？有哪些动态sql？Mybatis动态sql可以在Xml映射文件内，以标签的形式编写动态sql，执行原理是根据表达式的值 完成逻辑判断并动态拼接sql的功能。类似于OGNL表达式 Mybatis提供了9种动态sql标签：trim | where | set | foreach | if | choose | when | otherwise | bind。 16、Xml映射文件中，除了常见的select|insert|updae|delete标签之外，还有哪些标签？答：、、、、，加上动态sql的9个标签，其中为sql片段标签, 可重用，通过标签引入sql片段，为不支持自增的主键生成策略标签。 17、Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复； 原因就是namespace+id是作为Map&lt;String, MapperStatement&gt;的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。 18、为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。 19、 一对一、一对多的关联查询 ？ 一对一 123456789101112131415&lt;mapper namespace=\"com.lcb.mapping.userMapper\"&gt; &lt;select id=\"getClass\" parameterType=\"int\" resultMap=\"ClassesResultMap\"&gt; select * from class c,teacher t where c.teacher_id=t.t_id and c.c_id=#&#123;id&#125; &lt;/select&gt; &lt;!--association 一对一关联查询 --&gt; &lt;resultMap type=\"com.lcb.user.Classes\" id=\"ClassesResultMap\"&gt; &lt;!-- 实体类的字段名和数据表的字段名映射 --&gt; &lt;id property=\"id\" column=\"c_id\"/&gt; &lt;result property=\"name\" column=\"c_name\"/&gt; &lt;association property=\"teacher\" javaType=\"com.lcb.user.Teacher\"&gt; &lt;id property=\"id\" column=\"t_id\"/&gt; &lt;result property=\"name\" column=\"t_name\"/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;/mapper&gt; 一对多的关联查询 12345678910111213141516171819&lt;mapper namespace=\"com.lcb.mapping.userMapper\"&gt; &lt;!--collection 一对多关联查询 --&gt; &lt;select id=\"getClass2\" parameterType=\"int\" resultMap=\"ClassesResultMap2\"&gt; select * from class c,teacher t,student s where c.teacher_id=t.t_id and c.c_id=s.class_id and c.c_id=#&#123;id&#125; &lt;/select&gt; &lt;resultMap type=\"com.lcb.user.Classes\" id=\"ClassesResultMap2\"&gt; &lt;id property=\"id\" column=\"c_id\"/&gt; &lt;result property=\"name\" column=\"c_name\"/&gt; &lt;association property=\"teacher\" javaType=\"com.lcb.user.Teacher\"&gt; &lt;id property=\"id\" column=\"t_id\"/&gt; &lt;result property=\"name\" column=\"t_name\"/&gt; &lt;/association&gt; &lt;collection property=\"student\" ofType=\"com.lcb.user.Student\"&gt; &lt;id property=\"id\" column=\"s_id\"/&gt; &lt;result property=\"name\" column=\"s_name\"/&gt; &lt;/collection&gt; &lt;/resultMap&gt; &lt;/mapper&gt; 20、MyBatis实现一对一有几种方式?具体怎么操作的？ 有联合查询 联合查询是几个表联合查询,只查询一次, 通过在resultMap里面配置association节点配置一对一的类就可以完成； 和嵌套查询 嵌套查询是先查一个表，根据这个表里面的结果的 外键id，去再另外一个表里面查询数据,也是通过association配置，但另外一个表的查询通过select属性配置。 12345678910&lt;resultMap id=\"EmpAndDept3\" type=\"emp\"&gt; &lt;id column=\"id\" property=\"id\"&gt;&lt;/id&gt; &lt;result column=\"lastName\" property=\"lastName\"&gt;&lt;/result&gt; &lt;result column=\"email\" property=\"email\"&gt;&lt;/result&gt; &lt;result column=\"gender\" property=\"gender\"&gt;&lt;/result&gt; &lt;!-- 指定哪个属性是联合对象--&gt; &lt;association property=\"department\" select=\"com.yeung.mapper.annotation.DepartmentMapperAnnotation.getDeptById\" column=\"department\"&gt; &lt;/association&gt;&lt;/resultMap&gt; 21、MyBatis实现一对多有几种方式,怎么操作的？ 有联合查询 联合查询是几个表联合查询,只查询一次,通过在resultMap里面的collection节点配置一对多的类就可以完成； 嵌套查询 嵌套查询是先查一个表,根据这个表里面的 结果的外键id,去再另外一个表里面查询数据,也是通过配置collection,但另外一个表的查询通过select节点配置。 22、Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？答：Mybatis仅支持association关联对象和collection关联集合对象的延迟加载 association指的就是一对一 collection指的就是一对多查询 在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。 它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。 当然了，不光是Mybatis，几乎所有的包括Hibernate，支持延迟加载的原理都是一样的。 23、Mybatis的一级、二级缓存: 1）一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。 2）二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置 ； 3）对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。==(即, 执行了 增删改操作, 那么就会清空掉缓存中的该数据, 保证数据式最新的)== 24、什么是MyBatis的接口绑定？有哪些实现方式？接口绑定，就是在MyBatis中任意定义接口,然后把接口里面的方法和SQL语句绑定, 我们直接调用接口方法就可以,这样比起原来了SqlSession提供的方法我们可以有更加灵活的选择和设置。 接口绑定有两种实现方式,一种是通过注解绑定，就是在接口的方法上面加上 @Select、@Update等注解，里面包含Sql语句来绑定；另外一种就是通过xml里面写SQL来绑定, 在这种情况下,要指定xml映射文件里面的namespace必须为接口的全路径名。当Sql语句比较简单时候,用注解绑定, 当SQL语句比较复杂时候,用xml绑定,一般用xml绑定的比较多。 25、使用MyBatis的mapper接口调用时有哪些要求？ ① Mapper接口方法名和mapper.xml中定义的每个sql的id相同；② Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同；③ Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同；④ Mapper.xml文件中的namespace即是mapper接口的类路径。 26、Mapper编写有哪几种方式？ 第一种：接口实现类继承SqlSessionDaoSupport：使用此种方法需要编写mapper接口，mapper接口实现类、mapper.xml文件。 （1）在sqlMapConfig.xml中配置mapper.xml的位置 1234&lt;mappers&gt; &lt;mapper resource=\"mapper.xml文件的地址\" /&gt; &lt;mapper resource=\"mapper.xml文件的地址\" /&gt;&lt;/mappers&gt; （2）定义mapper接口 （3）实现类集成SqlSessionDaoSupport mapper方法中可以this.getSqlSession()进行数据增删改查。 （4）spring 配置 123&lt;bean id=\" \" class=\"mapper接口的实现\"&gt; &lt;property name=\"sqlSessionFactory\" ref=\"sqlSessionFactory\"&gt;&lt;/property&gt;&lt;/bean&gt; 第二种：使用org.mybatis.spring.mapper.MapperFactoryBean： （1）在sqlMapConfig.xml中配置mapper.xml的位置，如果mapper.xml和mappre接口的名称相同且在同一个目录，这里可以不用配置 1234&lt;mappers&gt; &lt;mapper resource=\"mapper.xml文件的地址\" /&gt; &lt;mapper resource=\"mapper.xml文件的地址\" /&gt;&lt;/mappers&gt; （2）定义mapper接口： ①mapper.xml中的namespace为mapper接口的地址 ②mapper接口中的方法名和mapper.xml中的定义的statement的id保持一致 ③Spring中定义 1234&lt;bean id=\"\" class=\"org.mybatis.spring.mapper.MapperFactoryBean\"&gt; &lt;property name=\"mapperInterface\" value=\"mapper接口地址\" /&gt; &lt;property name=\"sqlSessionFactory\" ref=\"sqlSessionFactory\" /&gt; &lt;/bean&gt; 第三种：使用mapper扫描器： （1）mapper.xml文件编写： mapper.xml中的namespace为mapper接口的地址；mapper接口中的方法名和mapper.xml中的定义的statement的id保持一致；如果将mapper.xml和mapper接口的名称保持一致则不用在sqlMapConfig.xml中进行配置。 （2）定义mapper接口：注意mapper.xml的文件名和mapper的接口名称保持一致，且放在同一个目录 （3）配置mapper扫描器： 1234&lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"mapper接口包地址\"&gt;&lt;/property&gt; &lt;property name=\"sqlSessionFactoryBeanName\" value=\"sqlSessionFactory\"/&gt; &lt;/bean&gt; （4）使用扫描器后从spring容器中获取mapper的实现对象。 27、简述Mybatis的插件运行原理，以及如何编写一个插件。 ParameterHandler、 ResultSetHandler、 StatementHandler、 Executor 答：Mybatis仅可以编写针对这4种接口的插件，Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 123456/** * 插件的编写方法: * 1. 编写Interceptor的实现类 * 2. 使用@Intercepts注解完成插件的签名 * 3. 将写好的插件注册到全局配置文件中 */ 28、SSM优缺点、使用场景？ Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句，不过mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。 总之，按照用户的需求在有限的资源环境下只要能做出维护性、扩展性良好的软件架构都是好架构，所以框架只有适合才是最好。 29、 简单介绍下你对mybatis的理解？ mybatis配置 SqlMapConfig.xml，此文件作为mybatis的全局配置文件，配置了mybatis的运行环境等信息。 mapper.xml文件即sql映射文件，文件中配置了操作数据库的sql语句。此文件需要在SqlMapConfig.xml中加载。 通过mybatis环境等配置信息构造SqlSessionFactory即会话工厂 由会话工厂创建sqlSession即会话，操作数据库需要通过sqlSession进行。 mybatis底层自定义了Executor执行器接口操作数据库，Executor接口有两个实现，一个是基本执行器、一个是缓存执行器。 Mapped Statement也是mybatis一个底层封装对象，它包装了mybatis配置信息及sql映射信息等。mapper.xml文件中一个sql对应一个Mapped Statement对象，sql的id即是Mapped statement的id。 Mapped Statement对sql执行输入参数进行定义，包括HashMap、基本类型、pojo，Executor通过Mapped Statement在执行sql前将输入的java对象映射至sql中，输入参数映射就是jdbc编程中对preparedStatement设置参数。 Mapped Statement对sql执行输出结果进行定义，包括HashMap、基本类型、pojo，Executor通过Mapped Statement在执行sql后将输出结果映射至java对象中，输出结果映射过程相当于jdbc编程中对结果的解析处理过程。","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"数据库","slug":"数据库","permalink":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"},{"name":"Mybatis","slug":"Mybatis","permalink":"/tags/Mybatis/"}]},{"title":"缓存","slug":"缓存","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T13:40:18.008Z","comments":true,"path":"2019/10/20/缓存/","link":"","permalink":"/2019/10/20/%E7%BC%93%E5%AD%98/","excerpt":"","text":"缓存 缓存是现在系统中必不可少的模块，并且已经成为了高并发高性能架构的一个关键组件 缓存是通过牺牲强一致性来提高性能的。所以使用缓存提升性能，就是会有数据更新的延迟。这需要我们在设计时结合业务仔细思考是否适合用缓存。然后缓存一定要设置过期时间，这个时间太短太长都不好，太短的话请求可能会比较多的落到数据库上，这也意味着失去了缓存的优势。太长的话缓存中的脏数据会使系统长时间处于一个延迟的状态，而且系统中长时间没有人访问的数据一直存在内存中不过期，浪费内存。 缓存能解决的问题 提升性能 绝大多数情况下，select 是出现性能问题最大的地方。 一方面，select 会有很多像 join、group、order、like 等这样丰富的语义，而这些语义是非常耗性能的； 另一方面，大多 数应用都是读多写少，所以加剧了慢查询的问题。 分布式系统中远程调用也会耗很多性能，因为有网络开销，会导致整体的响应时间下降。为了挽救这样的性能开销，在业务允许的情况（不需要太实时的数据）下，使用缓存是非常必要的事情。 缓解数据库压力当用户请求增多时，数据库的压力将大大增加，通过缓存能够大大降低数据库的压力。 缓存的适用场景 对于数据实时性要求不高 对于一些经常访问但是很少改变的数据，读明显多于写，适用缓存就很有必要。比如一些网站配置项。 对于性能要求高 比如一些秒杀活动场景。 缓存三种模式一般来说，缓存有以下三种模式： 这三种模式各有优劣，可以根据业务场景选择使用。 Cache Aside 更新模式 同时更新缓存和数据库（Cache Aside 更新模式） Read/Write Through 更新模式 先更新缓存，缓存负责同步更新数据库（Read/Write Through 更新模式） Write Behind Caching 更新模式 先更新缓存，缓存定时异步更新数据库（Write Behind Caching 更新模式）优点是直接操作内存速度快，多次操作可以合并持久化到数据库。缺点是数据可能会丢失，例如系统断电等。 Cache Aside 更新模式这是最常用的缓存模式了，具体的流程是： 失效：应用程序先从 cache 取数据，没有得到，则==从数据库中取数据，成功后，放到缓存中==。 命中：应用程序从 cache 中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。 ==为了防止高迸发, 导致数据不一致, 所以直接写入到库中== 注意我们上面所提到的，缓存更新时先更新数据库，然后在让缓存失效。那么为什么不是直接更新缓存呢？这里有一些缓存更新的坑，我们需要避免入坑。 避坑指南一先更新数据库，再更新缓存。这种做法最大的问题就是两个并发的写操作导致脏数据。如下图（以Redis和Mysql为例），两个并发更新操作，数据库先更新的反而后更新缓存，数据库后更新的反而先更新缓存。这样就会造成数据库和缓存中的数据不一致，应用程序中读取的都是脏数据。 避坑指南二先删除缓存，再更新数据库。这个逻辑是错误的，因为两个并发的读和写操作导致脏数据。如下图（以Redis和Mysql为例）。假设更新操作先删除了缓存，此时正好有一个并发的读操作，没有命中缓存后从数据库中取出老数据并且更新回缓存，这个时候更新操作也完成了数据库更新。此时，数据库和缓存中的数据不一致，应用程序中读取的都是原来的数据（脏数据）。 避坑指南三先更新数据库，再删除缓存。这种做法其实不能算是坑，在实际的系统中也推荐使用这种方式。但是这种方式理论上还是可能存在问题。如下图（以Redis和Mysql为例），查询操作没有命中缓存，然后查询出数据库的老数据。此时有一个并发的更新操作，更新操作在读操作之后更新了数据库中的数据并且删除了缓存中的数据。然而读操作将从数据库中读取出的老数据更新回了缓存。这样就会造成数据库和缓存中的数据不一致，应用程序中读取的都是原来的数据（脏数据）。 但是，仔细想一想，这种并发的概率极低。因为这个条件需要发生在读缓存时缓存失效，而且有一个并发的写操作。实际上数据库的写操作会比读操作慢得多，而且还要加锁，而读操作必需在写操作前进入数据库操作，又要晚于写操作更新缓存，所有这些条件都具备的概率并不大。但是为了避免这种极端情况造成脏数据所产生的影响，我们还是要为缓存设置过期时间。 Read/Write Through 更新模式在上面的 Cache Aside 更新模式中，应用代码需要维护两个数据存储，一个是缓存（Cache），一个是数据库（Repository）。而在Read/Write Through 更新模式中，应用程序只需要维护缓存，数据库的维护工作由缓存代理了。 Read ThroughRead Through 模式就是在查询操作中更新缓存，也就是说，当缓存失效的时候，Cache Aside 模式是由调用方负责把数据加载入缓存，而 Read Through 则用缓存服务自己来加载。 Write ThroughWrite Through 模式和 Read Through 相仿，不过是在更新数据时发生。当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后由缓存自己更新数据库（这是一个同步操作）。 Write Behind Caching 更新模式Write Behind Caching 更新模式就是在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。这个设计的好处就是直接操作内存速度快。因为异步，Write Behind Caching 更新模式还可以合并对同一个数据的多次操作到数据库，所以性能的提高是相当可观的。 但==其带来的问题是，数据不是强一致性的，而且可能会丢失==。另外，Write Behind Caching 更新模式实现逻辑比较复杂，因为它需要确认有哪些数据是被更新了的，哪些数据需要刷到持久层上。只有在缓存需要失效的时候，才会把它真正持久起来。 缓存失效策略 一般而言，缓存系统中都会对缓存的对象设置一个超时时间，避免浪费相对比较稀缺的缓存资源。对于缓存时间的处理有两种，分别是主动失效和被动失效。 主动失效主动失效是指系统有一个主动检查缓存是否失效的机制，比如通过定时任务或者单独的线程不断的去检查缓存队列中的对象是否失效，如果失效就把他们清除掉，避免浪费。主动失效的好处是能够避免内存的浪费，但是会占用额外的CPU时间。 被动失效被动失效是通过访问缓存对象的时候才去检查缓存对象是否失效，这样的好处是系统占用的CPU时间更少，但是风险是长期不被访问的缓存对象不会被系统清除。 缓存淘汰策略 缓存淘汰，又称为缓存逐出(cache replacement algorithms或者cache replacement policies)，是指在存储空间不足的情况下，缓存系统主动释放一些缓存对象获取更多的存储空间。一般LRU用的比较多，可以重点了解一下。 FIFO先进先出（First In First Out）是一种简单的淘汰策略，缓存对象以队列的形式存在，如果空间不足，就释放队列头部的（先缓存）对象。一般用链表实现。 LRU最近最久未使用（Least Recently Used），这种策略是根据访问的时间先后来进行淘汰的，如果空间不足，会释放最久没有访问的对象（上次访问时间最早的对象）。比较常见的是通过优先队列来实现。 LFU最近最少使用（Least Frequently Used），这种策略根据最近访问的频率来进行淘汰，如果空间不足，会释放最近访问频率最低的对象。这个算法也是用优先队列实现的比较常见。7 分布式缓存的常见问题缓存穿透 DB中不存在数据，==每次都穿过缓存查DB==，造成DB的压力。一般是网络攻击 解决方案：放入一个特殊对象（比如特定的无效对象，当然比较好的方式是使用包装对象） 12345678910111213141516171819# 我们先看看最简单的青铜姿势value = cache.get(key)if value is None: value = db.get(key) # 由于value为空，实际上缓存并没有写进去，一旦这个key成为热点，db的压力将会极大 cache.put(key, value, expire) return valueelse: return value# 简单优化一下，升级成为白银姿势wrapped_value = cache.get(key)if wrapped_value is None: value = db.get(key) # 即使是空对象也通过包装对象放到缓存，当然考虑到空间还可以采用特殊值（比如-1代表不存在）的方式 cache.put(key, wrapped_value(value), expire) return wrapped_value.valueelse: return wrapped_value.value 缓存击穿 在缓存失效的瞬间大量请求，造成DB的压力瞬间增大 解决方案：更新缓存时使用分布式锁锁住服务，防止请求穿透直达DB 1234567891011121314151617181920212223242526# 白银姿势wrapped_value = cache.get(key)if wrapped_value is None: value = db.get(key) # 在写入缓存之前，大量的请求突然涌入，db瞬间被打垮 cache.put(key, wrapped_value(value), expire) return wrapped_value.valueelse: return wrapped_value.value # 在白银姿势的基础上我们再优化成黄金姿势wrapped_value = cache.get(key)if wrapped_value is None: # 查db之前加一把锁 while wrapped_value is None: if try_lock(key): value = db.get(key) cache.put(key, wrapped_value(value), expire) return wrapped_value.value else: # 等待10毫秒之后重试 sleep(0.01) wrapped_value = cache.get(key) return wrapped_value.valueelse: return wrapped_value.value 缓存雪崩 大量缓存设置了相同的失效时间，同一时间失效，造成服务瞬间性能急剧下降 解决方案：缓存时间使用基本时间加上随机时间 1234567891011121314151617# 通过随机失效时间登上王者姿势wrapped_value = cache.get(key)if wrapped_value is None: # 查db之前加一把锁 while wrapped_value is None: if try_lock(key): value = db.get(key) # 嗯，就是一个随机失效时间，最好是在某个区间 cache.put(key, wrapped_value(value), random_expire()) return wrapped_value.value else: # 等待10毫秒之后重试 sleep(0.01) wrapped_value = cache.get(key) return wrapped_value.valueelse: return wrapped_value.value","categories":[{"name":"缓存","slug":"缓存","permalink":"/categories/%E7%BC%93%E5%AD%98/"}],"tags":[{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"缓存","slug":"缓存","permalink":"/tags/%E7%BC%93%E5%AD%98/"}]},{"title":"Spring面试","slug":"Spring面试","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T06:04:59.622Z","comments":true,"path":"2019/10/20/Spring面试/","link":"","permalink":"/2019/10/20/Spring%E9%9D%A2%E8%AF%95/","excerpt":"","text":"Spring1. Spring是什么?​ Spring是一个轻量级的IoC和AOP容器框架。是为Java应用程序提供基础性服务的一套框架，目的是用于简化企业应用程序的开发，它使得开发者只需要关心业务需求。常见的配置方式有三种：基于XML的配置、基于注解的配置、基于Java的配置。 主要由以下几个模块组成： Spring Core：核心类库，提供IOC服务； Spring Context：提供框架式的Bean访问方式，以及企业级功能（JNDI、定时任务等）； Spring AOP：AOP服务； Spring DAO：对JDBC的抽象，简化了数据访问异常的处理； Spring ORM (对象关系映射)：对现有的ORM框架的支持； Spring Web：提供了基本的面向Web的综合特性，例如多方文件上传； Spring MVC：提供面向Web应用的Model-View-Controller实现。 2. Spring 的优点？（1）spring属于低侵入式设计，代码的污染极低； （2）spring的DI机制将对象之间的依赖关系交由框架处理，减低组件的耦合性； （3）Spring提供了AOP技术，支持将一些通用任务，如安全、事务、日志、权限等进行集中式管理，从而提供更好的复用。 （4）spring对于主流的应用框架提供了集成支持。 3、Spring的AOP理解：OOP面向对象，允许开发者定义纵向的关系，但并适用于定义横向的关系，导致了大量代码的重复，而不利于各个模块的重用。 简单说就是AOP可以在不修改现有代码的情况下对现有代码增加一些功能，那么这就是AOP最强大的功能。 Aspect（切面）： Aspect 声明类似于 Java 中的类声明，在 Aspect 中会包含着一些 Pointcut 以及相应的 Advice。 Joint point（连接点）：表示在程序中明确定义的点，典型的包括方法调用，对类成员的访问以及异常处理程序块的执行等等，它自身还可以嵌套其它 joint point。 Pointcut（切点）：表示一组 joint point，这些 joint point 或是通过逻辑关系组合起来，或是通过通配、正则表达式等方式集中起来，它定义了相应的 Advice 将要发生的地方。 pAdvice（增强）：Advice 定义了在 Pointcut 里面定义的程序点具体要做的操作，它通过 before、after 和 around 来区别是在每个 joint point 之前、之后还是代替执行的代码。 Target（目标对象）：织入 Advice 的目标对象. Weaving（织入）：将 Aspect 和其他对象连接起来, 并创建 AOP，一般称为面向切面，作为面向对象的一种补充，用于将那些与业务无关，但却对多个对象产生影响的公共行为和逻辑，抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。可用于权限认证、日志、事务处理。 ==AOP实现的关键在于 代理模式==，AOP代理主要分为静态代理和动态代理。 静态代理的代表为AspectJ； AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。 动态代理则以Spring AOP为代表。 Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。 Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理： invocation 调用, 启用 ①JDK动态代理只提供接口的代理，不支持类的代理。核心InvocationHandler接口和Proxy类，InvocationHandler 通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy利用 InvocationHandler动态创建一个符合某一接口的的实例, 生成目标类的代理对象。 ②如果代理类没有实现 InvocationHandler 接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。 静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。 InvocationHandler 的 invoke(Object proxy,Method method,Object[] args)：proxy是最终生成的代理实例; method 是被代理目标实例的某个具体方法; args 是被代理目标实例某个方法的具体入参, 在方法反射调用时使用。 4、Spring的IoC理解：（1）IOC就是控制反转，是指创建对象的控制权的转移，以前创建对象的主动权和时机是由自己把控的，而现在这种权力转移到Spring容器中，并由容器根据配置文件去创建实例和管理各个实例之间的依赖关系，对象与对象之间松散耦合，也利于功能的复用。DI依赖注入，和控制反转是同一个概念的不同角度的描述，即 应用程序在运行时依赖IoC容器来动态注入对象需要的外部资源。 （2）最直观的表达就是，IOC让对象的创建不用去new了，可以由spring自动生产，使用java的反射机制，根据配置文件在运行时动态的去创建对象以及管理对象，并调用对象的方法的。 （3）Spring的IOC有三种注入方式 ： 构造器注入 setter方法注入 根据注解注入 IoC让相互协作的组件保持松散的耦合，而AOP编程允许你把遍布于应用各层的功能分离出来形成可重用的功能组件。 5、BeanFactory和ApplicationContext有什么区别？​ BeanFactory和ApplicationContext是Spring的两大核心接口，都可以当做Spring的容器。其中ApplicationContext是BeanFactory的子接口。 （1）BeanFactory：是Spring里面最底层的接口，包含了各种Bean的定义，读取bean配置文档，管理bean的加载、实例化，控制bean的生命周期，维护bean之间的依赖关系。ApplicationContext接口作为BeanFactory的派生，除了提供BeanFactory所具有的功能外，还提供了更完整的框架功能： ①继承MessageSource，因此支持国际化。 ②统一的资源文件访问方式。 ③提供在监听器中注册bean的事件。 ④同时加载多个配置文件。 ⑤载入多个（有继承关系）上下文 ，使得每一个上下文都专注于一个特定的层次，比如应用的web层。 （2）①==BeanFactroy采用的是延迟加载形式来注入Bean的==，即只有在使用到某个Bean时(调用getBean())，才对该Bean进行加载实例化。这样，我们就不能发现一些存在的Spring的配置问题。如果Bean的某一个属性没有注入，BeanFacotry加载后，直至第一次使用调用getBean方法才会抛出异常。 ②==ApplicationContext，它是在容器启动时，一次性创建了所有的Bean==。这样，在容器启动时，我们就可以发现Spring中存在的配置错误，这样有利于检查所依赖属性是否注入。 ApplicationContext启动后预载入所有的单实例Bean，通过预载入单实例bean ,确保当你需要的时候，你就不用等待，因为它们已经创建好了。 ③相对于基本的BeanFactory，ApplicationContext 唯一的不足是占用内存空间。当应用程序配置Bean较多时，程序启动较慢。 （3）BeanFactory通常以编程的方式被创建，ApplicationContext还能以声明的方式创建，如使用ContextLoader。 （4）BeanFactory和ApplicationContext都支持BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory需要手动注册，而ApplicationContext则是自动注册。 6、请解释Spring Bean的生命周期？spring中的核心类答：BeanFactory：产生一个新的实例，可以实现单例模式 BeanWrapper：提供统一的get及set方法 ApplicationContext:提供框架的实现，包括BeanFactory的所有功能 首先说一下Servlet的生命周期： 实例化 初始init 接收请求service 销毁destroy Spring上下文中的Bean生命周期也类似，如下：（1）实例化Bean： 对于BeanFactory容器，当客户向容器请求一个尚未初始化的bean时，或初始化bean的时候需要注入另一个尚未初始化的依赖时，容器就会调用createBean进行实例化。对于ApplicationContext容器，当容器启动结束后，通过获取BeanDefinition对象中的信息，实例化所有的bean。 （2）设置对象属性（依赖注入）：==先调用空的构造器, 然后再依次注入数据的== 实例化后的对象被封装在BeanWrapper对象中，紧接着，Spring根据BeanDefinition中的信息 以及 通过BeanWrapper提供的设置属性的接口完成依赖注入。 （3）处理Aware接口： 接着，Spring会检测该对象是否实现了xxxAware接口，并将相关的xxxAware实例注入给Bean： ①如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String beanId)方法，此处传递的就是Spring配置文件中Bean的id值； ②如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory()方法，传递的是Spring工厂自身。 ③如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文； （4）BeanPostProcessor： 如果想对Bean进行一些自定义的处理，那么可以让Bean实现了BeanPostProcessor接口，那将会调用postProcessBeforeInitialization(Object obj, String s)方法。由于这个方法是在Bean初始化结束时调用的，所以可以被应用于内存或缓存技术； （5）InitializingBean 与 init-method： 如果Bean在Spring配置文件中配置了 init-method 属性，则会自动调用其配置的初始化方法。 （6）如果这个Bean实现了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法； 以上几个步骤完成后，Bean就已经被正确创建了，之后就可以使用这个Bean了。 （7）DisposableBean： 当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用其实现的destroy()方法； （8）destroy-method： 最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。 7、 解释Spring支持的几种bean的作用域。Spring容器中的bean可以分为5个范围： （1）==singleton：默认==，每个容器中只有一个bean的实例，单例的模式由BeanFactory自身来维护。 （2）prototype：为每一个bean请求提供一个实例。(==适用于有状态的, 多线程安全情况==) （3）request：为每一个网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收。 （4）session：与request范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效。 （5）global-session：全局作用域，global-session和Portlet应用相关。当你的应用部署在Portlet容器中工作时，它包含很多portlet。如果你想要声明让所有的portlet共用全局的存储变量的话，那么这全局变量需要存储在global-session中。全局作用域与Servlet中的session作用域效果相同。 8、Spring框架中的单例Beans是线程安全的么？可以视为安全 Spring框架并没有对单例bean进行任何多线程的封装处理。关于单例bean的线程安全和并发问题需要开发者自行去搞定。但实际上，大部分的Spring bean并没有可变的状态(比如Serview类和DAO类)，所以在某种程度上说Spring的单例bean是线程安全的。如果你的bean有多种状态的话（比如 View Model 对象），就需要自行保证线程安全。最浅显的解决办法就是将多态bean的作用域由“singleton”变更为“prototype”。 9、Spring如何处理线程并发问题？ 无状态对象: 即无状态类，是指其本身没有内部变量和外部变量的操作的，在每个用户访问的线程栈中都是一个各自的实例。 在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域，因为==Spring对一些Bean中非线程安全状态采用ThreadLocal进行处理，解决线程安全问题==。 ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。同步机制采用了“时间换空间”的方式，仅提供一份变量，不同的线程在访问前需要获取锁，没获得锁的线程则需要排队。而ThreadLocal采用了“空间换时间”的方式。 ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。 10-1、Spring基于xml注入bean的几种方式：（1）Set方法注入；set方法（这是ioc的注入入口） （2）构造器注入：这种方式的注入是指带有参数的构造函数注入 public SpringAction(SpringDao springDao,User user){}①通过index设置参数的位置； &lt;bean name=&quot;springAction&quot; class=&quot;com.bless.springdemo.action.SpringAction&quot;&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;springDao&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg index=&quot;1&quot; ref=&quot;user&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; 123456789101112- ②通过type设置参数类型； ```xml &lt;!--配置bean,配置后该类由spring管理--&gt; &lt;bean name=&quot;springAction&quot; class=&quot;com.bless.springdemo.action.SpringAction&quot;&gt; &lt;!--(2)创建构造器注入,如果主类有带参的构造方法则需添加此配置--&gt; &lt;constructor-arg ref=&quot;springDao&quot;&gt;&lt;/constructor-arg&gt; &lt;constructor-arg ref=&quot;user&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean name=&quot;springDao&quot; class=&quot;com.bless.springdemo.dao.impl.SpringDaoImpl&quot;&gt;&lt;/bean&gt; &lt;bean name=&quot;user&quot; class=&quot;com.bless.springdemo.vo.User&quot;&gt;&lt;/bean&gt; （3）静态工厂注入；就是通过调用静态工厂的方法来获取自己需要的对象，为了让spring管理所有对象，我们不能直接通过”工程类.静态方法()”来获取对象，而是依然通过spring注入的形式获取 123456789101112package com.bless.springdemo.factory; import com.bless.springdemo.dao.FactoryDao; import com.bless.springdemo.dao.impl.FactoryDaoImpl; import com.bless.springdemo.dao.impl.StaticFacotryDaoImpl; public class DaoFactory &#123; //静态工厂 public static final FactoryDao getStaticFactoryDaoImpl()&#123; return new StaticFacotryDaoImpl(); &#125; &#125; 12345678&lt;!--配置bean,配置后该类由spring管理--&gt; &lt;bean name=\"springAction\" class=\"com.bless.springdemo.action.SpringAction\" &gt; &lt;!--(3)使用静态工厂的方法注入对象,对应下面的配置文件(3)--&gt; &lt;property name=\"staticFactoryDao\" ref=\"staticFactoryDao\"&gt;&lt;/property&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!--(3)此处获取对象的方式是从工厂类中获取静态方法--&gt; &lt;bean name=\"staticFactoryDao\" class=\"com.bless.springdemo.factory.DaoFactory\" factory-method=\"getStaticFactoryDaoImpl\"&gt;&lt;/bean&gt; （4）实例工厂；先new 工厂类, 再通过工厂类new 实例 123456789&lt;!--配置bean,配置后该类由spring管理--&gt; &lt;bean name=\"springAction\" class=\"com.bless.springdemo.action.SpringAction\"&gt; &lt;!--(4)使用实例工厂的方法注入对象,对应下面的配置文件(4)--&gt; &lt;property name=\"factoryDao\" ref=\"factoryDao\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--(4)此处获取对象的方式是从工厂类中获取实例方法--&gt; &lt;bean name=\"daoFactory\" class=\"com.bless.springdemo.factory.DaoFactory\"&gt;&lt;/bean&gt; &lt;bean name=\"factoryDao\" factory-bean=\"daoFactory\" factory-method=\"getFactoryDaoImpl\"&gt;&lt;/bean&gt; 详细内容可以阅读：https://blog.csdn.net/a745233700/article/details/89307518 10-2、Spring的自动装配：在spring中，对象无需自己查找或创建与其关联的其他对象，由容器负责把需要相互协作的对象引用赋予各个对象，使用autowire来配置自动装载模式。 在Spring框架xml配置中共有5种自动装配： （1）no：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean。 （2）byName：通过bean的名称进行自动装配，如果一个bean的 property 与另一bean 的name 相同，就进行自动装配。 （3）byType：通过参数的数据类型进行自动装配。 （4）constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。 （5）autodetect：自动探测，如果有构造方法，通过 construct的方式自动装配，否则使用 byType的方式自动装配。 基于注解的方式：使用@Autowired注解来自动装配指定的bean。在使用@Autowired注解之前需要在Spring配置文件进行配置，&lt;context:annotation-config /&gt;。在启动spring IoC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowied、@Resource或@Inject时，就会在IoC容器自动查找需要的bean，并装配给该对象的属性。在使用@Autowired时，首先在容器中查询对应类型的bean： 如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据； 如果查询的结果不止一个，那么@Autowired会根据名称来查找； 如果上述查找的结果为空，那么会抛出异常。解决方法时，使用required=false。 @Autowired可用于：构造函数、成员变量、Setter方法 注：@Autowired和@Resource之间的区别(1) @Autowired默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。 (2) @Resource默认是按照名称来装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入。 11、Spring 框架中都用到了哪些设计模式？!!!!! （1）工厂模式：BeanFactory就是简单工厂模式的体现，用来创建对象的实例； （2）单例模式：Bean默认为单例模式。 （3）代理模式：Spring的AOP功能用到了JDK的动态代理和CGLIB字节码生成技术； （4）模板方法：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。 （5）观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知被制动更新，如Spring中listener的实现–ApplicationListener。 12、Spring事务的实现方式和实现原理：Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。 （1）Spring事务的种类：spring支持编程式事务管理和声明式事务管理两种方式：编程式,顾名思义需要体现在业务代码的编写中, 而声明式相当于注解的方式, 写上注解之后,spring框架利用aop技术进行事务管理 ①编程式事务管理使用TransactionTemplate。 ②声明式事务管理建立在AOP之上的。其本质是通过AOP功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。 声明式事务最大的优点就是不需要在业务逻辑代码中掺杂事务管理的代码，只需在配置文件中做相关的事务规则声明或通过@Transactional注解的方式，便可以将事务规则应用到业务逻辑中。 声明式事务管理要优于编程式事务管理，这正是spring倡导的非侵入式的开发方式，使业务代码不受污染，只要加上注解就可以获得完全的事务支持。唯一不足地方是，最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。 （2）spring的事务传播行为：propagation 传播, 继承 spring事务的传播行为说的是，当多个事务同时存在的时候，spring如何处理这些事务的行为。 ==① PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。== ② PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。‘ ③ PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 ==④ PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。== ⑤ PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 ⑥ PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。 ==⑦ PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。== （3）Spring中的隔离级别： ① ISOLATION_DEFAULT：这是个 PlatfromTransactionManager 默认的隔离级别，使用数据库默认的事务隔离级别。 ② ISOLATION_READ_UNCOMMITTED：读未提交，允许另外一个事务可以看到这个事务未提交的数据。 ③ ISOLATION_READ_COMMITTED：读已提交，保证一个事务修改的数据提交后才能被另一事务读取，而且能看到该事务对已有记录的更新。 ④ ISOLATION_REPEATABLE_READ：可重复读，保证一个事务修改的数据提交后才能被另一事务读取，但是不能看到该事务对已有记录的更新。 ⑤ ISOLATION_SERIALIZABLE：一个事务在执行的过程中完全看不到其他事务对数据库所做的更新。 13、Spring框架中有哪些不同类型的事件？Spring 提供了以下5种标准的事件： （1）上下文更新事件（ContextRefreshedEvent）：在调用ConfigurableApplicationContext 接口中的refresh()方法时被触发。 （2）上下文开始事件（ContextStartedEvent）：当容器调用ConfigurableApplicationContext的Start()方法开始/重新开始容器时触发该事件。 （3）上下文停止事件（ContextStoppedEvent）：当容器调用ConfigurableApplicationContext的Stop()方法停止容器时触发该事件。 （4）上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。 （5）请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。 如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。 14、解释一下Spring AOP里面的几个名词： （1）切面（Aspect）：被抽取的公共模块，可能会横切多个对象。 在Spring AOP中，切面可以使用通用类（基于模式的风格） 或者在普通类中以 @AspectJ 注解来实现。 （2）连接点（Join point）：指方法，在Spring AOP中，一个连接点 总是 代表一个方法的执行。 （3）通知（Advice）：在切面的某个特定的连接点（Join point）上执行的动作。通知有各种类型，其中包括“around”、“before”和“after”等通知。许多AOP框架，包括Spring，都是以拦截器做通知模型， 并维护一个以连接点为中心的拦截器链。 （4）切入点（Pointcut）：切入点是指 我们要对哪些Join point进行拦截的定义。通过切入点表达式，指定拦截的方法，比如指定拦截add、search。 （5）引入（Introduction）：（也被称为内部类型声明（inter-type declaration））。声明额外的方法或者某个类型的字段。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用一个引入来使bean实现 IsModified 接口，以便简化缓存机制。 （6）目标对象（Target Object）： 被一个或者多个切面（aspect）所通知（advise）的对象。也有人把它叫做 被通知（adviced） 对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个 被代理（proxied） 对象。 （7）织入（Weaving）：指把增强应用到目标对象来创建新的代理对象的过程。Spring是在运行时完成织入。 切入点（pointcut）和连接点（join point）匹配的概念是AOP的关键，这使得AOP不同于其它仅仅提供拦截功能的旧技术。 切入点使得定位通知（advice）可独立于OO层次。 例如，一个提供声明式事务管理的around通知可以被应用到一组横跨多个对象中的方法上（例如服务层的所有业务操作）。 15、Spring通知有哪些类型？https://blog.csdn.net/qq_32331073/article/details/80596084 （1）前置通知（Before advice）：在某连接点（join point）之前执行的通知，但这个通知不能阻止连接点前的执行（除非它抛出一个异常）。 （2）返回后通知（After returning advice）：在某连接点（join point）正常完成后执行的通知：例如，一个方法没有抛出任何异常，正常返回。 （3）抛出异常后通知（After throwing advice）：在方法抛出异常退出时执行的通知。 （4）后通知（After (finally) advice）：当某连接点退出的时候执行的通知（不论是正常返回还是异常退出）。 （5）环绕通知（Around Advice）：包围一个连接点（join point）的通知，如方法调用。这是最强大的一种通知类型。 环绕通知可以在方法调用前后完成自定义的行为。它也会选择是否继续执行连接点或直接返回它们自己的返回值或抛出异常来结束执行。 环绕通知是最常用的一种通知类型。大部分基于拦截的AOP框架，例如Nanning和JBoss4，都只提供环绕通知。 同一个aspect，不同advice的执行顺序： ①没有异常情况下的执行顺序： *around *before advice before advice target method 执行 around after advice after advice afterReturning ②有异常情况下的执行顺序： around before advice before advice target method 执行 around** after advice after advice afterThrowing:异常发生 java.lang.RuntimeException: 异常发生 Springmvc[TOC] 1、什么是Spring MVC ？简单介绍下你对springMVC的理解?Spring MVC是一个基于Java的实现了MVC设计模式的请求驱动类型的轻量级Web框架，通过把Model，View，Controller分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。 2、SpringMVC的流程？（1）用户发送请求至前端控制器DispatcherServlet；（2） DispatcherServlet收到请求后，调用HandlerMapping处理器映射器，请求获取Handle；（3）处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet；（4）DispatcherServlet 调用 HandlerAdapter处理器适配器；（5）HandlerAdapter 经过适配调用 具体处理器(Handler，也叫后端控制器)；（6）Handler执行完成返回ModelAndView；（7）HandlerAdapter将Handler执行结果ModelAndView返回给DispatcherServlet；（8）DispatcherServlet将ModelAndView传给ViewResolver视图解析器进行解析；（9）ViewResolver解析后返回具体View；（10）DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中）（11）DispatcherServlet响应用户。 3、Springmvc的优点:（1）可以支持各种视图技术,而不仅仅局限于JSP； （2）与Spring框架集成（如IoC容器、AOP等）； （3）清晰的角色分配：前端控制器(dispatcherServlet) , 请求到处理器映射（handlerMapping), 处理器适配器（HandlerAdapter), 视图解析器（ViewResolver）。 （4） 支持各种请求资源的映射策略。 4、Spring MVC的主要组件？（1）前端控制器 DispatcherServlet（不需要程序员开发） 作用：接收请求、响应结果，相当于转发器，有了DispatcherServlet 就减少了其它组件之间的耦合度。 （2）处理器映射器HandlerMapping（不需要程序员开发） 作用：根据请求的URL来查找Handler （3）==处理器适配器HandlerAdapter== 注意：在编写Handler的时候要按照HandlerAdapter要求的规则去编写，这样适配器HandlerAdapter才可以正确的去执行Handler。 （4）==处理器Handler（需要程序员开发）== （5）视图解析器 ViewResolver（不需要程序员开发） 作用：进行视图的解析，根据视图逻辑名解析成真正的视图（view） （6）==视图View（需要程序员开发jsp）== View是一个接口， 它的实现类支持不同的视图类型（jsp，freemarker，pdf等等） 5、springMVC和struts2的区别有哪些?（1）springmvc的入口是一个servlet即前端控制器（DispatchServlet），而struts2入口是一个filter过虑器（StrutsPrepareAndExecuteFilter）。 （2）springmvc是基于方法开发(一个url对应一个方法)，请求参数传递到方法的形参，可以设计为单例或多例(建议单例)，struts2是基于类开发，传递参数是通过类的属性，只能设计为多例。 （3）Struts采用值栈存储请求和响应的数据，通过OGNL存取数据，springmvc通过参数解析器是将request请求内容解析，并给方法形参赋值，将数据和视图封装成ModelAndView对象，最后又将ModelAndView中的模型数据通过reques域传输到页面。Jsp视图解析器默认使用jstl。 6、SpringMVC怎么样设定重定向和转发的？（1）转发：在返回值前面加”forward:“，譬如”forward:user.do?name=method4” （2）重定向：在返回值前面加”redirect:“，譬如”redirect:http://www.baidu.com&quot; 7、SpringMvc怎么和AJAX相互调用的？通过Jackson框架就可以把Java里面的对象直接转化成Js可以识别的Json对象。具体步骤如下 ： （1）加入Jackson.jar, 需要相应的依赖包 （2）在配置文件中配置json的映射, 配置json数据的映射 （3）在接受Ajax方法里面可以直接返回Object,List等,但方法前面要加上@ResponseBody注解。 8、如何解决POST请求中文乱码问题，GET的又如何处理呢？（1）解决post请求乱码问题：在web.xml中配置一个CharacterEncodingFilter过滤器，设置成utf-8； 12345678910111213&lt;filter&gt;&lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt;&lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;&lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt;&lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt;&lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;!--过滤全局的请求, 都把字符集设为utf-8 --&gt;&lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; （2）get请求中文参数出现乱码解决方法有两个：①修改tomcat配置文件添加编码与工程编码一致，如下： &lt;ConnectorURIEncoding=”utf-8” connectionTimeout=”20000” port=”8080” protocol=”HTTP/1.1” redirectPort=”8443”/&gt; ②另外一种方法对参数进行重新编码： String userName = new String(request.getParamter(“userName”).getBytes(“ISO8859-1”),”utf-8”) ISO8859-1是tomcat默认编码，需要将tomcat编码后的内容按utf-8编码。 9、Spring MVC的异常处理 ？答：可以将异常抛给Spring框架，由Spring框架来处理；我们只需要配置简单的异常处理器，在异常处理器中添视图页面即可。 10、SpringMvc的控制器是不是单例模式,如果是,有什么问题,怎么解决？!!!答：是单例模式,所以在多线程访问的时候有线程安全问题,不要用同步,会影响性能的,解决方案是在控制器里面不能写字段。 不写字段就不会存在,有状态的资源共享, 就不会有线程安全问题 11、 SpringMVC常用的注解有哪些？@RequestMapping：用于处理请求 url 映射的注解，可用于类或方法上。用于类上，则表示类中的所有响应请求的方法都是以该地址作为父路径。 @RequestBody：注解实现接收http请求的json数据，将json转换为java对象。 @ResponseBody：注解实现将conreoller方法返回对象转化为json对象响应给客户。 12、SpingMvc中的控制器的注解一般用那个,有没有别的注解可以替代？答：一般用@Conntroller注解,表示是表现层,不能用别的注解代替。 13、如果在拦截请求中，我想拦截get方式提交的方法,怎么配置？答：可以在@RequestMapping注解里面加上method=RequestMethod.GET。 14、怎样在方法里面得到Request,或者Session？答：直接在方法的形参中声明request,SpringMvc就自动把request对象传入。 15、如果想在拦截的方法里面得到从前台传入的参数,怎么得到？答：直接在形参里面声明这个参数就可以,但必须名字和传过来的参数一样。==spirngmvc通过name属性来注入, 不是通过html中的id熟悉== 16、如果前台有很多个参数传入,并且这些参数都是一个对象的,那么怎么样快速得到这个对象？答：直接在方法中声明这个对象,SpringMvc就自动会把属性赋值到这个对象里面。 17、SpringMvc中函数的返回值是什么？答：返回值可以有很多类型,有String, ModelAndView。ModelAndView类把视图和数据都合并的一起的，但一般用String比较好, ==如果加了json的支持, 可以返回任何类型;== 18、SpringMvc用什么对象从后台向前台传递数据的？答：通过ModelMap对象,可以在这个对象里面调用put方法,把对象加到里面,前台就可以通过el表达式拿到。 19、怎么样把ModelMap里面的数据放入Session里面？答：可以在类上面加上@SessionAttributes注解,里面包含的字符串就是要放入session里面的key。 20、SpringMvc里面拦截器是怎么写的： 现HandlerInterceptor接口, 接着在接口方法当中，实现处理逻辑； 继承适配器类, 重写相应的方法 然后在SpringMvc的配置文件中配置拦截器即可： 123456789&lt;mvc:interceptors&gt;&lt;!-- 配置一个拦截器的Bean就可以了 默认是对所有请求都拦截 --&gt; &lt;bean id=\"myInterceptor\" class=\"com.zwp.action.MyHandlerInterceptor\"&gt;&lt;/bean&gt;&lt;!-- 只针对部分请求拦截 --&gt;&lt;mvc:interceptor&gt; &lt;mvc:mapping path=\"/modelMap.do\" /&gt; &lt;bean class=\"com.zwp.action.MyHandlerInterceptorAdapter\" /&gt;&lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt;&gt; 21、注解原理：注解本质是一个继承了Annotation的特殊接口，其具体实现类是Java运行时生成的动态代理类。我们通过反射获取注解时，返回的是Java运行时生成的动态代理对象。通过代理对象调用自定义注解的方法，会最终调用AnnotationInvocationHandler的invoke方法。该方法会从memberValues这个Map中索引出对应的值。而memberValues的来源是Java常量池。","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"},{"name":"服务端开发","slug":"服务端开发","permalink":"/tags/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"Spring","slug":"Spring","permalink":"/tags/Spring/"}]},{"title":"计算机网络","slug":"计算机网络","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T05:56:58.326Z","comments":true,"path":"2019/10/20/计算机网络/","link":"","permalink":"/2019/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","excerpt":"","text":"URIURI 包含 URL 和 URN。 请求和响应报文1. 请求报文 2. 响应报文 HTTP 方法 客户端发送的 请求报文 第一行为请求行，包含了方法字段。 GET 获取资源 当前网络请求中，绝大部分使用的是 GET 方法。 HEAD 获取报文首部 和 GET 方法类似，但是不返回报文实体主体部分。 主要用于确认 URL 的有效性以及资源更新的日期时间等。 POST 传输实体主体 POST 主要用来传输数据，而 GET 主要用来获取资源。 更多 POST 与 GET 的比较请见第九章。 PUT 上传文件 由于自身不带验证机制，任何人都可以上传文件，因此存在安全性问题，一般不使用该方法。 123456PUT /new.html HTTP/1.1Host: example.comContent-type: text/htmlContent-length: 16&lt;p&gt;New File&lt;/p&gt;Copy to clipboardErrorCopied PATCH 对资源进行部分修改 PUT 也可以用于修改资源，但是只能完全替代原始资源，PATCH 允许部分修改。 1234567PATCH /file.txt HTTP/1.1Host: www.example.comContent-Type: application/exampleIf-Match: \"e0023aa4e\"Content-Length: 100[description of changes]Copy to clipboardErrorCopied DELETE 删除文件 与 PUT 功能相反，并且同样不带验证机制。 1DELETE /file.html HTTP/1.1Copy to clipboardErrorCopied OPTIONS 查询支持的方法 查询指定的 URL 能够支持的方法。 会返回 Allow: GET, POST, HEAD, OPTIONS 这样的内容。 CONNECT 要求在与代理服务器通信时建立隧道 使用 SSL（Secure Sockets Layer，安全套接层）和 TLS（Transport Layer Security，传输层安全）协议把通信内容加密后经网络隧道传输。 1CONNECT www.example.com:443 HTTP/1.1Copy to clipboardErrorCopied TRACE 追踪路径 服务器会将通信路径返回给客户端。 发送请求时，在 Max-Forwards 首部字段中填入数值，每经过一个服务器就会减 1，当数值为 0 时就停止传输。 通常不会使用 TRACE，并且它容易受到 XST 攻击（Cross-Site Tracing，跨站追踪）。 HTTP 状态码服务器返回的 响应报文 中第一行为状态行，包含了状态码以及原因短语，用来告知客户端请求的结果。 状态码 类别 含义 1XX Informational（信息性状态码） 接收的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 1XX 信息 100 Continue ：表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。 2XX 成功 200 OK 204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端往服务器发送信息，而不需要返回数据时使用。 206 Partial Content ：表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。 3XX 重定向 301 Moved Permanently ：永久性重定向 302 Found ：临时性重定向 303 See Other ：和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。 注：虽然 HTTP 协议规定 301、302 状态下重定向时不允许把 POST 方法改成 GET 方法，但是大多数浏览器都会在 301、302 和 303 状态下的重定向把 POST 方法改成 GET 方法。 304 Not Modified ：如果请求报文首部包含一些条件，例如：If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。 307 Temporary Redirect ：临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。 4XX 客户端错误 400 Bad Request ：请求报文中存在语法错误。 401 Unauthorized ：该状态码表示发送的请求需要有认证信息（BASIC 认证、DIGEST 认证）。如果之前已进行过一次请求，则表示用户认证失败。 403 Forbidden ：请求被拒绝。 404 Not Found 5XX 服务器错误 500 Internal Server Error ：服务器正在执行请求时发生错误。 503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。 四、HTTP 首部有 4 种类型的首部字段：通用首部字段、请求首部字段、响应首部字段和实体首部字段。 各种首部字段及其含义如下（不需要全记，仅供查阅）： 通用首部字段 首部字段名 说明 Cache-Control 控制缓存的行为 Connection 控制不再转发给代理的首部字段、管理持久连接 Date 创建报文的日期时间 Pragma 报文指令 Trailer 报文末端的首部一览 Transfer-Encoding 指定报文主体的传输编码方式 Upgrade 升级为其他协议 Via 代理服务器的相关信息 Warning 错误通知 请求首部字段 首部字段名 说明 Accept 用户代理可处理的媒体类型 Accept-Charset 优先的字符集 Accept-Encoding 优先的内容编码 Accept-Language 优先的语言（自然语言） Authorization Web 认证信息 Expect 期待服务器的特定行为 From 用户的电子邮箱地址 Host 请求资源所在服务器 If-Match 比较实体标记（ETag） If-Modified-Since 比较资源的更新时间 If-None-Match 比较实体标记（与 If-Match 相反） If-Range 资源未更新时发送实体 Byte 的范围请求 If-Unmodified-Since 比较资源的更新时间（与 If-Modified-Since 相反） Max-Forwards 最大传输逐跳数 Proxy-Authorization 代理服务器要求客户端的认证信息 Range 实体的字节范围请求 Referer 对请求中 URI 的原始获取方 TE 传输编码的优先级 User-Agent HTTP 客户端程序的信息 响应首部字段 首部字段名 说明 Accept-Ranges 是否接受字节范围请求 Age 推算资源创建经过时间 ETag 资源的匹配信息 Location 令客户端重定向至指定 URI Proxy-Authenticate 代理服务器对客户端的认证信息 Retry-After 对再次发起请求的时机要求 Server HTTP 服务器的安装信息 Vary 代理服务器缓存的管理信息 WWW-Authenticate 服务器对客户端的认证信息 实体首部字段 首部字段名 说明 Allow 资源可支持的 HTTP 方法 Content-Encoding 实体主体适用的编码方式 Content-Language 实体主体的自然语言 Content-Length 实体主体的大小 Content-Location 替代对应资源的 URI Content-MD5 实体主体的报文摘要 Content-Range 实体主体的位置范围 Content-Type 实体主体的媒体类型 Expires 实体主体过期的日期时间 Last-Modified 资源的最后修改日期时间 五、具体应用连接管理 1. 短连接与长连接当浏览器访问一个包含多张图片的 HTML 页面时，除了请求访问的 HTML 页面资源，还会请求图片资源。如果每进行一次 HTTP 通信就要新建一个 TCP 连接，那么开销会很大。 长连接只需要建立一次 TCP 连接就能进行多次 HTTP 通信。 从 HTTP/1.1 开始默认是长连接的，如果要断开连接，需要由客户端或者服务器端提出断开，使用 Connection : close； 在 HTTP/1.1 之前默认是短连接的，如果需要使用长连接，则使用 Connection : Keep-Alive。 2. 流水线默认情况下，HTTP 请求是按顺序发出的，下一个请求只有在当前请求收到响应之后才会被发出。由于受到网络延迟和带宽的限制，在下一个请求被发送到服务器之前，可能需要等待很长时间。 流水线是在同一条长连接上连续发出请求，而不用等待响应返回，这样可以减少延迟。 CookieHTTP 协议是无状态的，主要是为了让 HTTP 协议尽可能简单，使得它能够处理大量事务。HTTP/1.1 引入 Cookie 来保存状态信息。 Cookie 是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器之后向同一服务器再次发起请求时被携带上，用于告知服务端两个请求是否来自同一浏览器。由于之后每次请求都会需要携带 Cookie 数据，因此会带来额外的性能开销（尤其是在移动环境下）。 Cookie 曾一度用于客户端数据的存储，因为当时并没有其它合适的存储办法而作为唯一的存储手段，但现在随着现代浏览器开始支持各种各样的存储方式，Cookie 渐渐被淘汰。新的浏览器 API 已经允许开发者直接将数据存储到本地，如使用 Web storage API（本地存储和会话存储）或 IndexedDB。 1. 用途 会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息） 个性化设置（如用户自定义设置、主题等） 浏览器行为跟踪（如跟踪分析用户行为等） 2. 创建过程服务器发送的响应报文包含 Set-Cookie 首部字段，客户端得到响应报文后把 Cookie 内容保存到浏览器中。 123456HTTP/1.0 200 OKContent-type: text/htmlSet-Cookie: yummy_cookie=chocoSet-Cookie: tasty_cookie=strawberry[page content]Copy to clipboardErrorCopied 客户端之后对同一个服务器发送请求时，会从浏览器中取出 Cookie 信息并通过 Cookie 请求首部字段发送给服务器。 123GET /sample_page.html HTTP/1.1Host: www.example.orgCookie: yummy_cookie=choco; tasty_cookie=strawberryCopy to clipboardErrorCopied 3. 分类 会话期 Cookie：浏览器关闭之后它会被自动删除，也就是说它仅在会话期内有效。 持久性 Cookie：指定过期时间（Expires）或有效期（max-age）之后就成为了持久性的 Cookie。 1Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT;Copy to clipboardErrorCopied 4. 作用域Domain 标识指定了哪些主机可以接受 Cookie。如果不指定，默认为当前文档的主机（不包含子域名）。如果指定了 Domain，则一般包含子域名。例如，如果设置 Domain=mozilla.org，则 Cookie 也包含在子域名中（如 developer.mozilla.org）。 Path 标识指定了主机下的哪些路径可以接受 Cookie（该 URL 路径必须存在于请求 URL 中）。以字符 %x2F (“/“) 作为路径分隔符，子路径也会被匹配。例如，设置 Path=/docs，则以下地址都会匹配： /docs /docs/Web/ /docs/Web/HTTP 5. JavaScript浏览器通过 document.cookie 属性可创建新的 Cookie，也可通过该属性访问非 HttpOnly 标记的 Cookie。 123document.cookie = \"yummy_cookie=choco\";document.cookie = \"tasty_cookie=strawberry\";console.log(document.cookie);Copy to clipboardErrorCopied 6. HttpOnly标记为 HttpOnly 的 Cookie 不能被 JavaScript 脚本调用。跨站脚本攻击 (XSS) 常常使用 JavaScript 的 document.cookie API 窃取用户的 Cookie 信息，因此使用 HttpOnly 标记可以在一定程度上避免 XSS 攻击。 1Set-Cookie: id=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT; Secure; HttpOnlyCopy to clipboardErrorCopied 7. Secure标记为 Secure 的 Cookie 只能通过被 HTTPS 协议加密过的请求发送给服务端。但即便设置了 Secure 标记，敏感信息也不应该通过 Cookie 传输，因为 Cookie 有其固有的不安全性，Secure 标记也无法提供确实的安全保障。 8. Session除了可以将用户信息通过 Cookie 存储在用户浏览器中，也可以利用 Session 存储在服务器端，存储在服务器端的信息更加安全。 Session 可以存储在服务器上的文件、数据库或者内存中。也可以将 Session 存储在 Redis 这种内存型数据库中，效率会更高。 使用 Session 维护用户登录状态的过程如下： 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中； 服务器验证该用户名和密码，如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID； 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中； 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。 应该注意 Session ID 的安全性问题，不能让它被恶意攻击者轻易获取，那么就不能产生一个容易被猜到的 Session ID 值。此外，还需要经常重新生成 Session ID。在对安全性要求极高的场景下，例如转账等操作，除了使用 Session 管理用户状态之外，还需要对用户进行重新验证，比如重新输入密码，或者使用短信验证码等方式。 9. 浏览器禁用 Cookie此时无法使用 Cookie 来保存用户信息，只能使用 Session。除此之外，不能再将 Session ID 存放到 Cookie 中，而是使用 URL 重写技术，将 Session ID 作为 URL 的参数进行传递。 10. Cookie 与 Session 选择 Cookie 只能存储 ASCII 码字符串，而 Session 则可以存储任何类型的数据，因此在考虑数据复杂性时首选 Session； Cookie 存储在浏览器中，容易被恶意查看。如果非要将一些隐私数据存在 Cookie 中，可以将 Cookie 值进行加密，然后在服务器进行解密； 对于大型网站，如果用户所有的信息都存储在 Session 中，那么开销是非常大的，因此不建议将所有的用户信息都存储到 Session 中。 缓存1. 优点 缓解服务器压力； 降低客户端获取资源的延迟：缓存通常位于内存中，读取缓存的速度更快。并且缓存服务器在地理位置上也有可能比源服务器来得近，例如浏览器缓存。 2. 实现方法 让代理服务器进行缓存； 让客户端浏览器进行缓存。 3. Cache-ControlHTTP/1.1 通过 Cache-Control 首部字段来控制缓存。 3.1 禁止进行缓存 no-store 指令规定不能对请求或响应的任何一部分进行缓存。 1Cache-Control: no-storeCopy to clipboardErrorCopied 3.2 强制确认缓存 no-cache 指令规定缓存服务器需要先向源服务器验证缓存资源的有效性，只有当缓存资源有效时才能使用该缓存对客户端的请求进行响应。 1Cache-Control: no-cacheCopy to clipboardErrorCopied 3.3 私有缓存和公共缓存 private 指令规定了将资源作为私有缓存，只能被单独用户使用，一般存储在用户浏览器中。 1Cache-Control: privateCopy to clipboardErrorCopied public 指令规定了将资源作为公共缓存，可以被多个用户使用，一般存储在代理服务器中。 1Cache-Control: publicCopy to clipboardErrorCopied 3.4 缓存过期机制 max-age 指令出现在请求报文，并且缓存资源的缓存时间小于该指令指定的时间，那么就能接受该缓存。 max-age 指令出现在响应报文，表示缓存资源在缓存服务器中保存的时间。 1Cache-Control: max-age=31536000Copy to clipboardErrorCopied Expires 首部字段也可以用于告知缓存服务器该资源什么时候会过期。 1Expires: Wed, 04 Jul 2012 08:26:05 GMTCopy to clipboardErrorCopied 在 HTTP/1.1 中，会优先处理 max-age 指令； 在 HTTP/1.0 中，max-age 指令会被忽略掉。 4. 缓存验证需要先了解 ETag 首部字段的含义，它是资源的唯一标识。URL 不能唯一表示资源，例如 http://www.google.com/ 有中文和英文两个资源，只有 ETag 才能对这两个资源进行唯一标识。 1ETag: \"82e22293907ce725faf67773957acd12\"Copy to clipboardErrorCopied 可以将缓存资源的 ETag 值放入 If-None-Match 首部，服务器收到该请求后，判断缓存资源的 ETag 值和资源的最新 ETag 值是否一致，如果一致则表示缓存资源有效，返回 304 Not Modified。 1If-None-Match: \"82e22293907ce725faf67773957acd12\"Copy to clipboardErrorCopied Last-Modified 首部字段也可以用于缓存验证，它包含在源服务器发送的响应报文中，指示源服务器对资源的最后修改时间。但是它是一种弱校验器，因为只能精确到一秒，所以它通常作为 ETag 的备用方案。如果响应首部字段里含有这个信息，客户端可以在后续的请求中带上 If-Modified-Since 来验证缓存。服务器只在所请求的资源在给定的日期时间之后对内容进行过修改的情况下才会将资源返回，状态码为 200 OK。如果请求的资源从那时起未经修改，那么返回一个不带有实体主体的 304 Not Modified 响应报文。 12Last-Modified: Wed, 21 Oct 2015 07:28:00 GMTCopy to clipboardErrorCopiedIf-Modified-Since: Wed, 21 Oct 2015 07:28:00 GMTCopy to clipboardErrorCopied 内容协商通过内容协商返回最合适的内容，例如根据浏览器的默认语言选择返回中文界面还是英文界面。 1. 类型1.1 服务端驱动型 客户端设置特定的 HTTP 首部字段，例如 Accept、Accept-Charset、Accept-Encoding、Accept-Language，服务器根据这些字段返回特定的资源。 它存在以下问题： 服务器很难知道客户端浏览器的全部信息； 客户端提供的信息相当冗长（HTTP/2 协议的首部压缩机制缓解了这个问题），并且存在隐私风险（HTTP 指纹识别技术）； 给定的资源需要返回不同的展现形式，共享缓存的效率会降低，而服务器端的实现会越来越复杂。 1.2 代理驱动型 服务器返回 300 Multiple Choices 或者 406 Not Acceptable，客户端从中选出最合适的那个资源。 2. Vary1Vary: Accept-LanguageCopy to clipboardErrorCopied 在使用内容协商的情况下，只有当缓存服务器中的缓存满足内容协商条件时，才能使用该缓存，否则应该向源服务器请求该资源。 例如，一个客户端发送了一个包含 Accept-Language 首部字段的请求之后，源服务器返回的响应包含 Vary: Accept-Language 内容，缓存服务器对这个响应进行缓存之后，在客户端下一次访问同一个 URL 资源，并且 Accept-Language 与缓存中的对应的值相同时才会返回该缓存。 内容编码内容编码将实体主体进行压缩，从而减少传输的数据量。 常用的内容编码有：gzip、compress、deflate、identity。 浏览器发送 Accept-Encoding 首部，其中包含有它所支持的压缩算法，以及各自的优先级。服务器则从中选择一种，使用该算法对响应的消息主体进行压缩，并且发送 Content-Encoding 首部来告知浏览器它选择了哪一种算法。由于该内容协商过程是基于编码类型来选择资源的展现形式的，响应报文的 Vary 首部字段至少要包含 Content-Encoding。 范围请求如果网络出现中断，服务器只发送了一部分数据，范围请求可以使得客户端只请求服务器未发送的那部分数据，从而避免服务器重新发送所有数据。 1. Range在请求报文中添加 Range 首部字段指定请求的范围。 123GET /z4d4kWk.jpg HTTP/1.1Host: i.imgur.comRange: bytes=0-1023Copy to clipboardErrorCopied 请求成功的话服务器返回的响应包含 206 Partial Content 状态码。 12345HTTP/1.1 206 Partial ContentContent-Range: bytes 0-1023/146515Content-Length: 1024...(binary content)Copy to clipboardErrorCopied 2. Accept-Ranges响应首部字段 Accept-Ranges 用于告知客户端是否能处理范围请求，可以处理使用 bytes，否则使用 none。 1Accept-Ranges: bytesCopy to clipboardErrorCopied 3. 响应状态码 在请求成功的情况下，服务器会返回 206 Partial Content 状态码。 在请求的范围越界的情况下，服务器会返回 416 Requested Range Not Satisfiable 状态码。 在不支持范围请求的情况下，服务器会返回 200 OK 状态码。 分块传输编码Chunked Transfer Encoding，可以把数据分割成多块，让浏览器逐步显示页面。 多部分对象集合一份报文主体内可含有多种类型的实体同时发送，每个部分之间用 boundary 字段定义的分隔符进行分隔，每个部分都可以有首部字段。 例如，上传多个表单时可以使用如下方式： 123456789101112Content-Type: multipart/form-data; boundary=AaB03x--AaB03xContent-Disposition: form-data; name=\"submit-name\"Larry--AaB03xContent-Disposition: form-data; name=\"files\"; filename=\"file1.txt\"Content-Type: text/plain... contents of file1.txt ...--AaB03x--Copy to clipboardErrorCopied 虚拟主机HTTP/1.1 使用虚拟主机技术，使得一台服务器拥有多个域名，并且在逻辑上可以看成多个服务器。 通信数据转发1. 代理代理服务器接受客户端的请求，并且转发给其它服务器。 使用代理的主要目的是： 缓存 负载均衡 网络访问控制 访问日志记录 代理服务器分为正向代理和反向代理两种： 用户察觉得到正向代理的存在。 而反向代理一般位于内部网络中，用户察觉不到。 2. 网关与代理服务器不同的是，网关服务器会将 HTTP 转化为其它协议进行通信，从而请求其它非 HTTP 服务器的服务。 3. 隧道使用 SSL 等加密手段，在客户端和服务器之间建立一条安全的通信线路。 六、HTTPSHTTP 有以下安全性问题： 使用明文进行通信，内容可能会被窃听； 不验证通信方的身份，通信方的身份有可能遭遇伪装； 无法证明报文的完整性，报文有可能遭篡改。 HTTPS 并不是新协议，而是让 HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信。 通过使用 SSL，HTTPS 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）。 加密1. 对称密钥加密对称密钥加密（Symmetric-Key Encryption），加密和解密使用同一密钥。 优点：运算速度快； 缺点：无法安全地将密钥传输给通信方。 2.非对称密钥加密非对称密钥加密，又称公开密钥加密（Public-Key Encryption），加密和解密使用不同的密钥。 公开密钥所有人都可以获得，通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密，接收方收到通信内容后使用私有密钥解密。 非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。 优点：可以更安全地将公开密钥传输给通信发送方； 缺点：运算速度慢。 3. HTTPS 采用的加密方式HTTPS 采用混合的加密机制，使用非对称密钥加密用于传输对称密钥来保证传输过程的安全性，之后使用对称密钥加密进行通信来保证通信过程的效率。（下图中的 Session Key 就是对称密钥） 认证通过使用 证书 来对通信方进行认证。 数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。 服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。 进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。 完整性保护SSL 提供报文摘要功能来进行完整性保护。 HTTP 也提供了 MD5 报文摘要功能，但不是安全的。例如报文内容被篡改之后，同时重新计算 MD5 的值，通信接收方是无法意识到发生了篡改。 HTTPS 的报文摘要功能之所以安全，是因为它结合了加密和认证这两个操作。试想一下，加密之后的报文，遭到篡改之后，也很难重新计算报文摘要，因为无法轻易获取明文。 HTTPS 的缺点 因为需要进行加密解密等过程，因此速度会更慢； 需要支付证书授权的高额费用。 七、HTTP/2.0HTTP/1.x 缺陷HTTP/1.x 实现简单是以牺牲性能为代价的： 客户端需要使用多个连接才能实现并发和缩短延迟； 不会压缩请求和响应首部，从而导致不必要的网络流量； 不支持有效的资源优先级，致使底层 TCP 连接的利用率低下。 二进制分帧层HTTP/2.0 将报文分成 HEADERS 帧和 DATA 帧，它们都是二进制格式的。 在通信过程中，只会有一个 TCP 连接存在，它承载了任意数量的双向数据流（Stream）。 一个数据流（Stream）都有一个唯一标识符和可选的优先级信息，用于承载双向信息。 消息（Message）是与逻辑请求或响应对应的完整的一系列帧。 帧（Frame）是最小的通信单位，来自不同数据流的帧可以交错发送，然后再根据每个帧头的数据流标识符重新组装。 服务端推送HTTP/2.0 在客户端请求一个资源时，会把相关的资源一起发送给客户端，客户端就不需要再次发起请求了。例如客户端请求 page.html 页面，服务端就把 script.js 和 style.css 等与之相关的资源一起发给客户端。 首部压缩HTTP/1.1 的首部带有大量信息，而且每次都要重复发送。 HTTP/2.0 要求客户端和服务器同时维护和更新一个包含之前见过的首部字段表，从而避免了重复传输。 不仅如此，HTTP/2.0 也使用 Huffman 编码对首部字段进行压缩。 八、HTTP/1.1 新特性详细内容请见上文 默认是长连接 支持流水线 支持同时打开多个 TCP 连接 支持虚拟主机 新增状态码 100 支持分块传输编码 新增缓存处理指令 max-age 九、GET 和 POST 比较作用GET 用于获取资源，而 POST 用于传输实体主体。 参数GET 和 POST 的请求都能使用额外的参数，但是 GET 的参数是以查询字符串出现在 URL 中，而 POST 的参数存储在实体主体中。不能因为 POST 参数存储在实体主体中就认为它的安全性更高，因为照样可以通过一些抓包工具（Fiddler）查看。 因为 URL 只支持 ASCII 码，因此 GET 的参数中如果存在中文等字符就需要先进行编码。例如 中文 会转换为 %E4%B8%AD%E6%96%87，而空格会转换为 %20。POST 参数支持标准字符集。 1234GET /test/demo_form.asp?name1=value1&amp;name2=value2 HTTP/1.1Copy to clipboardErrorCopiedPOST /test/demo_form.asp HTTP/1.1Host: w3schools.comname1=value1&amp;name2=value2Copy to clipboardErrorCopied 安全安全的 HTTP 方法不会改变服务器状态，也就是说它只是可读的。 GET 方法是安全的，而 POST 却不是，因为 POST 的目的是传送实体主体内容，这个内容可能是用户上传的表单数据，上传成功之后，服务器可能把这个数据存储到数据库中，因此状态也就发生了改变。 安全的方法除了 GET 之外还有：HEAD、OPTIONS。 不安全的方法除了 POST 之外还有 PUT、DELETE。 幂等性幂等的 HTTP 方法，同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的。换句话说就是，幂等方法不应该具有副作用（统计用途除外）。 所有的安全方法也都是幂等的。 在正确实现的条件下，GET，HEAD，PUT 和 DELETE 等方法都是幂等的，而 POST 方法不是。 GET /pageX HTTP/1.1 是幂等的，连续调用多次，客户端接收到的结果都是一样的： 1234GET /pageX HTTP/1.1GET /pageX HTTP/1.1GET /pageX HTTP/1.1GET /pageX HTTP/1.1Copy to clipboardErrorCopied POST /add_row HTTP/1.1 不是幂等的，如果调用多次，就会增加多行记录： 123POST /add_row HTTP/1.1 -&gt; Adds a 1nd rowPOST /add_row HTTP/1.1 -&gt; Adds a 2nd rowPOST /add_row HTTP/1.1 -&gt; Adds a 3rd rowCopy to clipboardErrorCopied DELETE /idX/delete HTTP/1.1 是幂等的，即使不同的请求接收到的状态码不一样： 123DELETE /idX/delete HTTP/1.1 -&gt; Returns 200 if idX existsDELETE /idX/delete HTTP/1.1 -&gt; Returns 404 as it just got deletedDELETE /idX/delete HTTP/1.1 -&gt; Returns 404Copy to clipboardErrorCopied 可缓存如果要对响应进行缓存，需要满足以下条件： 请求报文的 HTTP 方法本身是可缓存的，包括 GET 和 HEAD，但是 PUT 和 DELETE 不可缓存，POST 在多数情况下不可缓存的。 响应报文的状态码是可缓存的，包括：200, 203, 204, 206, 300, 301, 404, 405, 410, 414, and 501。 响应报文的 Cache-Control 首部字段没有指定不进行缓存。 XMLHttpRequest为了阐述 POST 和 GET 的另一个区别，需要先了解 XMLHttpRequest： XMLHttpRequest 是一个 API，它为客户端提供了在客户端和服务器之间传输数据的功能。它提供了一个通过 URL 来获取数据的简单方式，并且不会使整个页面刷新。这使得网页只更新一部分页面而不会打扰到用户。XMLHttpRequest 在 AJAX 中被大量使用。 在使用 XMLHttpRequest 的 POST 方法时，浏览器会先发送 Header 再发送 Data。但并不是所有浏览器会这么做，例如火狐就不会。 而 GET 方法 Header 和 Data 会一起发送。 参考资料 上野宣. 图解 HTTP[M]. 人民邮电出版社, 2014. MDN : HTTP HTTP/2 简介 htmlspecialchars Difference between file URI and URL in java How to Fix SQL Injection Using Java PreparedStatement &amp; CallableStatement 浅谈 HTTP 中 Get 与 Post 的区别 Are http:// and www really necessary? HTTP (HyperText Transfer Protocol) Web-VPN: Secure Proxies with SPDY &amp; Chrome File:HTTP persistent connection.svg Proxy server What Is This HTTPS/SSL Thing And Why Should You Care? What is SSL Offloading? Sun Directory Server Enterprise Edition 7.0 Reference - Key Encryption An Introduction to Mutual SSL Authentication The Difference Between URLs and URIs Cookie 与 Session 的区别 COOKIE 和 SESSION 有什么区别 Cookie/Session 的机制与安全 HTTPS 证书原理 What is the difference between a URI, a URL and a URN? XMLHttpRequest XMLHttpRequest (XHR) Uses Multiple Packets for HTTP POST? Symmetric vs. Asymmetric Encryption – What are differences? Web 性能优化与 HTTP/2 HTTP/2 简介 一 OSI与TCP/IP各层的结构与功能OSI的七层体系结构概念清楚，理论也很完整，但是它比较复杂而且不实用。在这里顺带提一下之前一直被一些大公司甚至一些国家政府支持的OSI失败的原因： OSI的专家缺乏实际经验，他们在完成OSI标准时缺乏商业驱动力 OSI的协议实现起来过分复杂，而且运行效率很低 OSI制定标准的周期太长，因而使得按OSI标准生产的设备无法及时进入市场（20世纪90年代初期，虽然整套的OSI国际标准 都已经制定出来，但基于TCP/IP的互联网已经抢先在全球相当大的范围成功运行了） OSI的层次划分不太合理，有些功能在多个层次中重复出现 五层协议的体系结构学习计算机网络时我们一般采用折中的办法，也就是中和OSI和TCP/IP的有点，采用一种只有五层协议的体系结构，这样既简洁又能将概念阐述清楚。 1 应用层（application layer）-&gt; 数据传输得单位是: 报文 应用层的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。对于不同的网络应用需要不同的应用层协议。 应用层主要使用以下几种协议： 域名系统DNS，支持万维网应用的 域名系统是因特网的一项核心服务，它作为可以将域名和IP地址相互映射的一个分布式数据库，能够使人更方便的访问互联网，而不用去记住能够被机器直接读取的IP数串。 支持电子邮件的SMTP协议 HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。 2 运输层（transport layer）-&gt; 数据单位: 报文(分段) 运输层的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。“通用的”是指并不针对某一个特定的网络应用，而是多种应用可以使用同一个运输层服务。由于一台主机可同时运行多个线程，因此运输层有复用和分用的功能。所谓复用就是指多个应用层进程可同时使用下面运输层的服务，分用和复用相反，是运输层把收到的信息分别交付上面应用层中的相应进程。 运输层主要使用以下两种协议： 传输控制协议TCP（Transmisson Control Protocol） –提供面向连接的，可靠的数据传输服务。每一条TCP连接只能有两个端点，每一条TCP连接只能是点对点的（一对一), TCP提供可靠交付的服务, TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据；面向字节流. 用户数据协议UDP（User Datagram Protocol） –提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性), 它得传输是面向报文的, UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等, UDP支持一对一、一对多、多对一和多对多的交互通信, UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短. 3 网络层（network layer）-&gt;数据单位: 分组(包) 网络层负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在TCP/IP体系结构中，由于网络层使用IP协议，因此分组也叫IP数据报，简称数据报。 这里要注意：不要把运输层的“用户数据报UDP”和网络层的“IP数据报”弄混。另外，无论是哪一层的数据单元，都可笼统地用“分组”来表示。 网络层的另一个任务就是选择合适的路由，使源主机运输层所传下来的分株，能通过网络层中的路由器找到目的主机。 ==这里强调指出，网络层中的“网络”二字已经不是我们通常谈到的具体网络，而是指计算机网络体系结构模型中第三层的名称.== 互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Prococol）和许多路由选择协议，因此互联网的网络层也叫做网际层或IP层。 主要协议: TCP/IP协议 4 数据链路层（data link layer）-&gt;数据单位:帧 数据链路层通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。 在两个相邻节点之间传送数据时，数据链路层将网络层交下来的IP数据报组装程帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。 主要协议: PPP点到点 ARP CDP 5 物理层（physical layer）-&gt; 数据单位: 比特(位) 在物理层上所传送的数据单位是比特。物理层的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。 在互联网使用的各种协中最重要和最著名的就是TCP/IP两个协议。现在人们经常提到的TCP/IP并不一定单指TCP和IP这两个具体的协议，而往往表示互联网所使用的整个TCP/IP协议族。 TCP三次握手和四次挥手（面试常客） 理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。 为了准确无误地把数据送达目标处，TCP协议采用了三次握手策略。 简单示意图： 客户端–发送带有SYN标志的数据包–一次握手–服务端 服务端–发送带有SYN/ACK标志的数据包–二次握手–客户端 客户端–发送带有带有ACK标志的数据包–三次握手–服务端 为什么要传回SYN（发起一个新链接）？ 接收端传回发送端所发送的SYN是为了告诉发送端，我接收到的信息确实就是你所发送的信号了。 传了SYN，为啥还要传ACK（确认序号有效）？ 双方通信无误必须是两者互相发送信息都无误。传了SYN，证明发送方到接收方的通道没有问题，但是接收方到发送方的通道还需要ACK信号来进行验证。 断开一个TCP连接则需要“四次挥手”： 客户端-发送一个FIN，用来关闭客户端到服务器的数据传送 服务器-收到这个FIN，它发回一个ACK，确认序号为收到的序号加1 。和SYN一样，一个FIN将占用一个序号 服务器-关闭与客户端的连接，发送一个FIN给客户端 客户端-发回ACK报文确认，并将确认序号设置为收到序号加1 常见面试题 【问题1】为什么连接的时候是三次握手，关闭的时候却是四次握手？答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。 【问题2】为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？答：虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。在Client发送出最后的ACK回复，但该ACK可能丢失。Server如果没有收到ACK，将不断重复发送FIN片段。所以Client不能立即关闭，它必须确认Server接收到了该ACK。Client会在发送出ACK之后进入到TIME_WAIT状态。Client会设置一个计时器，等待2MSL的时间。如果在该时间内再次收到FIN，那么Client会重发ACK并再次等待2MSL。所谓的2MSL是两倍的MSL(Maximum Segment Lifetime)。MSL指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。 【问题3】为什么不能用两次握手进行连接？答：3次握手完成两个重要的功能，既要双方做好发送数据的准备工作(双方都知道彼此已准备好)，也要允许双方就初始序列号进行协商，这个序列号在握手过程中被发送和确认。 现在把三次握手改成仅需要两次握手，死锁是可能发生的。作为例子，考虑计算机S和C之间的通信，假定C给S发送一个连接请求分组，S收到了这个分组，并发 送了确认应答分组。按照两次握手的协定，S认为连接已经成功地建立了，可以开始发送数据分组。可是，C在S的应答分组在传输中被丢失的情况下，将不知道S 是否已准备好，不知道S建立什么样的序列号，C甚至怀疑S是否收到自己的连接请求分组。在这种情况下，C认为连接还未建立成功，将忽略S发来的任何数据分 组，只等待连接确认应答分组。而S在发出的分组超时后，重复发送同样的分组。这样就形成了死锁。 【问题4】如果已经建立了连接，但是客户端突然出现故障了怎么办？TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 在浏览器中输入url地址 -&gt;&gt; 显示主页的过程（面试常客） 打开一个网页，整个过程会使用哪些协议 状态码 各种协议与HTTP协议之间的关系一般面试官会通过这样的问题来考察你对计算机网络知识体系的理解。 HTTP长连接、短连接 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 短连接 在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如JavaScript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。 长连接 从HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码： 1Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。 HTTP和HTTPS的区别 Http：超文本传输协议（Http，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。 Https：是以安全为目标的Http通道，是Http的安全版。Https的安全基础是SSL。 1、https协议需要到CA申请证书，一般免费证书较少，因而需要一定费用。(原来网易官网是http，而网易邮箱是https。) 2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。 3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 4、http的连接很简单，是无状态的。Https协议是由SSL+Http协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。(无状态的意思是其数据包的发送、传输和接收都是相互独立的。无连接的意思是指通信双方都不长久的维持对方的任何信息。) Https的缺点（对比优点） 1、Https协议握手阶段比较费时，会使页面的加载时间延长近。 2、Https连接缓存不如Http高效，会增加数据开销，甚至已有的安全措施也会因此而受到影响； 3、SSL证书通常需要绑定IP，不能在同一IP上绑定多个域名，IPv4资源不可能支撑这个消耗。 4、Https协议的加密范围也比较有限。最关键的，SSL证书的信用链体系并不安全，特别是在某些国家可以控制CA根证书的情况下，中间人攻击一样可行。 HTTP和RPC概念 HTTP就是一种RPC, 只要是远程调用都可以叫RPC，和是不是通过http没什么关系。 http好比普通话，rpc好比团伙内部黑话。讲普通话，好处就是谁都听得懂，谁都会讲。 讲黑话，好处是可以更精简、更加保密、更加可定制，坏处就是要求“说”黑话的那一方（client端）也要懂，而且一旦大家都说一种黑话了，换黑话就困难了。 这个回答里恰巧讲了一些rpc通信协议的细节，但是强调一遍通信协议不是rpc最重要的部分，不要被这个回答带偏了。如果要了解rpc请更多的去了解服务治理(soa)的一些基本策略,推荐去看看dubbo的文档。 首先 http 和 rpc 并不是一个并行概念。 rpc是远端过程调用，其调用协议通常包含传输协议和序列化协议。 传输协议包含: 如著名的 [gRPC](grpc / grpc.io) 使用的 http2 协议，也有如dubbo一类的自定义报文的tcp协议。 序列化协议包含: 如基于文本编码的 xml json，也有二进制编码的 protobuf hessian等。 Http与Rpc的区别 首先要否认一点 http 协议相较于自定义tcp报文协议rpc，增加的开销在于连接的建立与断开。http协议是支持连接池复用的，也就是建立一定数量的连接不断开，并不会频繁的创建和销毁连接。二要说的是http也可以使用protobuf这种二进制编码协议对内容进行编码，因此二者最大的区别还是在传输协议上。 传输协议 RPC，可以基于TCP协议，也可以基于HTTP协议 HTTP，基于HTTP协议 传输效率 RPC，使用自定义的TCP协议，可以让请求报文体积更小，或者使用HTTP2协议，也可以很好的减少报文的体积，提高传输效率 12345678910HTTP/1.0 200 OK Content-Type: text/plainContent-Length: 137582Expires: Thu, 05 Dec 1997 16:00:00 GMTLast-Modified: Wed, 5 August 1996 15:55:28 GMTServer: Apache 0.84&lt;html&gt; &lt;body&gt;Hello World&lt;/body&gt;&lt;/html&gt; 即使编码协议也就是body是使用二进制编码协议，报文元数据也就是header头的键值对却用了文本编码，非常占字节数。如上图所使用的报文中有效字节数仅仅占约 30%，也就是70%的时间用于传输元数据废编码。当然实际情况下报文内容可能会比这个长，但是报头所占的比例也是非常可观的。 那么假如我们使用自定义tcp协议的报文如下 报头占用的字节数也就只有16个byte，极大地精简了传输内容。 这也就是为什么后端进程间通常会采用自定义tcp协议的rpc来进行通信的原因。 所谓的效率优势是针对http1.1协议来讲的，http2.0协议已经优化编码效率问题，像grpc这种rpc库使用的就是http2.0协议。这么来说吧http容器的性能测试单位通常是kqps，自定义tcp协议则通常是以10kqps到100kqps为基准 HTTP，如果是基于HTTP1.1的协议，请求中会包含很多无用的内容，如果是基于HTTP2.0，那么简单的封装以下是可以作为一个RPC来使用的，这时标准RPC框架更多的是服务治理 性能消耗，主要在于序列化和反序列化的耗时 RPC，可以基于thrift实现高效的二进制传输 HTTP，大部分是通过json来实现的，字节大小和序列化耗时都比thrift要更消耗性能 负载均衡 RPC，基本都自带了负载均衡策略 HTTP，需要配置Nginx，HAProxy来实现 服务治理（下游服务新增，重启，下线时如何不影响上游调用者） RPC，能做到自动通知，不影响上游 简单来说成熟的rpc库相对http容器，更多的是封装了“服务发现”，”负载均衡“，“熔断降级”一类面向服务的高级特性。可以这么理解，rpc框架是面向[务的更高级的封装。如果把一个http servlet容器上封装一层服务发现和函数代理调用，那它就已经可以做一个rpc框架了。 HTTP，需要事先通知，修改Nginx/HAProxy配置 总结： RPC主要用于公司内部的服务调用，性能消耗低，传输效率高，服务治理方便。HTTP主要用于对外的异构环境，浏览器接口调用，APP接口调用，第三方接口调用等。因为良好的rpc调用是面向服务的封装，针对服务的可用性和效率等都做了优化。单纯使用http调用则缺少了这些特性。 Cookie和Session的区别 这里有说到，HTTP协议是无状态的，服务器中没有保存客户端的状态，客户端必须每次带上自己的状态去请求服务器 基于HTTP这种特点，就产生了cookie/session 1、cookie数据存放在客户的浏览器上，session数据放在服务器上。 2、cookie相比session不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗,考虑到安全应当使用session。 3、session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能,考虑到减轻服务器性能方面，应当使用cookie。 4、单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。而session存储在服务端，可以无限量存储 5、所以：将登录信息等重要信息存放为session;其他信息如果需要保留，可以放在cookie中 获取request的客户端IP地址123request.getHeader(\"X-Forwarded-For\")//这3中方法都可以用来进行解析Header中的IP地址request.getHeader(\"Proxy-Client-IP\")request.getHeader(\"Remote_Addr\")","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"},{"name":"网络","slug":"网络","permalink":"/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"zookeeper的使用","slug":"zookeeper教程","date":"2019-10-20T11:51:12.000Z","updated":"2019-10-20T06:05:26.979Z","comments":true,"path":"2019/10/20/zookeeper教程/","link":"","permalink":"/2019/10/20/zookeeper%E6%95%99%E7%A8%8B/","excerpt":"","text":"1.zookeeper的概述Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应，从而实现集群中类似Master/Slave管理模式 Zookeeper=文件系统+通知机制 2.应用场景提供的服务包括：分布式消息同步和协调机制、服务器节点动态上下线、统一配置管理、负载均衡、集群管理，分布式锁 3.实战3.1 安装前准备： （1）安装jdk （2）通过filezilla工具拷贝zookeeper到到linux系统下 （3）修改tar包权限chmod u+x zookeeper-3.4.10.tar.gz （4）解压到指定目录[atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ 3.2 配置修改将/opt/module/zookeeper-3.4.10/conf这个路径下的zoo_sample.cfg修改为zoo.cfg； 进入zoo.cfg文件：vim zoo.cfg 修改dataDir路径为 dataDir=/opt/module/zookeeper-3.4.10/data/zkData 在/opt/module/zookeeper-3.4.10/这个目录上创建data/zkData文件夹 mkdir -p data/zkData 3.3 操作zookeeper（1）启动zookeeper[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh start （2）查看进程是否启动[atguigu@hadoop102 zookeeper-3.4.10]$ jps 4020 Jps 4001 QuorumPeerMain （3）查看状态：[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh status ZooKeeper JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: standalone （4）启动客户端：[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkCli.sh （5）退出客户端：[zk: localhost:2181(CONNECTED) 0] quit （6）停止zookeeper[atguigu@hadoop102 zookeeper-3.4.10]$ bin/zkServer.sh stop 3.4 配置参数解读3.41 解读zoo.cfg 文件中参数含义 1）tickTime：通信心跳数，Zookeeper服务器心跳时间，单位毫秒Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) 2）initLimit：LF初始通信时限集群中的follower跟随者服务器(F)与leader领导者服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 投票选举新leader的初始化时间 Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 Leader允许F在initLimit时间内完成这个工作。 3）syncLimit：LF同步通信时限集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime， Leader认为Follwer死掉，从服务器列表中删除Follwer。 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 4）dataDir：数据文件目录+数据持久化路径保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。 5）clientPort：客户端连接端口监听客户端连接的端口 4. 数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。 很显然zookeeper集群自身维护了一套数据结构。这个存储结构是一个树形结构，其上的每一个节点，我们称之为”znode”，每一个znode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识 5. 节点类型1）Znode有两种类型： 短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除 持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 2）Znode有四种形式的目录节点（默认是persistent ） （1）持久化目录节点（PERSISTENT） 客户端与zookeeper断开连接后，该节点依旧存在 （2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 （3）临时目录节点（EPHEMERAL） 客户端与zookeeper断开连接后，该节点被删除 （4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 6. 特点 1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。 2）Leader负责进行投票的发起和决议，更新系统状态 3）Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票 4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。 5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。 6）更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。 7）数据更新原子性，一次数据更新要么成功，要么失败。 8）实时性，在一定时间范围内，client能读到最新数据。 7. 选举机制 1）半数机制：集群中半数以上机器存活，集群可用。所以zookeeper适合装在奇数台机器上。 2）Zookeeper虽然在配置文件中并没有指定master和slave。但是，zookeeper工作时，是有一个节点为leader，其他则为follower，Leader是通过内部的选举机制临时产生的 3）以一个简单的例子来说明整个选举的过程。 假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。 12345678910#选举leader案例（1）服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是LOOKING状态。（2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。（3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的leader。（4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它只能接收当小弟的命了。（5）服务器5启动，同4一样当小弟。 8. 监听器原理监听器是一个接口，我们的代码中可以实现Wather这个接口，实现其中的process方法，方法中即我们自己的业务逻辑 监听器的注册是在获取数据的操作中实现： getData(path,watch?)监听的事件是：节点数据变化事件 getChildren(path,watch?)监听的事件是：节点下的子节点增减变化事件 9.0 分布式zookeeper的搭建9.1 集群规划在hadoop2、hadoop3和hadoop4三个节点上部署Zookeeper。 9.2 解压安装（1）解压zookeeper安装包到/opt/module/目录下[atguigu@hadoop102 software]$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/ （2）在/opt/module/zookeeper-3.4.10/这个目录下创建data/zkDatamkdir -p data/zkData （3）重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfgmv zoo_sample.cfg zoo.cfg 9.3 配置zoo.cfg文件（1）具体配置dataDir=/opt/module/zookeeper-3.4.10/data/zkData 增加如下配置 #######################cluster########################## server.2=hadoop102:2888:3888 server.3=hadoop103:2888:3888 server.4=hadoop104:2888:3888 （2）配置参数解读Server.A=B:C:D。 A是一个数字，表示这个是第几号服务器； B是这个服务器的ip地址； C是这个服务器与集群中的Leader服务器交换信息的端口； D是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。 9.4 集群操作（1）在/opt/module/zookeeper-3.4.10/data/zkData目录下创建一个myid的文件touch myid 添加myid文件，注意一定要在linux里面创建，在notepad++里面很可能乱码 （2）编辑myid文件vi myid 在文件中添加与server对应的编号：如2 （3）拷贝配置好的zookeeper到其他机器上scp -r zookeeper-3.4.10/ root@hadoop3.atguigu.com:/opt/app/ scp -r zookeeper-3.4.10/ root@hadoop4.atguigu.com:/opt/app/ 并分别修改myid文件中内容为3、4 （4）分别启动zookeeper[root@hadoop2 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop3 zookeeper-3.4.10]# bin/zkServer.sh start [root@hadoop4 zookeeper-3.4.10]# bin/zkServer.sh start （5）查看状态[root@hadoop2 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@hadoop3 zookeeper-3.4.10]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader [root@hadoop4 zookeeper-3.4.5]# bin/zkServer.sh status JMX enabled by default Using config: /opt/module/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower 9.5 客户端命令行操作1）启动客户端[atguigu@hadoop103 zookeeper-3.4.10]$ bin/zkCli.sh 2）显示所有操作命令[zk: localhost:2181(CONNECTED) 1] help 3）查看当前znode中所包含的内容[zk: localhost:2181(CONNECTED) 0] ls / [zookeeper] 4）查看当前节点数据并能看到更新次数等数据[zk: localhost:2181(CONNECTED) 1] ls2 / 12345678910111213#根目录下的数据[zookeeper]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x0cversion = -1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1 5）创建普通节点[zk: localhost:2181(CONNECTED) 2] create /app1 “hello app1” Created /app1 [zk: localhost:2181(CONNECTED) 4] create /app1/server101 “192.168.1.101” Created /app1/server101 6）获得节点的值[zk: localhost:2181(CONNECTED) 6] get /app1 123456789101112hello app1cZxid = 0x20000000actime = Mon Jul 17 16:08:35 CST 2017mZxid = 0x20000000amtime = Mon Jul 17 16:08:35 CST 2017pZxid = 0x20000000bcversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 10numChildren = 1 [zk: localhost:2181(CONNECTED) 8] get /app1/server101 123456789101112192.168.1.101cZxid = 0x20000000bctime = Mon Jul 17 16:11:04 CST 2017mZxid = 0x20000000bmtime = Mon Jul 17 16:11:04 CST 2017pZxid = 0x20000000bcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 13numChildren = 0 7）创建短暂节点[zk: localhost:2181(CONNECTED) 9] create -e /app-emphemeral 8888 （1）在当前客户端是能查看到的[zk: localhost:2181(CONNECTED) 10] ls / [app1, app-emphemeral, zookeeper] （2）退出当前客户端然后再重启启动客户端[zk: localhost:2181(CONNECTED) 12] quit [atguigu@hadoop104 zookeeper-3.4.10]$ bin/zkCli.sh （3）再次查看根目录下短暂节点已经删除[zk: localhost:2181(CONNECTED) 0] ls / [app1, zookeeper] 8）创建带序号的节点（1）先创建一个普通的根节点app2[zk: localhost:2181(CONNECTED) 11] create /app2 “app2” （2）创建带序号的节点[zk: localhost:2181(CONNECTED) 13] create -s /app2/aa 888 Created /app2/aa0000000000 [zk: localhost:2181(CONNECTED) 14] create -s /app2/bb 888 Created /app2/bb0000000001 [zk: localhost:2181(CONNECTED) 15] create -s /app2/cc 888 Created /app2/cc0000000002 如果原节点下有1个节点，则再排序时从1开始，以此类推。 [zk: localhost:2181(CONNECTED) 16] create -s /app1/aa 888 Created /app1/aa0000000001 9）修改节点数据值[zk: localhost:2181(CONNECTED) 2] set /app1 999 10）节点的值变化监听（1）在104主机上注册监听/app1节点数据变化 [zk: localhost:2181(CONNECTED) 26] get /app1 watch （2）在103主机上修改/app1节点的数据 [zk: localhost:2181(CONNECTED) 5] set /app1 777 （3）观察104主机收到数据变化的监听 WATCHER:: WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1 11）节点的子节点变化监听（路径变化）（1）在104主机上注册监听/app1节点的子节点变化[zk: localhost:2181(CONNECTED) 1] ls /app1 watch [aa0000000001, server101] （2）在103主机/app1节点上创建子节点[zk: localhost:2181(CONNECTED) 6] create /app1/bb 666 Created /app1/bb （3）观察104主机收到子节点变化的监听WATCHER:: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 12）删除节点[zk: localhost:2181(CONNECTED) 4] delete /app1/bb 13）递归删除节点[zk: localhost:2181(CONNECTED) 7] rmr /app2 14）查看节点状态123456789101112[zk: localhost:2181(CONNECTED) 12] stat /app1cZxid = 0x20000000actime = Mon Jul 17 16:08:35 CST 2017mZxid = 0x200000018mtime = Mon Jul 17 16:54:38 CST 2017pZxid = 0x20000001ccversion = 4dataVersion = 2aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 2 9.6 操作命令 创建节点：create /路径 值 创建时的可选参数： -e（表示短暂节点）， -s（表示顺序节点） 删除节点值： delete /路径 递归删除：rmr /路径 修改数据：set /路径 新的值 获取数据：get /路径 查看所有节点：ls /路径","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"},{"name":"服务端开发","slug":"服务端开发","permalink":"/tags/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"zookeeper","slug":"zookeeper","permalink":"/tags/zookeeper/"}]},{"title":"Linux","slug":"Linux","date":"2019-10-20T11:45:12.000Z","updated":"2019-10-20T12:20:10.990Z","comments":true,"path":"2019/10/20/Linux/","link":"","permalink":"/2019/10/20/Linux/","excerpt":"","text":"基本理论知一、常用操作以及概念快捷键 Tab：命令和文件名补全； Ctrl+C：中断正在运行的程序； Ctrl+D：结束键盘输入（End Of File，EOF） 求助1. –help指令的基本用法与选项介绍。 2. manman 是 manual 的缩写，将指令的具体信息显示出来。 当执行 man date 时，有 DATE(1) 出现，其中的数字代表指令的类型，常用的数字及其类型如下： 代号 类型 1 用户在 shell 环境中可以操作的指令或者可执行文件 5 配置文件 8 系统管理员可以使用的管理指令 3. infoinfo 与 man 类似，但是 info 将文档分成一个个页面，每个页面可以进行跳转。 4. doc/usr/share/doc 存放着软件的一整套说明文件。 关机1. who在关机前需要先使用 who 命令查看有没有其它用户在线。 2. sync为了加快对磁盘文件的读写速度，位于内存中的文件数据不会立即同步到磁盘上，因此关机之前需要先进行 sync 同步操作。 3. shutdown12345# shutdown [-krhc] 时间 [信息]-k ： 不会关机，只是发送警告信息，通知所有在线的用户-r ： 将系统的服务停掉后就重新启动-h ： 将系统的服务停掉后就立即关机-c ： 取消已经在进行的 shutdown 指令内容 PATH可以在环境变量 PATH 中声明可执行文件的路径，路径之间用 : 分隔。 1/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/dmtsai/.local/bin:/home/dmtsai/bin sudosudo 允许一般用户使用 root 可执行的命令，不过只有在 /etc/sudoers 配置文件中添加的用户才能使用该指令。 包管理工具RPM 和 DPKG 为最常见的两类软件包管理工具： RPM 全称为 Redhat Package Manager，最早由 Red Hat 公司制定实施，随后被 GNU 开源操作系统接受并成为许多 Linux 系统的既定软件标准。YUM 基于 RPM，具有依赖管理和软件升级功能。 与 RPM 竞争的是基于 Debian 操作系统的 DEB 软件包管理工具 DPKG，全称为 Debian Package，功能方面与 RPM 相似。 发行版Linux 发行版是 Linux 内核及各种应用软件的集成版本。 基于的包管理工具 商业发行版 社区发行版 RPM Red Hat Fedora / CentOS DPKG Ubuntu Debian VIM 三个模式 一般指令模式（Command mode）：VIM 的默认模式，可以用于移动游标查看内容； 编辑模式（Insert mode）：按下 “i” 等按键之后进入，可以对文本进行编辑； 指令列模式（Bottom-line mode）：按下 “:” 按键之后进入，用于保存退出等操作。 在指令列模式下，有以下命令用于离开或者保存文件。 命令 作用 :w 写入磁盘 :w! 当文件为只读时，强制写入磁盘。到底能不能写入，与用户对该文件的权限有关 :q 离开 :q! 强制离开不保存 :wq 写入磁盘后离开 :wq! 强制写入磁盘后离开 GNUGNU 计划，译为革奴计划，它的目标是创建一套完全自由的操作系统，称为 GNU，其内容软件完全以 GPL 方式发布。其中 GPL 全称为 GNU 通用公共许可协议（GNU General Public License），包含了以下内容： 以任何目的运行此程序的自由； 再复制的自由； 改进此程序，并公开发布改进的自由。 开源协议 Choose an open source license 如何选择开源许可证？ 二、磁盘磁盘接口1. IDEIDE（ATA）全称 Advanced Technology Attachment，接口速度最大为 133MB/s，因为并口线的抗干扰性太差，且排线占用空间较大，不利电脑内部散热，已逐渐被 SATA 所取代。 2. SATASATA 全称 Serial ATA，也就是使用串口的 ATA 接口，抗干扰性强，且对数据线的长度要求比 ATA 低很多，支持热插拔等功能。SATA-II 的接口速度为 300MiB/s，而 SATA-III 标准可达到 600MiB/s 的传输速度。SATA 的数据线也比 ATA 的细得多，有利于机箱内的空气流通，整理线材也比较方便。 3. SCSISCSI 全称是 Small Computer System Interface（小型机系统接口），SCSI 硬盘广为工作站以及个人电脑以及服务器所使用，因此会使用较为先进的技术，如碟片转速 15000rpm 的高转速，且传输时 CPU 占用率较低，但是单价也比相同容量的 ATA 及 SATA 硬盘更加昂贵。 4. SASSAS（Serial Attached SCSI）是新一代的 SCSI 技术，和 SATA 硬盘相同，都是采取序列式技术以获得更高的传输速度，可达到 6Gb/s。此外也通过缩小连接线改善系统内部空间等。 磁盘的文件名Linux 中每个硬件都被当做一个文件，包括磁盘。磁盘以磁盘接口类型进行命名，常见磁盘的文件名如下： IDE 磁盘：/dev/hd[a-d] SATA/SCSI/SAS 磁盘：/dev/sd[a-p] 其中文件名后面的序号的确定与系统检测到磁盘的顺序有关，而与磁盘所插入的插槽位置无关。 三、分区分区表磁盘分区表主要有两种格式，一种是限制较多的 MBR 分区表，一种是较新且限制较少的 GPT 分区表。 1. MBRMBR 中，第一个扇区最重要，里面有主要开机记录（Master boot record, MBR）及分区表（partition table），其中主要开机记录占 446 bytes，分区表占 64 bytes。 分区表只有 64 bytes，最多只能存储 4 个分区，这 4 个分区为主分区（Primary）和扩展分区（Extended）。其中扩展分区只有一个，它使用其它扇区来记录额外的分区表，因此通过扩展分区可以分出更多分区，这些分区称为逻辑分区。 Linux 也把分区当成文件，分区文件的命名方式为：磁盘文件名 + 编号，例如 /dev/sda1。注意，逻辑分区的编号从 5 开始。 2. GPT扇区是磁盘的最小存储单位，旧磁盘的扇区大小通常为 512 bytes，而最新的磁盘支持 4 k。GPT 为了兼容所有磁盘，在定义扇区上使用逻辑区块地址（Logical Block Address, LBA），LBA 默认大小为 512 bytes。 GPT 第 1 个区块记录了主要开机记录（MBR），紧接着是 33 个区块记录分区信息，并把最后的 33 个区块用于对分区信息进行备份。这 33 个区块第一个为 GPT 表头纪录，这个部份纪录了分区表本身的位置与大小和备份分区的位置，同时放置了分区表的校验码 (CRC32)，操作系统可以根据这个校验码来判断 GPT 是否正确。若有错误，可以使用备份分区进行恢复。 GPT 没有扩展分区概念，都是主分区，每个 LBA 可以分 4 个分区，因此总共可以分 4 * 32 = 128 个分区。 MBR 不支持 2.2 TB 以上的硬盘，GPT 则最多支持到 233 TB = 8 ZB。 开机检测程序1. BIOSBIOS（Basic Input/Output System，基本输入输出系统），它是一个固件（嵌入在硬件中的软件），BIOS 程序存放在断电后内容不会丢失的只读内存中。 BIOS 是开机的时候计算机执行的第一个程序，这个程序知道可以开机的磁盘，并读取磁盘第一个扇区的主要开机记录（MBR），由主要开机记录（MBR）执行其中的开机管理程序，这个开机管理程序会加载操作系统的核心文件。 主要开机记录（MBR）中的开机管理程序提供以下功能：选单、载入核心文件以及转交其它开机管理程序。转交这个功能可以用来实现多重引导，只需要将另一个操作系统的开机管理程序安装在其它分区的启动扇区上，在启动开机管理程序时，就可以通过选单选择启动当前的操作系统或者转交给其它开机管理程序从而启动另一个操作系统。 下图中，第一扇区的主要开机记录（MBR）中的开机管理程序提供了两个选单：M1、M2，M1 指向了 Windows 操作系统，而 M2 指向其它分区的启动扇区，里面包含了另外一个开机管理程序，提供了一个指向 Linux 的选单。 安装多重引导，最好先安装 Windows 再安装 Linux。因为安装 Windows 时会覆盖掉主要开机记录（MBR），而 Linux 可以选择将开机管理程序安装在主要开机记录（MBR）或者其它分区的启动扇区，并且可以设置开机管理程序的选单。 2. UEFIBIOS 不可以读取 GPT 分区表，而 UEFI 可以。 四、文件系统分区与文件系统对分区进行格式化是为了在分区上建立文件系统。一个分区通常只能格式化为一个文件系统，但是磁盘阵列等技术可以将一个分区格式化为多个文件系统。 组成最主要的几个组成部分如下： inode：一个文件占用一个 inode，记录文件的属性，同时记录此文件的内容所在的 block 编号； block：记录文件的内容，文件太大时，会占用多个 block。 除此之外还包括： superblock：记录文件系统的整体信息，包括 inode 和 block 的总量、使用量、剩余量，以及文件系统的格式与相关信息等； block bitmap：记录 block 是否被使用的位图。 文件读取对于 Ext2 文件系统，当要读取一个文件的内容时，先在 inode 中查找文件内容所在的所有 block，然后把所有 block 的内容读出来。 而对于 FAT 文件系统，它没有 inode，每个 block 中存储着下一个 block 的编号。 磁盘碎片指一个文件内容所在的 block 过于分散，导致磁盘磁头移动距离过大，从而降低磁盘读写性能。 block在 Ext2 文件系统中所支持的 block 大小有 1K，2K 及 4K 三种，不同的大小限制了单个文件和文件系统的最大大小。 大小 1KB 2KB 4KB 最大单一文件 16GB 256GB 2TB 最大文件系统 2TB 8TB 16TB 一个 block 只能被一个文件所使用，未使用的部分直接浪费了。因此如果需要存储大量的小文件，那么最好选用比较小的 block。 inodeinode 具体包含以下信息： 权限 (read/write/excute)； 拥有者与群组 (owner/group)； 容量； 建立或状态改变的时间 (ctime)； 最近读取时间 (atime)； 最近修改时间 (mtime)； 定义文件特性的旗标 (flag)，如 SetUID…； 该文件真正内容的指向 (pointer)。 inode 具有以下特点： 每个 inode 大小均固定为 128 bytes (新的 ext4 与 xfs 可设定到 256 bytes)； 每个文件都仅会占用一个 inode。 inode 中记录了文件内容所在的 block 编号，但是每个 block 非常小，一个大文件随便都需要几十万的 block。而一个 inode 大小有限，无法直接引用这么多 block 编号。因此引入了间接、双间接、三间接引用。间接引用让 inode 记录的引用 block 块记录引用信息。 目录建立一个目录时，会分配一个 inode 与至少一个 block。block 记录的内容是目录下所有文件的 inode 编号以及文件名。 可以看到文件的 inode 本身不记录文件名，文件名记录在目录中，因此新增文件、删除文件、更改文件名这些操作与目录的写权限有关。 日志如果突然断电，那么文件系统会发生错误，例如断电前只修改了 block bitmap，而还没有将数据真正写入 block 中。 ext3/ext4 文件系统引入了日志功能，可以利用日志来修复文件系统。 挂载挂载利用目录作为文件系统的进入点，也就是说，进入目录之后就可以读取文件系统的数据。 目录配置为了使不同 Linux 发行版本的目录结构保持一致性，Filesystem Hierarchy Standard (FHS) 规定了 Linux 的目录结构。最基础的三个目录如下： / (root, 根目录) /usr (unix software resource)：所有系统默认软件都会安装到这个目录； /var (variable)：存放系统或程序运行过程中的数据文件。 五、文件文件属性用户分为三种：文件拥有者、群组以及其它人，对不同的用户有不同的文件权限。 使用 ls 查看一个文件时，会显示一个文件的信息，例如 drwxr-xr-x 3 root root 17 May 6 00:14 .config，对这个信息的解释如下： drwxr-xr-x：文件类型以及权限，第 1 位为文件类型字段，后 9 位为文件权限字段 3：链接数 root：文件拥有者 root：所属群组 17：文件大小 May 6 00:14：文件最后被修改的时间 .config：文件名 常见的文件类型及其含义有： d：目录 -：文件 l：链接文件 9 位的文件权限字段中，每 3 个为一组，共 3 组，每一组分别代表对文件拥有者、所属群组以及其它人的文件权限。一组权限中的 3 位分别为 r、w、x 权限，表示可读、可写、可执行。 文件时间有以下三种： modification time (mtime)：文件的内容更新就会更新； status time (ctime)：文件的状态（权限、属性）更新就会更新； access time (atime)：读取文件时就会更新。 文件与目录的基本操作1. ls列出文件或者目录的信息，目录的信息就是其中包含的文件。 1234# ls [-aAdfFhilnrRSt] file|dir-a ：列出全部的文件-d ：仅列出目录本身-l ：以长数据串行列出，包含文件的属性与权限等等数据 2. du 用来查看目录或文件所占用磁盘空间的大小。常用选项组合为：du -sh du常用的选项： -h：以人类可读的方式显示 -a：显示目录占用的磁盘空间大小，还要显示其下目录和文件占用磁盘空间的大小 -s：显示目录占用的磁盘空间大小，不要显示其下子目录和文件占用的磁盘空间大小 -c：显示几个目录或文件占用的磁盘空间大小，还要统计它们的总和 –apparent-size：显示目录或文件自身的大小 -l ：统计硬链接占用磁盘空间的大小 -L：统计符号链接所指向的文件占用的磁盘空间大小 du -sh : 查看当前目录总共占的容量。而不单独列出各子项占用的容量 du -lh –max-depth=1 : 查看当前目录下一级子文件和子目录占用的磁盘容量。 du -sh * | sort -n 统计当前文件夹(目录)大小，并按文件大小排序du -sk filename 查看指定文件大小 3. mkdir创建目录。 123# mkdir [-mp] 目录名称-m ：配置目录权限-p ：递归创建目录 4. rmdir删除目录，目录必须为空。 12rmdir [-p] 目录名称-p ：递归删除目录 5. touch更新文件时间或者建立新文件。 123456# touch [-acdmt] filename-a ： 更新 atime-c ： 更新 ctime，若该文件不存在则不建立新文件-m ： 更新 mtime-d ： 后面可以接更新日期而不使用当前日期，也可以使用 --date=\"日期或时间\"-t ： 后面可以接更新时间而不使用当前时间，格式为[YYYYMMDDhhmm] 6. cp复制文件。如果源文件有两个以上，则目的文件一定要是目录才行。 12345678cp [-adfilprsu] source destination-a ：相当于 -dr --preserve=all-d ：若来源文件为链接文件，则复制链接文件属性而非文件本身-i ：若目标文件已经存在时，在覆盖前会先询问-p ：连同文件的属性一起复制过去-r ：递归复制-u ：destination 比 source 旧才更新 destination，或 destination 不存在的情况下才复制--preserve=all ：除了 -p 的权限相关参数外，还加入 SELinux 的属性, links, xattr 等也复制了 7. rm删除文件。 12# rm [-fir] 文件或目录-r ：递归删除 8. mv移动文件。 123# mv [-fiu] source destination# mv [options] source1 source2 source3 .... directory-f ： force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖 修改权限可以将一组权限用数字来表示，此时一组权限的 3 个位当做二进制数字的位，从左到右每个位的权值为 4、2、1，即每个权限对应的数字权值为 r : 4、w : 2、x : 1。 1# chmod [-R] xyz dirname/filename 示例：将 .bashrc 文件的权限修改为 -rwxr-xr–。 1# chmod 754 .bashrc 也可以使用符号来设定权限。 12345678# chmod [ugoa] [+-=] [rwx] dirname/filename- u：拥有者- g：所属群组- o：其他人- a：所有人- +：添加权限- -：移除权限- =：设定权限 示例：为 .bashrc 文件的所有用户添加写权限。 1# chmod a+w .bashrc 默认权限 文件默认权限：文件默认没有可执行权限，因此为 666，也就是 -rw-rw-rw- 。 目录默认权限：目录必须要能够进入，也就是必须拥有可执行权限，因此为 777 ，也就是 drwxrwxrwx。 可以通过 umask 设置或者查看默认权限，通常以掩码的形式来表示，例如 002 表示其它用户的权限去除了一个 2 的权限，也就是写权限，因此建立新文件时默认的权限为 -rw-rw-r–。 目录的权限文件名不是存储在一个文件的内容中，而是存储在一个文件所在的目录中。因此，拥有文件的 w 权限并不能对文件名进行修改。 目录存储文件列表，一个目录的权限也就是对其文件列表的权限。因此，目录的 r 权限表示可以读取文件列表；w 权限表示可以修改文件列表，具体来说，就是添加删除文件，对文件名进行修改；x 权限可以让该目录成为工作目录，x 权限是 r 和 w 权限的基础，如果不能使一个目录成为工作目录，也就没办法读取文件列表以及对文件列表进行修改了。 链接 123# ln [-sf] source_filename dist_filename-s ：默认是实体链接，加 -s 为符号链接-f ：如果目标文件存在时，先删除目标文件 1. 实体链接在目录下创建一个条目，记录着文件名与 inode 编号，这个 inode 就是源文件的 inode。 删除任意一个条目，文件还是存在，只要引用数量不为 0。 有以下限制：不能跨越文件系统、不能对目录进行链接。 1234# ln /etc/crontab .# ll -i /etc/crontab crontab34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 crontab34474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab 2. 符号链接符号链接文件保存着源文件所在的绝对路径，在读取时会定位到源文件上，可以理解为 Windows 的快捷方式。 当源文件被删除了，链接文件就打不开了。 因为记录的是路径，所以可以为目录建立符号链接。 123# ll -i /etc/crontab /root/crontab234474855 -rw-r--r--. 2 root root 451 Jun 10 2014 /etc/crontab53745909 lrwxrwxrwx. 1 root root 12 Jun 23 22:31 /root/crontab2 -&gt; /etc/crontab 获取文件内容容)1. cat取得文件内容。 12# cat [-AbEnTv] filename-n ：打印出行号，连同空白行也会有行号，-b 不会 2. tac是 cat 的反向操作，从最后一行开始打印。 3. more和 cat 不同的是它可以一页一页查看文件内容，比较适合大文件的查看。 4. less和 more 类似，但是多了一个向前翻页的功能。 5. head取得文件前几行。 12# head [-n number] filename-n ：后面接数字，代表显示几行的意思 6. tail是 head 的反向操作，只是取得是后几行。 7. od以字符或者十六进制的形式显示二进制文件。 指令与文件搜索1. which指令搜索。 12# which [-a] command-a ：将所有指令列出，而不是只列第一个 2. whereis文件搜索。速度比较快，因为它只搜索几个特定的目录。 1# whereis [-bmsu] dirname/filename 3. locate文件搜索。可以用关键字或者正则表达式进行搜索。 locate 使用 /var/lib/mlocate/ 这个数据库来进行搜索，它存储在内存中，并且每天更新一次，所以无法用 locate 搜索新建的文件。可以使用 updatedb 来立即更新数据库。 12# locate [-ir] keyword-r：正则表达式 4. find文件搜索。可以使用文件的属性和权限进行搜索。 12# find [basedir] [option]example: find . -name \"shadow*\" ① 与时间有关的选项 1234-mtime n ：列出在 n 天前的那一天修改过内容的文件-mtime +n ：列出在 n 天之前 (不含 n 天本身) 修改过内容的文件-mtime -n ：列出在 n 天之内 (含 n 天本身) 修改过内容的文件-newer file ： 列出比 file 更新的文件 +4、4 和 -4 的指示的时间范围如下： ② 与文件拥有者和所属群组有关的选项 123456-uid n-gid n-user name-group name-nouser ：搜索拥有者不存在 /etc/passwd 的文件-nogroup：搜索所属群组不存在于 /etc/group 的文件 ③ 与文件权限和名称有关的选项 123456-name filename-size [+-]SIZE：搜寻比 SIZE 还要大 (+) 或小 (-) 的文件。这个 SIZE 的规格有：c: 代表 byte，k: 代表 1024bytes。所以，要找比 50KB 还要大的文件，就是 -size +50k-type TYPE-perm mode ：搜索权限等于 mode 的文件-perm -mode ：搜索权限包含 mode 的文件-perm /mode ：搜索权限包含任一 mode 的文件 六、压缩与打包压缩文件名Linux 底下有很多压缩文件名，常见的如下： 扩展名 压缩程序 *.Z compress *.zip zip *.gz gzip *.bz2 bzip2 *.xz xz *.tar tar 程序打包的数据，没有经过压缩 *.tar.gz tar 程序打包的文件，经过 gzip 的压缩 *.tar.bz2 tar 程序打包的文件，经过 bzip2 的压缩 *.tar.xz tar 程序打包的文件，经过 xz 的压缩 压缩指令1. gzipgzip 是 Linux 使用最广的压缩指令，可以解开 compress、zip 与 gzip 所压缩的文件。 经过 gzip 压缩过，源文件就不存在了。 有 9 个不同的压缩等级可以使用。 可以使用 zcat、zmore、zless 来读取压缩文件的内容。 123456$ gzip [-cdtv#] filename-c ：将压缩的数据输出到屏幕上-d ：解压缩-t ：检验压缩文件是否出错-v ：显示压缩比等信息-# ： # 为数字的意思，代表压缩等级，数字越大压缩比越高，默认为 6 2. bzip2提供比 gzip 更高的压缩比。 查看命令：bzcat、bzmore、bzless、bzgrep。 12$ bzip2 [-cdkzv#] filename-k ：保留源文件 3. xz提供比 bzip2 更佳的压缩比。 可以看到，gzip、bzip2、xz 的压缩比不断优化。不过要注意的是，压缩比越高，压缩的时间也越长。 查看命令：xzcat、xzmore、xzless、xzgrep。 1$ xz [-dtlkc#] filename 打包压缩指令只能对一个文件进行压缩，而打包能够将多个文件打包成一个大文件。tar 不仅可以用于打包，也可以使用 gzip、bzip2、xz 将打包文件进行压缩。 123456789101112$ tar [-z|-j|-J] [cv] [-f 新建的 tar 文件] filename... ==打包压缩$ tar [-z|-j|-J] [tv] [-f 已有的 tar 文件] ==查看$ tar [-z|-j|-J] [xv] [-f 已有的 tar 文件] [-C 目录] ==解压缩-z ：使用 zip；-j ：使用 bzip2；-J ：使用 xz；-c ：新建打包文件；-t ：查看打包文件里面有哪些文件；-x ：解打包或解压缩的功能；-v ：在压缩/解压缩的过程中，显示正在处理的文件名；-f : filename：要处理的文件；-C 目录 ： 在特定目录解压缩。 使用方式 命令 打包压缩 tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称 查 看 tar -jtv -f filename.tar.bz2 解压缩 tar -jxv -f filename.tar.bz2 -C 要解压缩的目录 七、Bash可以通过 Shell 请求内核提供服务，Bash 正是 Shell 的一种。 特性 命令历史：记录使用过的命令 命令与文件补全：快捷键：tab 命名别名：例如 ll 是 ls -al 的别名 shell scripts 通配符：例如 ls -l /usr/bin/X* 列出 /usr/bin 下面所有以 X 开头的文件 变量操作对一个变量赋值直接使用 =。 对变量取用需要在变量前加上 $ ，也可以用 ${} 的形式； 输出变量使用 echo 命令。 123$ x=abc$ echo $x$ echo $&#123;x&#125; 变量内容如果有空格，必须使用双引号或者单引号。 双引号内的特殊字符可以保留原本特性，例如 x=”lang is $LANG”，则 x 的值为 lang is zh_TW.UTF-8； 单引号内的特殊字符就是特殊字符本身，例如 x=’lang is $LANG’，则 x 的值为 lang is $LANG。 可以使用 指令 或者 $(指令) 的方式将指令的执行结果赋值给变量。例如 version=$(uname -r)，则 version 的值为 4.15.0-22-generic。 可以使用 export 命令将自定义变量转成环境变量，环境变量可以在子程序中使用，所谓子程序就是由当前 Bash 而产生的子 Bash。 Bash 的变量可以声明为数组和整数数字。注意数字类型没有浮点数。如果不进行声明，默认是字符串类型。变量的声明使用 declare 命令： 12345$ declare [-aixr] variable-a ： 定义为数组类型-i ： 定义为整数类型-x ： 定义为环境变量-r ： 定义为 readonly 类型 使用 [ ] 来对数组进行索引操作： 123$ array[1]=a$ array[2]=b$ echo $&#123;array[1]&#125; 指令搜索顺序 以绝对或相对路径来执行指令，例如 /bin/ls 或者 ./ls ； 由别名找到该指令来执行； 由 Bash 内置的指令来执行； 按 $PATH 变量指定的搜索路径的顺序找到第一个指令来执行。 数据流重定向重定向指的是使用文件代替标准输入、标准输出和标准错误输出。 1 代码 运算符 标准输入 (stdin) 0 &lt; 或 &lt;&lt; 标准输出 (stdout) 1 &gt; 或 &gt;&gt; 标准错误输出 (stderr) 2 2&gt; 或 2&gt;&gt; 其中，有一个箭头的表示以覆盖的方式重定向，而有两个箭头的表示以追加的方式重定向。 可以将不需要的标准输出以及标准错误输出重定向到 /dev/null，相当于扔进垃圾箱。 如果需要将标准输出以及标准错误输出同时重定向到一个文件，需要将某个输出转换为另一个输出，例如 2&gt;&amp;1 表示将标准错误输出转换为标准输出。 1$ find /home -name .bashrc &gt; list 2&gt;&amp;1 八、管道指令管道是将一个命令的标准输出作为另一个命令的标准输入，在数据需要经过多个步骤的处理之后才能得到我们想要的内容时就可以使用管道。 在命令之间使用 | 分隔各个管道命令。 1$ ls -al /etc | less 提取指令cut 对数据进行切分，取出想要的部分。 切分过程一行一行地进行。 1234$ cut-d ：分隔符-f ：经过 -d 分隔后，使用 -f n 取出第 n 个区间-c ：以字符为单位取出区间 示例 1：last 显示登入者的信息，取出用户名。 123456$ lastroot pts/1 192.168.201.101 Sat Feb 7 12:35 still logged inroot pts/1 192.168.201.101 Fri Feb 6 12:13 - 18:46 (06:33)root pts/1 192.168.201.254 Thu Feb 5 22:37 - 23:53 (01:16)$ last | cut -d ' ' -f 1 示例 2：将 export 输出的信息，取出第 12 字符以后的所有字符串。 12345678$ exportdeclare -x HISTCONTROL=\"ignoredups\"declare -x HISTSIZE=\"1000\"declare -x HOME=\"/home/dmtsai\"declare -x HOSTNAME=\"study.centos.vbird\".....(其他省略).....$ export | cut -c 12- 排序指令sort 用于排序。 123456789$ sort [-fbMnrtuk] [file or stdin]-f ：忽略大小写-b ：忽略最前面的空格-M ：以月份的名字来排序，例如 JAN，DEC-n ：使用数字-r ：反向排序-u ：相当于 unique，重复的内容只出现一次-t ：分隔符，默认为 tab-k ：指定排序的区间 示例：/etc/passwd 文件内容以 : 来分隔，要求以第三列进行排序。 12345$ cat /etc/passwd | sort -t ':' -k 3root:x:0:0:root:/root:/bin/bashdmtsai:x:1000:1000:dmtsai:/home/dmtsai:/bin/bashalex:x:1001:1002::/home/alex:/bin/basharod:x:1002:1003::/home/arod:/bin/bash uniq 可以将重复的数据只取一个。 123$ uniq [-ic]-i ：忽略大小写-c ：进行计数 示例：取得每个人的登录总次数 1234567$ last | cut -d ' ' -f 1 | sort | uniq -c16 (unknown47 dmtsai4 reboot7 root1 wtmp 双向输出重定向输出重定向会将输出内容重定向到文件中，而 tee 不仅能够完成这个功能，还能保留屏幕上的输出。也就是说，使用 tee 指令，一个输出会同时传送到文件和屏幕上。 1$ tee [-a] file 字符转换指令tr 用来删除一行中的字符，或者对字符进行替换。 12$ tr [-ds] SET1 ...-d ： 删除行中 SET1 这个字符串 示例，将 last 输出的信息所有小写转换为大写。 1$ last | tr '[a-z]' '[A-Z]' col 将 tab 字符转为空格字符。 12$ col [-xb]-x ： 将 tab 键转换成对等的空格键 expand 将 tab 转换一定数量的空格，默认是 8 个。 12$ expand [-t] file-t ：tab 转为空格的数量 join 将有相同数据的那一行合并在一起。 12345$ join [-ti12] file1 file2-t ：分隔符，默认为空格-i ：忽略大小写的差异-1 ：第一个文件所用的比较字段-2 ：第二个文件所用的比较字段 paste 直接将两行粘贴在一起。 12$ paste [-d] file1 file2-d ：分隔符，默认为 tab 分区指令split 将一个文件划分成多个文件。 1234$ split [-bl] file PREFIX-b ：以大小来进行分区，可加单位，例如 b, k, m 等-l ：以行数来进行分区。- PREFIX ：分区文件的前导名称 九、正则表达式grepg/re/p（globally search a regular expression and print)，使用正则表示式进行全局查找并打印。 123456$ grep [-acinv] [--color=auto] 搜寻字符串 filename-c ： 统计个数-i ： 忽略大小写-n ： 输出行号-v ： 反向选择，也就是显示出没有 搜寻字符串 内容的那一行--color=auto ：找到的关键字加颜色显示 示例：把含有 the 字符串的行提取出来（注意默认会有 –color=auto 选项，因此以下内容在 Linux 中有颜色显示 the 字符串） 123456$ grep -n 'the' regular_express.txt8:I can't finish the test.12:the symbol '*' is represented as start.15:You are the best is mean you are the no. 1.16:The world Happy is the same with \"glad\".18:google is the best tools for search keyword 因为 { 和 } 在 shell 是有特殊意义的，因此必须要使用转义字符进行转义。 1$ grep -n 'go\\&#123;2,5\\&#125;g' regular_express.txt printf用于格式化输出。它不属于管道命令，在给 printf 传数据时需要使用 $( ) 形式。 1234$ printf '%10s %5i %5i %5i %8.2f \\n' $(cat printf.txt) DmTsai 80 60 92 77.33 VBird 75 55 80 70.00 Ken 60 90 70 73.33 awk是由 Alfred Aho，Peter Weinberger, 和 Brian Kernighan 创造，awk 这个名字就是这三个创始人名字的首字母。 awk 每次处理一行，处理的最小单位是字段，每个字段的命名方式为：$n，n 为字段号，从 1 开始，$0 表示一整行。 示例：取出最近五个登录用户的用户名和 IP 1234567$ last -n 5dmtsai pts/0 192.168.1.100 Tue Jul 14 17:32 still logged indmtsai pts/0 192.168.1.100 Thu Jul 9 23:36 - 02:58 (03:22)dmtsai pts/0 192.168.1.100 Thu Jul 9 17:23 - 23:36 (06:12)dmtsai pts/0 192.168.1.100 Thu Jul 9 08:02 - 08:17 (00:14)dmtsai tty1 Fri May 29 11:55 - 12:11 (00:15)$ last -n 5 | awk '&#123;print $1 \"\\t\" $3&#125;' 可以根据字段的某些条件进行匹配，例如匹配字段小于某个值的那一行数据。 1$ awk '条件类型 1 &#123;动作 1&#125; 条件类型 2 &#123;动作 2&#125; ...' filename 示例：/etc/passwd 文件第三个字段为 UID，对 UID 小于 10 的数据进行处理。 1234$ cat /etc/passwd | awk &apos;BEGIN &#123;FS=&quot;:&quot;&#125; $3 &lt; 10 &#123;print $1 &quot;\\t &quot; $3&#125;&apos;root 0bin 1daemon 2 awk 变量： 变量名称 代表意义 NF 每一行拥有的字段总数 NR 目前所处理的是第几行数据 FS 目前的分隔字符，默认是空格键 示例：显示正在处理的行号以及每一行有多少字段 123456$ last -n 5 | awk '&#123;print $1 \"\\t lines: \" NR \"\\t columns: \" NF&#125;'dmtsai lines: 1 columns: 10dmtsai lines: 2 columns: 10dmtsai lines: 3 columns: 10dmtsai lines: 4 columns: 10dmtsai lines: 5 columns: 9 十、进程管理查看进程1. ps查看某个时间点的进程信息。 示例一：查看自己的进程 1# ps -l 示例二：查看系统所有进程 1# ps aux 示例三：查看特定的进程 1# ps aux | grep threadx 2. pstree查看进程树。 示例：查看所有进程树 1# pstree -A 3. top实时显示进程占用资源信息(cpu, 内存)。 示例：两秒钟刷新一次 1# top -d 2 4. netstat查看占用端口的进程 示例：查看特定端口的进程 1# netstat -anp | grep port 进程状态 状态 说明 R running or runnable (on run queue) 正在执行或者可执行，此时进程位于执行队列中。 D uninterruptible sleep (usually I/O) 不可中断阻塞，通常为 IO 阻塞。 S interruptible sleep (waiting for an event to complete) 可中断阻塞，此时进程正在等待某个事件完成。 Z zombie (terminated but not reaped by its parent) 僵死，进程已经终止但是尚未被其父进程获取信息。 T stopped (either by a job control signal or because it is being traced) 结束，进程既可以被作业控制信号结束，也可能是正在被追踪。 SIGCHLD当一个子进程改变了它的状态时（停止运行，继续运行或者退出），有两件事会发生在父进程中： 得到 SIGCHLD 信号； waitpid() 或者 wait() 调用会返回。 其中子进程发送的 SIGCHLD 信号包含了子进程的信息，比如进程 ID、进程状态、进程使用 CPU 的时间等。 在子进程退出时，它的进程描述符不会立即释放，这是为了让父进程得到子进程信息，父进程通过 wait() 和 waitpid() 来获得一个已经退出的子进程的信息。 wait()1pid_t wait(int *status) 父进程调用 wait() 会一直阻塞，直到收到一个子进程退出的 SIGCHLD 信号，之后 wait() 函数会销毁子进程并返回。 如果成功，返回被收集的子进程的进程 ID；如果调用进程没有子进程，调用就会失败，此时返回 -1，同时 errno 被置为 ECHILD。 参数 status 用来保存被收集的子进程退出时的一些状态，如果对这个子进程是如何死掉的毫不在意，只想把这个子进程消灭掉，可以设置这个参数为 NULL。 waitpid()1pid_t waitpid(pid_t pid, int *status, int options) 作用和 wait() 完全相同，但是多了两个可由用户控制的参数 pid 和 options。 pid 参数指示一个子进程的 ID，表示只关心这个子进程退出的 SIGCHLD 信号。如果 pid=-1 时，那么和 wait() 作用相同，都是关心所有子进程退出的 SIGCHLD 信号。 options 参数主要有 WNOHANG 和 WUNTRACED 两个选项，WNOHANG 可以使 waitpid() 调用变成非阻塞的，也就是说它会立即返回，父进程可以继续执行其它任务。 孤儿进程一个父进程退出，而它的一个或多个子进程还在运行，那么这些子进程将成为孤儿进程。 孤儿进程将被 init 进程（进程号为 1）所收养，并由 init 进程对它们完成状态收集工作。 由于孤儿进程会被 init 进程收养，所以孤儿进程不会对系统造成危害。 僵尸进程一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 wait() 或 waitpid() 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 wait() 或 waitpid()，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。 僵尸进程通过 ps 命令显示出来的状态为 Z（zombie）。 系统所能使用的进程号是有限的，如果产生大量僵尸进程，将因为没有可用的进程号而导致系统不能产生新的进程。 要消灭系统中大量的僵尸进程，只需要将其父进程杀死，此时僵尸进程就会变成孤儿进程，从而被 init 进程所收养，这样 init 进程就会释放所有的僵尸进程所占有的资源，从而结束僵尸进程。 参考资料 鸟哥. 鸟 哥 的 Linux 私 房 菜 基 础 篇 第 三 版[J]. 2009. Linux 平台上的软件包管理 Linux 之守护进程、僵死进程与孤儿进程 What is the difference between a symbolic link and a hard link? Linux process states GUID Partition Table 详解 wait 和 waitpid 函数 IDE、SATA、SCSI、SAS、FC、SSD 硬盘类型介绍 Akai IB-301S SCSI Interface for S2800,S3000 Parallel ATA ADATA XPG SX900 256GB SATA 3 SSD Review – Expanded Capacity and SandForce Driven Speed Decoding UCS Invicta – Part 1 硬盘 Difference between SAS and SATA BIOS File system design case studies Programming Project #4 FILE SYSTEM DESIGN","categories":[],"tags":[{"name":"IT","slug":"IT","permalink":"/tags/IT/"},{"name":"技术笔记","slug":"技术笔记","permalink":"/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/"},{"name":"Job","slug":"Job","permalink":"/tags/Job/"},{"name":"Linux","slug":"Linux","permalink":"/tags/Linux/"}]}]}